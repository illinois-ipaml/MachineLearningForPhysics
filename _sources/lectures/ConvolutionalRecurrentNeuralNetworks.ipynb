{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWJA7ZtrbZxT"
   },
   "source": [
    "# Convolutional and Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWChhLTIbZxU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess\n",
    "import matplotlib.collections\n",
    "import scipy.signal\n",
    "from sklearn import model_selection\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMOvSQpCbZxV"
   },
   "outputs": [],
   "source": [
    "# Use CPU rather than GPU for keras neural networks\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "from tensorflow import keras\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtGN9cFub_EU"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_v2_behavior()      # ensure TF2 behavior\n",
    "tf.config.run_functions_eagerly(True)  # optional, makes debugging easier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjBGinEp2ehp"
   },
   "source": [
    "## <span style=\"color:Orange\">Recurrent Networks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-y7GUMX2ehp"
   },
   "source": [
    "All the architectures we have seen so far are **feed-foward** networks, with input data always from left (input layer) to right (output layer). A <span style=\"color:Violet\">recurrent neural network</span> (RNN) adds links that feed back into a previous layer. This simple modification adds significant complexity but also expressive power (comparable to the electronics revolution associated with the idea of transistor feedback).\n",
    "\n",
    "Architectures with feedback are still maturing but some useful building blocks have emerged, such as the [long short-term memory unit](https://en.wikipedia.org/wiki/Long_short-term_memory), which allows a network to remember some internal state but also forget it based on new input.\n",
    "\n",
    "Some practical considerations for RNN designs:\n",
    " - The order of training data is now significant and defines a \"model time\", but the network can be reset whenever needed.\n",
    "\n",
    " - Input data can be packaged into variable-length messages that generate variable (and different) length output messages. This is exactly what language translation needs.\n",
    "\n",
    " - Optimization of the weights using gradients is still possible but requires \"unrolling\" the network by cloning it enough times to process the longest allowed messages.\n",
    "\n",
    "A feed-foward network implements a universal approximating function. Since the internal state of an RNN acts like local variables, you can think of an RNN as a universal approximating program.\n",
    "\n",
    "See this [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) for an example based on natural language synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MH94-Fw2ehp"
   },
   "source": [
    "A \"recurrent\" neural network is not exactly feedforward.\n",
    "There are a variety of forms for a recurrent network, but using the previous diagramming method, we can write the most common form as:\n",
    "\n",
    "![](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Time_Series_Analysis_and_Forecasting/diagrams/recurrent_nn_loop.drawio.svg)\n",
    "\n",
    "As you can see, there is a loop (the recurrent part) which passes information from one evaluation of the function to the next time the function is evaluated.\n",
    "This might seem strange at first glance but makes more sense when you consider a sequence of events.\n",
    "For example, words.\n",
    "If we have three words of a sentence, predicting the next word likely depends on all three words rather than only the previous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKFJADIr2ehp"
   },
   "source": [
    "### <span style=\"color:LightGreen\">Stucture</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQfU258i2ehq"
   },
   "source": [
    "The \"looped\" diagram shown above can also be written in an \"unrolled\" form as follows:\n",
    "\n",
    "___<span style=\"color:Tan\">Many-to-many</span>___\n",
    "\n",
    "![](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Time_Series_Analysis_and_Forecasting/diagrams/recurrent_nn1.drawio.svg)\n",
    "\n",
    "Note that this form of recurrent neural network requires inputs at each step and gives outputs at each step.\n",
    "This is is not strictly necessary and you could instead have only the end output or only one input and one output as show below:\n",
    "\n",
    "___<span style=\"color:Tan\">Many-to-one</span>___\n",
    "![](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Time_Series_Analysis_and_Forecasting/diagrams/recurrent_nn2.drawio.svg)\n",
    "\n",
    "___<span style=\"color:Tan\">One-to-one</span>___\n",
    "![](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Time_Series_Analysis_and_Forecasting/diagrams/recurrent_nn3.drawio.svg)\n",
    "\n",
    "\n",
    "Each of the demonstrated diagrams features a very simple version of the neural network, but it could have many layers at each step such as the following:\n",
    "\n",
    "![](https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Time_Series_Analysis_and_Forecasting/diagrams/recurrent_nn_loop2.drawio.svg)\n",
    "\n",
    "Fortunately, these can all be easily implemented using the `keras` framework.\n",
    "Let's return to our example of input data of the form $[\\sin(t),\\cos(t)]$ and outputs of the form $\\sin(t)\\cos(t)$ but lets try to use 2 values of $t$ to get the next value.\n",
    "So, we will pass in something like\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{bmatrix}\n",
    "\\sin(t_1) & \\cos(t_1) \\\\\n",
    "\\sin(t_2) & \\cos(t_2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "to get the output $\\sin(t_3)\\cos(t_3)$.\n",
    "We'll use two layers.\n",
    "See below for examples of these recurrent neural network forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaNu4Obc2ehq"
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(0)\n",
    "\n",
    "# Make input layer of appropriate size (2 samples of size 2 or 1 sample of size 2)\n",
    "x_many   = keras.layers.Input(shape=(2,2))\n",
    "x_one    = keras.layers.Input(shape=(1,2))\n",
    "\n",
    "# Pass input x into first layer of size 3 x 2\n",
    "# return_sequences=True means there is an output for each input\n",
    "y_many = keras.layers.SimpleRNN(3,activation=\"tanh\", return_sequences=True)(x_many)\n",
    "y_one  = keras.layers.SimpleRNN(3,activation=\"tanh\", return_sequences=True)(x_one)\n",
    "y_many_to_many = keras.layers.SimpleRNN(1,activation=\"tanh\", return_sequences=True)(y_many)\n",
    "y_many_to_one  = keras.layers.SimpleRNN(1,activation=\"tanh\", return_sequences=False)(y_many)\n",
    "y_one_to_one   = keras.layers.SimpleRNN(1,activation=\"tanh\", return_sequences=False)(y_one)\n",
    "\n",
    "many_to_many = keras.Model(inputs=x_many,outputs=y_many_to_many)\n",
    "many_to_one  = keras.Model(inputs=x_many,outputs=y_many_to_one)\n",
    "one_to_one   = keras.Model(inputs=x_one ,outputs=y_one_to_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Z86oKoy2ehq",
    "outputId": "c7e79062-f126-4cc6-cd57-6bef04cced3c"
   },
   "outputs": [],
   "source": [
    "# 1 batch of 2 random samples of size 2\n",
    "sample_many = np.random.random(size=(1,2,2))\n",
    "sample_one  = np.random.random(size=(1,1,2))\n",
    "\n",
    "print(\"Sample of 2: \\n{}\".format(sample_many))\n",
    "print(\"Many to many output: \\n{}\".format(many_to_many(sample_many)))\n",
    "print(\"Many to one output: \\n{}\".format(many_to_one(sample_many)))\n",
    "print(\"Sample of 1: \\n{}\".format(sample_one))\n",
    "print(\"One to one output: \\n{}\".format(one_to_one(sample_one)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5r6pMDD2ehq"
   },
   "source": [
    "The number passed into the `SimpleRNN` is the number of loops performed.\n",
    "In the case described above, we want to take samples at times $t_1$ and $t_2$ and output the value of the function at time $t_3$.\n",
    "This is a \"many-to-one\" case.\n",
    "\n",
    "In order to train the network to perform well in this case, we need to arrange our data in pairs of\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{bmatrix}\n",
    "\\sin(t_i) & \\cos(t_{i}) \\\\\n",
    "\\sin(t_{i+1}) & \\cos(t_{i+1})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "aligned with outputs $\\sin(t_{i+2})\\cos(t_{i+2})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17pQAWx_2ehq"
   },
   "outputs": [],
   "source": [
    "# Make our data samples\n",
    "t_values = np.linspace(0,10,100)\n",
    "samples = np.vstack([np.sin(t_values), np.cos(t_values)]).T\n",
    "output_values = np.sin(t_values)*np.cos(t_values)\n",
    "\n",
    "# Arrange our data samples\n",
    "input_samples = []\n",
    "output_samples = []\n",
    "for i in range(98):\n",
    "  # Take two samples at time t_i and t_{i+1}\n",
    "  input_samples.append(samples[i:i+2])\n",
    "  # Get function output at time t_{i+2}\n",
    "  output_samples.append(output_values[i+2])\n",
    "\n",
    "input_samples = np.array(input_samples)\n",
    "output_samples = np.array(output_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhzmPTYc2ehq"
   },
   "source": [
    "We can now compile the many to one model and train it on this test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582,
     "referenced_widgets": [
      "7ffd6e23f42a46f2bbe2926a66b285c1",
      "2a3b6480e1a1450cb766e3f282963393",
      "81958822c99f4e43964f7ca03974e8af",
      "7d0ed1d607114bf9a6e276ef6e4b2895",
      "bffa111964c04858930dc66cebbe95d0",
      "2044eadebda849809b0b198a63dd7aab",
      "d25f35b8c4d24c5f9383d9d63292bd6a",
      "052498b31e404b1eb06e6006dd3df318",
      "4596de04458f4a2e8c401051b37a90cc",
      "9e9ab673fe59453abf2ca1c77ffc4572",
      "7d0c6198563d40489201d50116e0d5cf"
     ]
    },
    "id": "VvKDN1K62ehq",
    "outputId": "57c03fdd-994b-4f9f-cf96-b77f52f75bc2"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "many_to_one.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = keras.losses.MeanSquaredError()\n",
    ")\n",
    "history = many_to_one.fit(\n",
    "    input_samples,\n",
    "    output_samples,\n",
    "    batch_size=10,         # The training takes groups of samples (in this case 10 samples at a time)\n",
    "    epochs=500,            # The number of times to iterate through our dataset\n",
    "    validation_split = 0.2,# Use 20% of data to check accuracy\n",
    "    verbose=0,             # Don't print info as it trains\n",
    "    callbacks=[TqdmCallback(verbose=0)]\n",
    ")\n",
    "\n",
    "# Plot prediction and the true values\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(t_values, output_values, label=\"$\\sin(t)\\cos(t)$\")\n",
    "plt.plot(t_values[2:], many_to_one(input_samples), label=\"many_to_one(t)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"t\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw4Y_eLD2ehq"
   },
   "source": [
    "Similar to the feedforward neural network, the recurrent architecture was able to roughly approximate the curve!\n",
    "Although it is not very noticeable in this case, the recurrent model can also uses less weights and biases (because it has the looping behavior built in) making it less computationally expensive and easier to train!\n",
    "This has made the recurrent architecture very popular for time series like applications for real world problems (in fact, modern transformers such as ChatGPT are built on the same concepts as RNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wse-rkOqbZxX"
   },
   "source": [
    "## <span style=\"color:Orange\">Convolutional Networks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I3_D-nwbZxf"
   },
   "source": [
    "A _<span style=\"color:Violet\">Convolutional Neural Network</span>_ (CNN) is a special architecture that:\n",
    " - Assumes that input features measure some property on a grid. The grid is usually spatial or temporal, but this is not required. For example, a 1D spectrum or time series, a 2D monochrome image, or a 3D stack of 2D images in different filters (RGB, etc).\n",
    "\n",
    " - Performs translation-invariant learning efficiently. For example, identifying a galaxy wherever it appears in an image, or a transient pulse wherever it appears in a time series. The main efficiency is a much reduced number of parameters compared to the number of input features, relative to the dense fully connected networks we have seen so far.\n",
    "\n",
    "As we saw in the previous lecture, Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.\n",
    "\n",
    "The fully-connected, feed-forward neural networks we have studied thus far do not scale well to large image data. For example, a modest 200 $\\times$ 200 $\\times$ 3 (x-pixels, y-pixels, 3 colors) image would lead to neurons that have 200 $\\times$ 200 $\\times$ 3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\n",
    "\n",
    "Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way to reduce the number of parameters. In particular, unlike a regular Neural Network, the layers of a CNN have neurons arranged in 3 dimensions: width, height, depth.\n",
    "\n",
    "(Note that the word \"depth\" here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network...)\n",
    "\n",
    "The neurons in a CNN layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.\n",
    "\n",
    "A CNN is made up of layers of different types (convolutions, pooling, fully-connected), in general. Every layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sODlPGCebZxg"
   },
   "source": [
    "We will use the following problem to motivate and demonstration a CNN:\n",
    "\n",
    " - The input data consists of triplets of digitized waveforms.\n",
    "\n",
    " - Each waveform has a slowly varying level with some narrow pulses superimposed.\n",
    "\n",
    " - Each triplet has a single pulse that is synchronized (coincident) in all three waveforms.\n",
    "\n",
    " - Waveforms also contain a random number of unsynchronized \"background\" pulses.\n",
    "\n",
    " - Synchronized and unsynchronized pulses can overlap in time and between traces.\n",
    "\n",
    "The goal is to identify the location of the synchronized pulses in each triplet. This is a simplified version of a common task in data acquisition trigger systems and transient analysis pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp5-HfnxbZxg"
   },
   "outputs": [],
   "source": [
    "def generate(N=10000, ntrace=3, nt=100, nbg=1., A=5., nsmooth=3, T=1., seed=123):\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    t_grid = np.linspace(0., T, nt)\n",
    "    # Generate the smooth background shapes as superpositions of random cosines.\n",
    "    wlen = 2 * T * gen.lognormal(mean=0., sigma=0.2, size=(nsmooth, N, ntrace, 1))\n",
    "    phase = gen.uniform(size=wlen.shape)\n",
    "    X = np.cos(2 * np.pi * (t_grid + phase * wlen) / wlen).sum(axis=0)\n",
    "    # Superimpose short pulses.\n",
    "    sigma = 0.02 * T\n",
    "    tsig = T * gen.uniform(0.05, 0.95, size=N)\n",
    "    y = np.empty(N, dtype=int)\n",
    "    nbg = gen.poisson(lam=nbg, size=(N, ntrace))\n",
    "    for i in range(N):\n",
    "        # Add a coincident pulse to all traces.\n",
    "        xsig = A * np.exp(-0.5 * (t_grid - tsig[i]) ** 2 / sigma ** 2)\n",
    "        y[i] = np.argmax(xsig)\n",
    "        X[i] += xsig\n",
    "        # Add non-coincident background pulses to each trace.\n",
    "        for j in range(ntrace):\n",
    "            if nbg[i, j] > 0:\n",
    "                t0 = T * gen.uniform(size=(nbg[i, j], 1))\n",
    "                X[i, j] += (A * np.exp(-0.5 * (t_grid - t0) ** 2 / sigma ** 2)).sum(axis=0)\n",
    "    return X.astype(np.float32), y\n",
    "\n",
    "X, y = generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "kpUkpdFnbZxg",
    "outputId": "eaa9b862-d970-4bf6-c7e0-ba06fab4df3f"
   },
   "outputs": [],
   "source": [
    "def plot_traces(X, y):\n",
    "    Nsample, Ntrace, D = X.shape\n",
    "    _, ax = plt.subplots(Nsample, 1, figsize=(9, 1.5 * Nsample))\n",
    "    t = np.linspace(0., 1., 100)\n",
    "    dt = t[1] - t[0]\n",
    "    for i in range(Nsample):\n",
    "        for j in range(Ntrace):\n",
    "            ax[i].plot(t, X[i, j], lw=1)\n",
    "        ax[i].axvline(t[y[i]], c='k', ls=':')\n",
    "        ax[i].set_yticks([])\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_xlim(-0.5 * dt, 1 + 0.5 * dt)\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.99, hspace=0.1)\n",
    "\n",
    "plot_traces(X[:5], y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NZRn3WtbZxg"
   },
   "source": [
    "The derivative of $f(x)$ can be approximated as\n",
    "\n",
    "$$ \\Large\n",
    "f'(x) \\simeq \\frac{f(x + \\delta) - f(x - \\delta)}{2\\delta}\n",
    "$$\n",
    "\n",
    "for small $\\delta$. We can use this approximation to convert an array of $f(n \\Delta x)$ values into an array of estimated $f'(n \\Delta x)$ values using:\n",
    "```\n",
    "K = np.array([-1, 0, +1]) / ( 2 * dx)\n",
    "fp[0] = K.dot(f[[0,1,2]])\n",
    "fp[1] = K.dot(f[[1,2,3]])\n",
    "...\n",
    "fp[N-2] = K.dot(f[[N-3,N-2,N-1]]\n",
    "```\n",
    "The numpy [convolve function](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.convolve.html) automates this process of sliding an arbitrary <span style=\"color:Violet\">kernel</span> $K$ along an input array like this. The result only estimates a first (or higher-order) derivative when the kernel contains [special values](https://en.wikipedia.org/wiki/Finite_difference_coefficient) (and you should normally use the numpy [gradient function](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.gradient.html) for this), but any convolution is a valid and potentially useful transformation.\n",
    "\n",
    "A clarifying word about terminology: In the context of convolutional networks, <span style=\"color:Violet\">kernel</span> is a simple group of weights shared all over the input space that is engineered to determine what specific features are to be detected. The kernel is also sometimes referred to as a \"feature map\" or \"filter\" in this context.\n",
    "\n",
    "See for example the application of a kernel in convolution over a simple black-and-white image:\n",
    "[here](https://i.stack.imgur.com/9Iu89.gif).\n",
    "\n",
    "\n",
    "The kernel needs to completely overlap the input array it is being convolved with, which means that the output array is smaller and offset. Alternatively, you can pad the input array with zeros to extend the output array. There are three different conventions for handling these edge effects via the `mode` parameter to `np.convolve`:\n",
    " - **valid**: no zero padding, so output length is $N - K + 1$ and offset is $(K-1)/2$.\n",
    "\n",
    " - **same**: apply zero padding and trim so output length equals input length $N$, and offset is zero.\n",
    "\n",
    " - **full**: apply zero padding without trimming, so output length is $N + K - 1$ and offset is $-(K-1)/2$.\n",
    "\n",
    "(Here $N$ and $K$ are the input and kernel lengths, respectively).\n",
    "\n",
    "We can use a convolution to identify features in our input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxqkGpVQbZxg"
   },
   "outputs": [],
   "source": [
    "def plot_convolved(x, kernel, smax=50):\n",
    "    t = np.arange(len(x))\n",
    "    plt.plot(t, x, lw=1, c='gray')\n",
    "    z = np.convolve(x, kernel, mode='same')\n",
    "    for sel, c in zip(((z > 0), (z < 0)), 'rb'):\n",
    "        plt.scatter(t[sel], x[sel], c=c, s=smax * np.abs(z[sel]), lw=0)\n",
    "    plt.gca()\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cyYvvxWbZxg"
   },
   "source": [
    "First, let's pick out regions of large positive (red) or negative slope (notice how the edge padding causes some artifacts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "lhyV5i-vbZxg",
    "outputId": "ab405673-0de2-4dbd-e226-679a8428e5dd"
   },
   "outputs": [],
   "source": [
    "plot_convolved(X[1, 1], [0.5,0,-0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rE9zEc0bZxg"
   },
   "source": [
    "We can also pick out regions of large curvature (using the finite-difference coefficients for a second derivative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "JWB0G0z9bZxg",
    "outputId": "13761bac-eafa-4fbd-92c9-f30df8cd403b"
   },
   "outputs": [],
   "source": [
    "plot_convolved(X[1, 1], [1.,-2.,1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzABiL7SbZxg"
   },
   "source": [
    "We can apply both of these convolutions to transform our input data to a new representation that highlights regions of large first or second derivative. Use a `tanh` activation to accentuate the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1ModXuAbZxg"
   },
   "outputs": [],
   "source": [
    "def apply_convolutions(X, *kernels):\n",
    "    N1, N2, D = X.shape\n",
    "    print(X.shape)\n",
    "    out = []\n",
    "    for i in range(N1):\n",
    "        sample = []\n",
    "        for j in range(N2):\n",
    "            for K in kernels:\n",
    "                sample.append(np.tanh(np.convolve(X[i, j], K, mode='valid')))\n",
    "        out.append(sample)\n",
    "    return np.asarray(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOdCr2LNbZxg",
    "outputId": "3706c444-c4a6-4e25-8fce-ffab8a26416e"
   },
   "outputs": [],
   "source": [
    "out = apply_convolutions(X, [0.5,0,-0.5], [1.,-2.,1.])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfICVs0AbZxg"
   },
   "source": [
    "The resulting array can be viewed as a synthetic image and offers an easy way to visually identify individual narrow peaks and their correlations between traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "i70i1DQLbZxg",
    "outputId": "1cf45d2a-bd53-47bc-9a0d-6f212b231f2a"
   },
   "outputs": [],
   "source": [
    "def plot_synthetic(Z):\n",
    "    _, ax = plt.subplots(len(Z), 1, figsize=(9, len(Z)))\n",
    "    for i, z in enumerate(Z):\n",
    "        ax[i].imshow(z, aspect='auto', origin='upper', interpolation='none',\n",
    "                   cmap='coolwarm', vmin=-1, vmax=+1);\n",
    "        ax[i].grid(False)\n",
    "        ax[i].axis('off')\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.99, hspace=0.1)\n",
    "\n",
    "plot_synthetic(out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOHasz3UbZxg"
   },
   "source": [
    "The patterns that identify individual and coincident peaks are all translation invariant so can be identified in this array using a new convolution, but now in the 2D space of these synthetic images.\n",
    "\n",
    "Since matrix convolution is a linear operation, it is a special case of our general neural network unit,\n",
    "\n",
    "$$ \\Large\n",
    "\\mathbf{f}(\\mathbf{x}) = W\\mathbf{x} + \\mathbf{b} \\; ,\n",
    "$$\n",
    "\n",
    "but with the matrix $W$ now having many repeated elements so its effective number of dimensions is greatly reduced in typical applications.\n",
    "\n",
    "A <span style=\"color:Violet\">convolutional layer</span> takes an arbitrary input array and applies a number of filters with the same shape in parallel. By default, the filter kernels march with single-element steps through the input array, but you can also specify larger <span style=\"color:Violet\">stride vector</span>.\n",
    "\n",
    "In the general case, the input array, kernels and stride vector are all multidimensional, but with the same dimension. Tensorflow provides convenience functions for 1D, 2D and 3D convolutional layers, for example:\n",
    "```\n",
    "hidden = tf.layers.Conv2D(\n",
    "    filters=3, kernel_size=[4, 5], strides=[2, 1],\n",
    "    padding='same', activation=tf.nn.relu)\n",
    "```\n",
    "Note that `padding` specifies how edges effects are handled, but only `same` and `valid` are supported (and `valid` is the default). You can also implement higher-dimensional convolutional layers using the lower-level APIs.\n",
    "\n",
    "A _<span style=\"color:Violet\">convolutional neural network</span>_ (CNN) is a network containing convolutional layers. A typical architecture starts with convolutional layers, processing the input, then finishes with some fully connected dense layers to calculate the output. Since one of the goals of a CNN is reduce the number of parameters, a CNN often also incorporates [pooling layers](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer) to reduce the size of the array fed to to later layers by \"downsampling\" (typically using a maximum or mean value). See [these Stanford CS231n notes](http://cs231n.github.io/convolutional-networks/) for more details in the context of image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEAOdm9sbZxh"
   },
   "outputs": [],
   "source": [
    "!rm -rf tfs/pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "id": "Wurg978ca3CP",
    "outputId": "12b0a02c-69ad-468e-fd75-921c6a24de3f"
   },
   "outputs": [],
   "source": [
    "# Keras builder (similar to Estimator used in the original version)\n",
    "def build_pulse_model(params):\n",
    "    D   = params['time_steps']\n",
    "    M   = params['number_of_traces']\n",
    "    n1  = params['conv1_width']\n",
    "    n2  = params['conv2_width']\n",
    "    lr  = params.get('learning_rate', 1e-3)\n",
    "\n",
    "    assert n1 % 2 == 1 and n2 % 2 == 1\n",
    "    Lout   = D - n2 + 1\n",
    "    offset = (n2 - 1) // 2\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(M, D, 1), name='X')\n",
    "\n",
    "    # per-trace temporal features (1 x n1), SAME padding\n",
    "    x = tf.keras.layers.Conv2D(filters=2, kernel_size=(1, n1), padding='same',\n",
    "                 activation='tanh', name='conv1')(inp)\n",
    "\n",
    "    # coincidence across traces (M x n2), VALID padding\n",
    "    x = tf.keras.layers.Conv2D(filters=1, kernel_size=(M, n2), padding='valid',\n",
    "                 activation=None, name='conv2')(x)  # (batch size, 1, Lout, 1)\n",
    "\n",
    "    logits = tf.keras.layers.Reshape((Lout,), name='logits')(x)  # (batch size, Lout)\n",
    "    model = tf.keras.Model(inp, logits, name='pulse_model')\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='acc')]\n",
    "        # , run_eagerly=True  # optional: handy for debugging\n",
    "    )\n",
    "    return model, offset, Lout\n",
    "\n",
    "# Train/Evaluate\n",
    "def train_eval(X, y, params, batch_size=500, steps=500, seed=123):\n",
    "    tf.random.set_seed(seed); np.random.seed(seed)\n",
    "    N, M, D = X.shape\n",
    "    assert D == params['time_steps'] and M == params['number_of_traces']\n",
    "\n",
    "    Xtr, Xte, ytr, yte = model_selection.train_test_split(\n",
    "        X, y, test_size=0.4, random_state=seed\n",
    "    )\n",
    "    model, offset, Lout = build_pulse_model(params)\n",
    "\n",
    "    # align labels to logits index space [0..Lout-1]\n",
    "    ytr_adj = ytr - offset\n",
    "    yte_adj = yte - offset\n",
    "\n",
    "    steps_per_epoch = int(np.ceil(len(Xtr) / batch_size))\n",
    "    epochs = int(np.ceil(steps / max(1, steps_per_epoch)))\n",
    "\n",
    "    model.fit(Xtr[:,:,:, np.newaxis], ytr_adj, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    eval_dict = model.evaluate(Xte[:, :, :, np.newaxis], yte_adj, verbose=0, return_dict=True)\n",
    "    return model, offset, eval_dict, (Xte, yte)\n",
    "\n",
    "# Predict and plot\n",
    "def plot_predictions(model, offset, X, y):\n",
    "    import matplotlib.pyplot as plt\n",
    "    Nsample, M, D = X.shape\n",
    "    logits = model.predict(X[:,:,:, np.newaxis], verbose=0)\n",
    "    probs_all = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "\n",
    "    t  = np.linspace(0., 1., D)\n",
    "    dt = t[1] - t[0]\n",
    "    bins = np.linspace(-0.5*dt, 1 + 0.5*dt, D + 1)\n",
    "\n",
    "    _, ax = plt.subplots(Nsample, 1, figsize=(9, 1.5*Nsample))\n",
    "    ax = np.atleast_1d(ax)\n",
    "\n",
    "    for i in range(Nsample):\n",
    "        pred_idx = probs_all[i].argmax() + offset  # map back to 0..D-1\n",
    "        Lout = probs_all.shape[1]\n",
    "        n2   = D - Lout + 1\n",
    "        offset  = (n2 - 1) // 2\n",
    "\n",
    "        probs = np.zeros(D, dtype=float)\n",
    "        probs[offset:offset+Lout] = probs_all[i]\n",
    "\n",
    "        for x in X[i]:\n",
    "            ax[i].plot(t, x, lw=1)\n",
    "        ax[i].axvline(t[y[i]],     c='k', ls=':',  lw=1)  # truth\n",
    "        ax[i].axvline(t[pred_idx], c='r', ls='--', lw=1)  # prediction\n",
    "\n",
    "        rhs = ax[i].twinx()\n",
    "        rhs.hist(t, weights=probs, bins=bins, histtype='stepfilled', alpha=0.25, color='k')\n",
    "        rhs.set_ylim(0., 1.); rhs.set_yticks([])\n",
    "        ax[i].set_xlim(bins[0], bins[-1]); ax[i].set_xticks([]); ax[i].set_yticks([]); ax[i].grid(False)\n",
    "\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.99, hspace=0.1)\n",
    "\n",
    "params = dict(\n",
    "    time_steps=100,\n",
    "    number_of_traces=3,\n",
    "    conv1_width=3,\n",
    "    conv2_width=7,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "model, offset, results, (X_test, y_test) = train_eval(X, y, params, batch_size=500, steps=500, seed=123)\n",
    "print(results)   # e.g., {'loss': ..., 'acc': ...}\n",
    "plot_predictions(model, offset, X_test[:5], y_test[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dipdNbXGbZxh"
   },
   "source": [
    "Evaluate how well the trained network performs on the test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ooj4PixbZxh"
   },
   "source": [
    "We find that about 95% of test samples are classified \"correctly\", defined as the network predicting the bin containing the the coincidence maximum exactly.  However, The RMS error between the predicted and true bins is only 0.4 bins, indicating that the network usually predicts a neighboring bin in the 5% of \"incorrect\" test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRObvcK6bZxh",
    "outputId": "13c986fd-6fb6-4427-ada1-baa0a875af5d"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7c9bEKjbZxh"
   },
   "source": [
    "Finally, compare the predicted (gray histogram) and true (dotted line) coincidence locations for a few test samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcqBM8Q2bZxi"
   },
   "source": [
    "Note that our loss function does not know that consecutive labels are close and being off by one is almost as good as getting the right label. We could change this by treating this as a regression problem, but a nice feature of our multi-category approach is that we can predict a a full probability density over labels (the gray histograms above) which is often useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiRhBwGz2ehu"
   },
   "source": [
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer (Updates: Aaron Pearlman)\n",
    "\n",
    "© Copyright 2024"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "052498b31e404b1eb06e6006dd3df318": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2044eadebda849809b0b198a63dd7aab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a3b6480e1a1450cb766e3f282963393": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2044eadebda849809b0b198a63dd7aab",
      "placeholder": "​",
      "style": "IPY_MODEL_d25f35b8c4d24c5f9383d9d63292bd6a",
      "value": "100%"
     }
    },
    "4596de04458f4a2e8c401051b37a90cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7d0c6198563d40489201d50116e0d5cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d0ed1d607114bf9a6e276ef6e4b2895": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e9ab673fe59453abf2ca1c77ffc4572",
      "placeholder": "​",
      "style": "IPY_MODEL_7d0c6198563d40489201d50116e0d5cf",
      "value": " 500/500 [04:14&lt;00:00,  2.10epoch/s, loss=0.000182, val_loss=0.000285]"
     }
    },
    "7ffd6e23f42a46f2bbe2926a66b285c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a3b6480e1a1450cb766e3f282963393",
       "IPY_MODEL_81958822c99f4e43964f7ca03974e8af",
       "IPY_MODEL_7d0ed1d607114bf9a6e276ef6e4b2895"
      ],
      "layout": "IPY_MODEL_bffa111964c04858930dc66cebbe95d0"
     }
    },
    "81958822c99f4e43964f7ca03974e8af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_052498b31e404b1eb06e6006dd3df318",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4596de04458f4a2e8c401051b37a90cc",
      "value": 500
     }
    },
    "9e9ab673fe59453abf2ca1c77ffc4572": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bffa111964c04858930dc66cebbe95d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d25f35b8c4d24c5f9383d9d63292bd6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
