{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44acb20d-4df9-444e-83d3-047742275dfb",
   "metadata": {},
   "source": [
    "# Super Computer Usage 101\n",
    "\n",
    "## Accessing Resource:\n",
    "Go to the Illinois [ICRN webpage](https://docs.ncsa.illinois.edu/systems/icrn/)\n",
    "\n",
    "To use GPU resources, select \"A100 GPU 2CPU/8GB\". \n",
    "\n",
    "## Create an environment:\n",
    "\n",
    "The point of a virtual environment is to isolate project dependencies so that different projects can use different package versions without conflict. This creates a \"sandbox\" for each project, containing its own specific Python interpreter and installed libraries, which makes development more organized and reproducible. \n",
    "\n",
    "1. First, shut down all kernels. \n",
    "2. Run the command below in a terminal: \n",
    "- ~: Your home directory, equivalent to /home/NET_ID\n",
    "- --prefix: Telling mamba where to install your environment.\n",
    "```\n",
    "mamba create --prefix ~/myenv python tensorflow[and-cuda]=2.17 ipykernel pytorch pandas seaborn tqdm matplotlib pytorch-cuda -c pytorch -c nvidia -c conda-forge\n",
    "```\n",
    "\n",
    "This will take some time. \n",
    "\n",
    "2. Activate your environment\n",
    "```\n",
    "source activate ~/env_name\n",
    "```\n",
    "3. Run a new kernel session\n",
    "```\n",
    "python -m ipykernel install --user --name=session \n",
    "```\n",
    "Click \"+\" and you will see your session has been added. You may also open the Kernel menu and select Change kernel. You can also learn more about Mamba [here](https://mamba.readthedocs.io/en/latest/index.html)!\n",
    "\n",
    "### Common bash commands\n",
    "- pwd -P show current absolute path\n",
    "- cat print out everything in the file\n",
    "- ls (folder name) show everything in the current folder.\n",
    "- nvidia-smi show the current GPU status.\n",
    "- rm remove a file\n",
    "- source let bash run the script.\n",
    "- cp [dir1] [dir2] copy files from 1 directory to another directory. If copying a folder, use cp -r\n",
    "\n",
    "\n",
    "## What and Why GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba770ec4-cbaa-4bf8-b174-de61e9a77373",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38bdcd0-7761-4697-886c-8d23b8d8bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create two large random tensors\n",
    "a = torch.randn(10000, 10000)\n",
    "b = torch.randn(10000, 10000)\n",
    "\n",
    "# --- 1. CPU Test ---\n",
    "start_time = time.time()\n",
    "c_cpu = a + b\n",
    "cpu_time = time.time() - start_time\n",
    "print(f\"CPU Time: {cpu_time:.6f} seconds\")\n",
    "\n",
    "# --- 2. GPU Test ---\n",
    "# Move data to the GPU (over the PCIe bus)\n",
    "a_gpu = a.to(\"cuda\")\n",
    "b_gpu = b.to(\"cuda\")\n",
    "\n",
    "# We must synchronize to get an accurate time!\n",
    "# This waits for the GPU to finish its work.\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "c_gpu = a_gpu + b_gpu\n",
    "torch.cuda.synchronize()\n",
    "gpu_time = time.time() - start_time\n",
    "\n",
    "print(f\"GPU Time: {gpu_time:.6f} seconds\")\n",
    "print(f\"GPU is {cpu_time/gpu_time:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88986d44-f8ea-486a-8021-7791d000754e",
   "metadata": {},
   "source": [
    "Tip: You can also use %%timeit to time a cell.\n",
    "\n",
    "## GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb92075-b11e-4736-9b34-eec646989139",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "a = np.arange(10**6) \n",
    "np.sum(a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbca127-f1de-417e-a22f-692ebc0bd20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530789e1-a81f-45c3-b577-a8396c2416bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39392d-57a1-4b78-ba61-5c5659f0172f",
   "metadata": {},
   "source": [
    "The GPU is faster because it has thousands of simple cores (for throughput), while the CPU has a few complex cores (for latency)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb4f9e-ae9d-414a-82f8-c3c9d0bf7c7d",
   "metadata": {},
   "source": [
    "## The Real Enemy: PCIe bus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e6a99-0fd7-49c8-8e8b-bcea07dbddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ff8d3-a81e-470b-8e87-610a8cf28359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(time_us):\n",
    "    \"\"\"Converts microseconds to a formatted ms or us string\"\"\"\n",
    "    if time_us == 0:\n",
    "        return \"0.000us\"\n",
    "    if time_us > 1000 or time_us < -1000:\n",
    "        return f\"{time_us / 1000:.3f}ms\"\n",
    "    return f\"{time_us:.3f}us\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b398ab6-2a1e-4d6d-94d2-56fa7df992b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pd(prof):\n",
    "    key_averages = prof.key_averages()\n",
    "    total_self_cpu = prof.key_averages().self_cpu_time_total\n",
    "    total_self_cuda = prof.key_averages().self_cpu_time_total\n",
    "    profiler_data = []\n",
    "    for avg in key_averages:\n",
    "        profiler_data.append({\n",
    "            \"Name\": avg.key,\n",
    "            \n",
    "            # CPU Columns\n",
    "            \n",
    "            #\"Self CPU %\": f\"{avg.self_cpu_time_total / total_self_cpu * 100:.2f}%\" if total_self_cpu > 0 else \"0.00%\",\n",
    "            \"Self CPU\": format_time(avg.self_cpu_time_total),\n",
    "            \"CPU total %\": f\"{avg.cpu_time_total / total_self_cpu * 100:.2f}%\" if total_self_cpu > 0 else \"0.00%\", # Follows profiler's table logic\n",
    "            \"CPU total\": format_time(avg.cpu_time_total),\n",
    "            \"CPU time avg\": format_time(avg.cpu_time_total / avg.count),\n",
    "            \n",
    "            # CUDA Columns\n",
    "            #\"Self CUDA %\": f\"{avg.self_device_time_total / total_self_cuda * 100:.2f}%\" if total_self_cuda > 0 else \"0.00%\",\n",
    "            \"Self CUDA\": format_time(avg.self_device_time_total),\n",
    "            \"CUDA total\": format_time(avg.device_time_total),\n",
    "            \"CUDA time avg\": format_time(avg.device_time_total / avg.count),\n",
    "            \n",
    "            \"# of Calls\": avg.count,\n",
    "            \"_cuda_total_raw\": avg.device_time_total # Internal column just for sorting\n",
    "        })\n",
    "    print(f\"total cpu time:{total_self_cpu}\")\n",
    "    print(f\"total gpu time:{total_self_cuda}\")\n",
    "    return pd.DataFrame(profiler_data).sort_values(by=\"_cuda_total_raw\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db84f3-7a18-450d-8849-9ccc218d72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor on the CPU\n",
    "z_cpu = torch.randn(5000, 5000)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as prof:\n",
    "    # This loop does NO compute, just data transfer\n",
    "    for _ in range(10):\n",
    "        z_gpu = z_cpu.to(\"cuda\")\n",
    "        z_back = z_gpu.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a66042-cf27-4e4c-a3fa-b02ffb69a039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "\n",
    "a_cpu = torch.randn(2000, 2000)\n",
    "b_cpu = torch.randn(2000, 2000)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as prof:\n",
    "    for _ in range(10):\n",
    "        # --- BAD: Move to GPU inside the loop ---\n",
    "        a_gpu = a_cpu.to(\"cuda\")\n",
    "        b_gpu = b_cpu.to(\"cuda\")\n",
    "        \n",
    "        c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "        \n",
    "        # --- BAD: Move back to CPU inside the loop ---\n",
    "        c_cpu = c_gpu.to(\"cpu\")\n",
    "\n",
    "to_pd(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e471db-5303-4bf1-aed2-b2bcab155742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GOOD: Move data ONCE ---\n",
    "a_gpu = a_cpu.to(\"cuda\")\n",
    "b_gpu = b_cpu.to(\"cuda\")\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as prof:\n",
    "    for _ in range(10):\n",
    "        # --- All computation stays on the GPU ---\n",
    "        c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "\n",
    "# --- GOOD: Bring final result back ONCE ---\n",
    "c_cpu = c_gpu.to(\"cpu\")\n",
    "\n",
    "to_pd(prof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054c73d-b78d-4862-8d75-990daa6ef3e8",
   "metadata": {},
   "source": [
    "All the time is spent in cudaMemcpy... This is the data moving across the PCIe bus from RAM to VRAM. Your kernel can be infinitely fast, but you'll still be slow if you're bottlenecked by data transfer.\"\n",
    "\n",
    "## Kernel Launch Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d853c-89ad-48c5-898c-6311f8339d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# --- BAD: 100,000 tiny kernels ---\n",
    "a = torch.randn(1, device='cuda')\n",
    "b = torch.randn(1, device='cuda')\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "for _ in range(100000):\n",
    "    c = a + b  # A new kernel launch every loop!\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Time for 100,000 small launches: {time.time() - start:.6f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4772840a-da63-42e8-8f95-79b70bc7b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GOOD: One big, vectorized kernel ---\n",
    "a = torch.randn(100000, device='cuda')\n",
    "b = torch.randn(100000, device='cuda')\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "c = a + b  # One single kernel launch\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(f\"Time for 1 big launch: {time.time() - start:.6f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa0590-ebda-4b46-8aa2-4224c44107ed",
   "metadata": {},
   "source": [
    "The vectorized (one-launch) version will be dramatically faster, even though it's doing the same amount of math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766259a-d96d-46b5-91c6-5f2fb22207a8",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [ICRN docs](https://docs.ncsa.illinois.edu/systems/icrn/en/latest/index.html)\n",
    "- [Cornell GPU workshop](https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/index)\n",
    "- [LeetGPU-- GPU version of leetcode.](https://leetgpu.com/challenges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d966d9-e39a-49bc-9796-497f5c5481eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
