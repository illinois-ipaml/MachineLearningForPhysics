{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "KyDfK0sCkc3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Wo_YN8EziO1K"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os.path\n",
        "import subprocess\n",
        "import matplotlib.collections\n",
        "import scipy.signal\n",
        "from sklearn import model_selection\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from math import ceil\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    roc_auc_score, average_precision_score\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers for Getting, Loading, and Locating Data"
      ],
      "metadata": {
        "id": "SF_H0ud2kkuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wget_data(url: str):\n",
        "    local_path = './tmp_data'\n",
        "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
        "    rc = None\n",
        "    while rc is None:\n",
        "      line = p.stderr.readline().strip('\\n')\n",
        "      if len(line) > 0:\n",
        "        print(line)\n",
        "      rc = p.poll()\n",
        "\n",
        "def locate_data(name, check_exists=True):\n",
        "    local_path='./tmp_data'\n",
        "    path = os.path.join(local_path, name)\n",
        "    if check_exists and not os.path.exists(path):\n",
        "        raise RuxntimeError('No such data file: {}'.format(path))\n",
        "    return path"
      ],
      "metadata": {
        "id": "4tb2EM5QktKd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data"
      ],
      "metadata": {
        "id": "GSV0jrWfk0WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/circles_data.hf5')\n",
        "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/circles_targets.hf5')\n",
        "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/spectra_data.hf5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVtqVn8Lk4HP",
        "outputId": "70377d50-a1bf-41e9-db48-01c0e24aeaa5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘./tmp_data/circles_data.hf5’ already there; not retrieving.\n",
            "File ‘./tmp_data/circles_targets.hf5’ already there; not retrieving.\n",
            "File ‘./tmp_data/spectra_data.hf5’ already there; not retrieving.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:Orange\">Neural Network Architectures for Deep Learning</span>"
      ],
      "metadata": {
        "id": "L4XAS_fzlQBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We previously looked at the basic building blocks of a neural network. Here we will put it together to show the process of training and evaluating a simple neural network that, for networks with additional layers, would illustrate deep learning.\n",
        "\n",
        "We will later look at some other novel architectures that are currently driving the [deep-learning revolution](https://www.techrepublic.com/article/the-deep-learning-revolution-how-understanding-the-brain-will-let-us-supercharge-ai/):\n",
        "\n",
        "- Convolutional networks\n",
        "\n",
        " - Recurrent networks\n",
        "\n",
        "We conclude with some reflections on where \"deep learning\" is headed.\n",
        "\n",
        "We learned about tensorflow and PyTorch last time. Here, will focus on PyTorch, which has become the more common toolbox for deep learning."
      ],
      "metadata": {
        "id": "xF4ycfBelcPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Data"
      ],
      "metadata": {
        "id": "MIH-WSRZoifS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create a dataset where we split the circles data into train(400) and test (100) datasets."
      ],
      "metadata": {
        "id": "xO7J91teoLds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_hdf(locate_data('circles_data.hf5'))\n",
        "y = pd.read_hdf(locate_data('circles_targets.hf5'))\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "    X, y, test_size=100, random_state=123)"
      ],
      "metadata": {
        "id": "x9_UaCC1onf5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to NumPy float32\n",
        "Xtr = X_train.astype(\"float32\").to_numpy()\n",
        "ytr = y_train.squeeze().astype(\"float32\").to_numpy()\n",
        "Xte = X_test.astype(\"float32\").to_numpy()\n",
        "yte = y_test.squeeze().astype(\"float32\").to_numpy()"
      ],
      "metadata": {
        "id": "077QVTUEpiBD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the array sizes"
      ],
      "metadata": {
        "id": "vlSjb5ZIqEzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(Xtr), Xtr.shape, Xtr.dtype)\n",
        "print(type(ytr), ytr.shape, ytr.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nll5WM5yqKv9",
        "outputId": "7bd86fa8-438a-4b67-982b-f3d199f33734"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> (400, 2) float32\n",
            "<class 'numpy.ndarray'> (400,) float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set some Torch variables and  DataLoader"
      ],
      "metadata": {
        "id": "7PaD5OZtqPDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")  # or \"cuda\" if available and desired\n",
        "\n",
        "Xtr_t = torch.from_numpy(Xtr)\n",
        "ytr_t = torch.from_numpy(ytr)\n",
        "Xte_t = torch.from_numpy(Xte)\n",
        "yte_t = torch.from_numpy(yte)\n",
        "\n",
        "batch_size = 50\n",
        "train_ds = TensorDataset(Xtr_t, ytr_t)\n",
        "g = torch.Generator().manual_seed(123)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, generator=g)"
      ],
      "metadata": {
        "id": "qOgdpZoArCEC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a simple model"
      ],
      "metadata": {
        "id": "Eq5jlFBtolwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = Xtr.shape[1]\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D, 4),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(4, 1),\n",
        "    nn.Sigmoid(),\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "NRiGm-ycr5mg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the optmizer. Here we use Adagrad, which is gradient descent with a learning rate that adapts based on past gradients (alternatively, we could could use Adam like before). For the loss function, we use binary classification entropy (BCE)."
      ],
      "metadata": {
        "id": "Fo0vg_DirzBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adagrad(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    initial_accumulator_value=0.1,\n",
        "    eps=1e-10,\n",
        ")\n",
        "criterion = nn.BCELoss(reduction=\"mean\")  # average loss per batch"
      ],
      "metadata": {
        "id": "RO3hVTLvsO4P"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model"
      ],
      "metadata": {
        "id": "AFeeV8Jmswri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "np.random.seed(123)\n",
        "\n",
        "# Train for 5000 updates\n",
        "steps_per_epoch = ceil(len(Xtr) / batch_size)\n",
        "target_steps = 5000\n",
        "epochs = ceil(target_steps / steps_per_epoch)\n",
        "\n",
        "model.train()\n",
        "global_step = 0\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device).view(-1, 1)\n",
        "\n",
        "        # forward propagation\n",
        "        probs = model(xb)                # (B,1) after final Sigmoid\n",
        "        loss = criterion(probs, yb)\n",
        "\n",
        "        # backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        global_step += 1\n",
        "        if global_step >= target_steps:\n",
        "            break\n",
        "    if global_step >= target_steps:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "QkRYcb6gsydI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at some performance metrics evaluated on the test data set"
      ],
      "metadata": {
        "id": "6PI8TT7GtNkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    probs_te = model(Xte_t.to(device)).cpu().numpy().squeeze()  # (N_test,)\n",
        "\n",
        "# Thresholded predictions at 0.5\n",
        "preds_te = (probs_te >= 0.5).astype(np.float32)\n",
        "\n",
        "# Core metrics\n",
        "acc  = accuracy_score(yte, preds_te)\n",
        "prec = precision_score(yte, preds_te, zero_division=0)\n",
        "rec  = recall_score(yte, preds_te, zero_division=0)\n",
        "\n",
        "# AUCs (need both classes present; handle edge cases)\n",
        "try:\n",
        "    auc_roc = roc_auc_score(yte, probs_te)\n",
        "except ValueError:\n",
        "    auc_roc = float(\"nan\")\n",
        "try:\n",
        "    auc_pr  = average_precision_score(yte, probs_te)\n",
        "except ValueError:\n",
        "    auc_pr = float(\"nan\")\n",
        "\n",
        "# Binary cross-entropy on test set (mean and sum)\n",
        "eps = 1e-7\n",
        "p = np.clip(probs_te, eps, 1 - eps)\n",
        "avg_loss = float(-np.mean(yte * np.log(p) + (1 - yte) * np.log(1 - p)))\n",
        "sum_loss = float(avg_loss * len(yte))\n",
        "\n",
        "label_mean = float(np.mean(yte))\n",
        "pred_mean  = float(np.mean(probs_te))\n",
        "acc_base   = float(max(label_mean, 1.0 - label_mean))\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\":             float(acc),\n",
        "    \"accuracy_baseline\":    acc_base,\n",
        "    \"auc\":                  float(auc_roc),\n",
        "    \"auc_precision_recall\": float(auc_pr),\n",
        "    \"average_loss\":         avg_loss,\n",
        "    \"label/mean\":           label_mean,\n",
        "    \"loss\":                 sum_loss,\n",
        "    \"precision\":            float(prec),\n",
        "    \"prediction/mean\":      pred_mean,\n",
        "    \"recall\":               float(rec),\n",
        "    \"global_step\":          int(global_step),\n",
        "}\n",
        "\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLxwBJEctRA1",
        "outputId": "97dc7e76-bc52-4e75-a1d9-255ab0e15cb9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 1.0, 'accuracy_baseline': 0.5299999713897705, 'auc': 1.0, 'auc_precision_recall': 1.0, 'average_loss': 0.07818835973739624, 'label/mean': 0.5299999713897705, 'loss': 7.818835973739624, 'precision': 1.0, 'prediction/mean': 0.5324277281761169, 'recall': 1.0, 'global_step': 5000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at the weights and biases in each layer"
      ],
      "metadata": {
        "id": "ZNWx3PtJuVsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_linear = model[0]  # nn.Linear(D,4)\n",
        "W1 = first_linear.weight.detach().cpu().numpy()  # shape (4, D)\n",
        "b1 = first_linear.bias.detach().cpu().numpy()    # shape (4,)\n",
        "\n",
        "second_linear = model[2]  # nn.Linear(4,1)\n",
        "W2 = second_linear.weight.detach().cpu().numpy() # shape (1, 4)\n",
        "b2 = second_linear.bias.detach().cpu().numpy()   # shape (1,)\n",
        "\n",
        "print(\"First layer W (shape {}):\\n\".format(W1.shape), W1)\n",
        "print(\"First layer b (shape {}):\\n\".format(b1.shape), b1)\n",
        "print(\"Second layer W (shape {}):\\n\".format(W2.shape), W2)\n",
        "print(\"Second layer b (shape {}):\\n\".format(b2.shape), b2)\n",
        "\n",
        "for j in range(W1.shape[0]):  # 4 hidden units\n",
        "    print(f\"\\nHidden unit {j}:\")\n",
        "    for i, col in enumerate(X_train.columns):\n",
        "        print(f\"  {col:20s}  {W1[j, i]: .6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW6hQhbBub9s",
        "outputId": "b2226a1d-d8f7-4827-a271-6ab1f08364d8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First layer W (shape (4, 2)):\n",
            " [[-0.4580419  2.366767 ]\n",
            " [-1.4072431  2.9816208]\n",
            " [-1.656121   2.6113465]\n",
            " [-0.6646377  3.5004601]]\n",
            "First layer b (shape (4,)):\n",
            " [ 1.2029158  2.1204805 -2.8945494 -3.1695764]\n",
            "Second layer W (shape (1, 4)):\n",
            " [[ 1.102744   1.8003888 -2.1245427 -2.530982 ]]\n",
            "Second layer b (shape (1,)):\n",
            " [-0.9632067]\n",
            "\n",
            "Hidden unit 0:\n",
            "  x0                    -0.458042\n",
            "  x1                     2.366767\n",
            "\n",
            "Hidden unit 1:\n",
            "  x0                    -1.407243\n",
            "  x1                     2.981621\n",
            "\n",
            "Hidden unit 2:\n",
            "  x0                    -1.656121\n",
            "  x1                     2.611346\n",
            "\n",
            "Hidden unit 3:\n",
            "  x0                    -0.664638\n",
            "  x1                     3.500460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Outlook"
      ],
      "metadata": {
        "id": "8LW2PeBOjysD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The depth of \"deep learning\" comes primarily from network architectures that stack many layers. In another sense, deep learning is very shallow since it often performs well using little to no specific knowledge about the problem it is solving, using generic building blocks.\n",
        "\n",
        "The field of modern deep learning started around 2012 when the architectures described above were first used successfully, and the necessary large-scale computing and datasets were available. Massive neural networks are now the state of the art for many benchmark problems, including image classification, speech recognition and language translation.\n",
        "\n",
        "However, less than a decade into the field, there are signs that deep learning is reaching its limits. Some of the pioneers and others are taking a critical look at the current state of the field:\n",
        "\n",
        "- Deep learning does not use data efficiently.\n",
        "\n",
        "- Deep learning does not integrate prior knowledge.\n",
        "\n",
        "- Deep learning often give correct answers but without associated uncertainties.\n",
        "\n",
        "- Deep learning applications are hard to interpret and transfer to related problems.\n",
        "\n",
        "- Deep learning is excellent at learning stable input-output mappings but does not cope well with varying conditions.\n",
        "\n",
        "- Deep learning cannot distinguish between correlation and causation.\n",
        "\n",
        "These are mostly concerns for the future of neural networks as a general model for artificial intelligence, but they also limit the potential of scientific applications.\n",
        "\n",
        "However, there are many challenges in scientific data analysis and interpretation that could benefit from deep learning approaches, so I encourage you to follow the field and experiment. Through this course, you now have a pretty solid foundation in data science and machine learning to further your studies toward more advanced and current topics!"
      ],
      "metadata": {
        "id": "ULii9nAhjsfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgments"
      ],
      "metadata": {
        "id": "bXypr8CnjfVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial version: Mark Neubauer\n",
        "\n",
        "Updates: Aaron Pearlman\n",
        "\n",
        "© Copyright 2024"
      ],
      "metadata": {
        "id": "AxU4ZWOoi9d9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Je3lpgcki3t_"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}