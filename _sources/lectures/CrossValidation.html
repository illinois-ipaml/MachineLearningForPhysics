

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Cross Validation &#8212; PHYS 503</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_sources/lectures/CrossValidation';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Homework 08: Cross Validation" href="../homework/Homework_08.html" />
    <link rel="prev" title="Artificial Intelligence and Machine Learning" href="Learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="PHYS 503 - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="PHYS 503 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    <span style="color:Blue">Instrumentation Physics: Applications of Machine Learning</span>
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Machine Learning and Data Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_01.html"><span style="color: blue;"><b>Course Introduction</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1vq4b3zxrhEMJbfeCH52hufBbXvTwhufEXdSs8mWY2ec/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="JupyterNumpy.html">Jupyter Notebooks and Numerical Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="Pandas.html">Handling Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_01.html">Homework 01: Numerical python and data handling</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_02.html"><span style="color: blue;"><b>Visualizing &amp; Finding Structure in Data</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1_LstEfghjdZUheyrqbjx4PK9y1J0cN-_hOdCInTldp8/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Visualization.html">Visualizing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Clustering.html">Finding Structure in Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_02.html">Homework 02: Visualization and Expectation-Maximization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_03.html"><span style="color: blue;"><b>Dimensionality, Linearity and Kernel Functions</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1x4bWQr7kEAh6Z6L7iaLdaNiY6SDTHtvHxzvjFM1wYnE/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Dimensionality.html">Measuring and Reducing Dimensionality</a></li>
<li class="toctree-l2"><a class="reference internal" href="Nonlinear.html">Adapting Linear Methods to Non-Linear Data and Kernel Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_03.html">Homework 03: K-means and Principle Component Analysis</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability and Statistics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_04.html"><span style="color: blue;"><b>Probability Theory</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1qW-gCHY3bQMmB0-klM0crTD9020UG3DTlT_awlOhy2A/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityTheory.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityDistributions.html">Important Probability Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_04.html">Homework 04: Probability Theory and Common Distributions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_05.html"><span style="color: blue;"><b>Kernel Density Estimation and Statistics</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1XZoeBdXzhcfezIbUrH0a9-4-QmM-5iNksLCB7X4q1wI/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="DensityEstimation.html">Estimating Probability Density from Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Statistics.html">Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="MonteCarloSamplingMethods.html">Monte Carlo and Sampling Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_05.html">Homework 05: Kernel Density Estimation, Covariance and Correlation</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_06.html"><span style="color: blue;"><b>Bayesian Statistics and Markov Chain Monte Carlo</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1L_lz0WbrrUu9qDPnKxYu5S_fCXKjl01Mrs2RnHN5_6E/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="BayesianInference.html">Bayesian Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="MarkovChainMonteCarlo.html">Markov Chain Monte Carlo in Practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_06.html">Homework 06: Bayesian Statistics and MCMC</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_01.html"><span style="color: blue;"><b>Project 01</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_HiggsTauTau.html">Higgs Boson Decaying to Tau Leptons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_ExoticParticles.html">Searching for Exotic Particles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_GalaxyZoo.html">Galaxy Zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_NuclearGeometryQGP.html">Nuclear Geometry and Characterization of the Quark Gluon Plasma</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_AberratedImages.html">Aberrated Image Recovery of Ultracold Atoms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_DarkEnergySurvey.html">Dark Energy Survey</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_07.html"><span style="color: blue;"><b>Stochastic Processes, Markov Chains &amp; Variational Inference</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/11Mzc9rBUcnEh_D3SKeDUoCW-iwIA9ilcxsx_4yuu32A/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="MarkovChains.html">Stochastic Processes and Markov-Chain Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="VariationalInference.html">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_07.html">Homework 07: Markov Chains</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_08.html"><span style="color: blue;"><b>Optimization and Model Selection</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1KshmOwKTWptL-3PASHrW6WT2PkH2ltU-XwoYOflhKQk/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Optimization.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="ModelSelection.html">Bayesian Model Selection</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supervised Learning &amp; Cross Validation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../Week_09.html"><span style="color: blue;"><b>Learning &amp; Cross Validation</b></span></a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1a2cjkREM0LYxRjrLwrfHPTotM0n_CgVrZ_WQjEyc_Jc/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Learning.html">Artificial Intelligence and Machine Learning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Cross Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_08.html">Homework 08: Cross Validation</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Artificial Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_10.html"><span style="color: blue;"><b>Supervised Learning &amp; Artificial Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1vyg7eSo5XaUAtYDwxmLY5qeUrwKxKqJcEEHPB40yVpE/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="SupervisedLearning.html">Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="ArtificialNeuralNetworks.html">Artificial Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_09.html">Homework 09: Artificial Neural Networks</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_11.html"><span style="color: blue;"><b>Deep Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1RnFI0k15C_m2j43QtFDGCFRBCcQ-Rx6EG-9U3EumOHc/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="DeepLearning.html">Deep Learning</a></li>


<li class="toctree-l2"><a class="reference internal" href="ConvolutionalRecurrentNeuralNetworks.html">Convolutional and Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_10.html">Homework 10: Forecasting Projectile Motion with Recurrent Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_12.html"><span style="color: blue;"><b>Graph Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1fxZdnCU_8pWocQbjMQ5HUOSUL3MgbCyg3xFWxfeNaeY/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="GraphNeuralNetworks.html">Graph Neural Networks</a></li>






</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_02.html"><span style="color: blue;"><b>Project 02</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_HiggsTauTau.html">Higgs Boson Decaying to Tau Leptons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_ExoticParticles.html">Searching for Exotic Particles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_GalaxyZoo.html">Galaxy Zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_NuclearGeometryQGP.html">Nuclear Geometry and Characterization of the Quark Gluon Plasma</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_AberratedImages.html">Aberrated Image Recovery of Ultracold Atoms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_GravitationalWaves.html">Detection of Gravitational Waves</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_13.html"><span style="color: blue;"><b>AI Explainablility and Uncertainty Quantification</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1jGxr3j5t7Ahi3Ai6501dVJXOIzvlYMGhe9ZDqHlcfjo/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="AIExplainabilityUncertaintyQuantification.html">AI Explainability and Uncertainty Quantification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_11.html"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Explainable AI and Accelerated Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_14.html"><span style="color: blue;"><b>Unsupervised Learning and Anomaly Detection</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1-_9DcO71v6fQN1kNhKRH2d2iSjhs_Ddi2wLxiXy6RNg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="UnsupervisedLearningAnomalyDetection.html">Unsupervised Learning and Anomaly Detection</a></li>

</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Accelerated Machine Learning and Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_15.html"><span style="color: blue;"><b>Accelerated Machine Learning and Inference</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1FbBa1MC9zhnxwbiiUJcxUiJ1hZF27JvwxS_jdTl7_gs/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="AcceleratedML.html">Accelerated Machine Learning and Inference</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/illinois-ipaml/MachineLearningForPhysics/blob/main/_sources/lectures/CrossValidation.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/illinois-ipaml/MachineLearningForPhysics" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/_sources/lectures/CrossValidation.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Cross Validation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-model-selection-with-cross-validation-span"><span style="color:Orange">Model Selection With Cross Validation</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-train-vs-validate-vs-test-data-span"><span style="color:Lightgreen">Train vs. Validate vs. Test Data</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-cross-validation-cv-process-span"><span style="color:Lightgreen">Cross Validation (CV) Process</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-overfitting-and-generalization-span"><span style="color:Lightgreen">Overfitting and Generalization</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-train-test-split-span"><span style="color:Lightgreen">Train-Test Split</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-k-folding-span"><span style="color:Lightgreen">K-Folding</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-hyperparameter-grid-search-span"><span style="color:Lightgreen">Hyperparameter Grid Search</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-comparison-with-bayesian-evidence-using-pymc-span"><span style="color:Lightgreen">Comparison with Bayesian Evidence Using PyMC</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-the-pymc-model-and-mcmc-sampling-span"><span style="color:Orange">The PyMC Model and MCMC Sampling</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-gmm-based-log-evidence-calculation-span"><span style="color:Orange">GMM-Based Log Evidence Calculation</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#a-span-style-color-purple-log-joint-calculation-span-the-numerator">A. <span style="color: Purple">Log Joint Calculation</span> (The Numerator)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#b-span-style-color-purple-log-posterior-estimation-span-the-denominator">B. <span style="color: Purple">Log Posterior Estimation</span> (The Denominator)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#c-span-style-color-purple-final-estimate-span">C. <span style="color: Purple">Final Estimate</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-appendix-comparison-with-bayesian-evidence-using-emcee-span-alternative-implementation-for-reference"><span style="color:Lightgreen">Appendix: Comparison with Bayesian Evidence using Emcee</span> (alternative implementation for reference)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="cross-validation">
<h1>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">()</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">linear_model</span><span class="p">,</span> <span class="n">mixture</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="span-style-color-orange-model-selection-with-cross-validation-span">
<h2><span style="color:Orange">Model Selection With Cross Validation</span><a class="headerlink" href="#span-style-color-orange-model-selection-with-cross-validation-span" title="Permalink to this heading">#</a></h2>
<p>Recall that there are three main types of learning:</p>
<ul class="simple">
<li><p>Learning model parameters from data.</p></li>
<li><p>Learning to predict new data (unsupervised learning).</p></li>
<li><p>Learning to predict target features in new data (supervised learning).</p></li>
</ul>
<p>All three types of learning require an assumed model with associated parameters, and predicting new data is only possible after learning model parameters from old data.</p>
<p>Since all types of learning assume a model, they must all solve the meta-problem of comparing competing models. The Bayesian evidence <span class="math notranslate nohighlight">\(P(D\mid M)\)</span> is our primary quantitative tool for comparing how well different models explain the same data.  When we are primarily interested in a model’s ability to generalize and predict new data, <em><strong><span style="color:violet">cross validation</span></strong></em> is a useful alternative.</p>
<section id="span-style-color-lightgreen-train-vs-validate-vs-test-data-span">
<h3><span style="color:Lightgreen">Train vs. Validate vs. Test Data</span><a class="headerlink" href="#span-style-color-lightgreen-train-vs-validate-vs-test-data-span" title="Permalink to this heading">#</a></h3>
<p>During an ML model development and evaluation, various types of data are utilized. Below, we review these types of data, their purpose and the differences between each in how they are used.</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/CrossValidation-splits.png" width=800></img>
</div><p><span style="color:Tan">Training Dataset</span>: The sample of data used to fit the model.</p>
<ul class="simple">
<li><p>The actual dataset that we use to train the ML model under development (weights and biases in the case of a Neural Network). The model sees and learns from this data. Training data are collections of examples or samples that are used to “teach” or “train” the model. The model uses a training data set to understand the patterns and relationships within the data, thereby learning to make predictions or decisions without being explicitly programmed to perform a specific task.</p></li>
</ul>
<p><span style="color:Tan">Validation Dataset</span>: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset (i.e. a “trained model”) while tuning model hyperparameters. The evaluation becomes more biased as more skill on the validation dataset is incorporated into the model configuration.</p>
<ul class="simple">
<li><p>It is still possible to tune and control the model at this stage. Working on validation data is used to assess the model performance and fine-tune the parameters of the model. This becomes an iterative process wherein the model learns from the training data and is then validated and fine-tuned on the validation set. A validation dataset tells us how well the model is learning and adapting, allowing for adjustments and optimizations to be made to the model’s parameters or hyperparameters before it’s finally put to the test. So the validation set affects a model, but only indirectly.</p>
<ul>
<li><p>Note that not all models require validation sets. Some experts consider that ML models with no hyperparameters or those that do not have tuning options do not need a validation set. Still, in most practical applications, validation sets play a crucial role in ensuring the model’s robustness and performance.</p></li>
</ul>
</li>
</ul>
<p><span style="color:Tan">Test Dataset</span>: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.</p>
<ul class="simple">
<li><p>The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). It is a separate sample, an unseen data set, to provide an unbiased final evaluation of a model. Its primary purpose is to offer a fair and final assessment of how the model would perform when it encounters new data in a live, operational environment.</p>
<ul>
<li><p>The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner).</p></li>
<li><p>Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world on data it has never seen before. The inputs in the test data are similar to the previous stages but not the same data.</p></li>
</ul>
</li>
</ul>
</section>
<section id="span-style-color-lightgreen-cross-validation-cv-process-span">
<h3><span style="color:Lightgreen">Cross Validation (CV) Process</span><a class="headerlink" href="#span-style-color-lightgreen-cross-validation-cv-process-span" title="Permalink to this heading">#</a></h3>
<p>Cross validation is a process that provides the ability to estimate model performance on unseen data not used while training. It is a systematic process that can involve tuning the model hyperparameters, testing different properties of the overall datasets, and iterating the training process. There are many variations of the CV process - we will study primarily the <span style="color:violet">K-folds method</span> in this lecture. We will show some examples using random sampling (called “folds”) of these types of data to illustrate the process. For simplicity, we will focus on the test and train data and not consider validation data explicitly in the following.</p>
<p>The figure below summarizes the data splitting and process:</p>
<div>
<img src="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/CrossValidation-process.png" width=600></img>
</div><p>The basic idea of <em><strong><span style="color:violet">cross validation</span></strong></em> is to:</p>
<ol class="arabic simple">
<li><p>split the observed data into separate training and test datasets</p></li>
<li><p>learn the model from the training dataset</p></li>
<li><p>measure the model’s ability to predict the test dataset</p></li>
<li><p>repeat the steps above with different splits (“folds”) and combine the results.</p></li>
</ol>
</section>
<section id="span-style-color-lightgreen-overfitting-and-generalization-span">
<h3><span style="color:Lightgreen">Overfitting and Generalization</span><a class="headerlink" href="#span-style-color-lightgreen-overfitting-and-generalization-span" title="Permalink to this heading">#</a></h3>
<p>Generate some data consisting of 2D points along the path of a projectile, where <span class="math notranslate nohighlight">\(x\)</span> is measured with negligible error and <span class="math notranslate nohighlight">\(y\)</span> has a known error <span class="math notranslate nohighlight">\(\sigma_y\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xlo</span><span class="p">,</span> <span class="n">xhi</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span>
<span class="n">poly_coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">poly_coefs</span><span class="p">,</span> <span class="p">[</span><span class="n">X</span> <span class="o">**</span> <span class="mi">0</span><span class="p">,</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span> <span class="o">**</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">sigma_y</span> <span class="o">=</span> <span class="mf">0.2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">xlo</span><span class="p">,</span> <span class="n">xhi</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">gen</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">sigma_y</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<p>Compare results with different numbers of samples from the same model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">plotXy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">fits</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xlo</span><span class="p">,</span> <span class="n">xhi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_grid</span><span class="p">)),</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fit</span> <span class="ow">in</span> <span class="n">fits</span><span class="p">:</span>
        <span class="n">y_fit</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_grid</span><span class="p">,</span> <span class="n">y_fit</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
    <span class="n">ylo</span><span class="p">,</span> <span class="n">yhi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">yhi</span> <span class="o">-</span> <span class="n">ylo</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylo</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">dy</span><span class="p">,</span> <span class="n">yhi</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">dy</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plotXy</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plotXy</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/6b7124b1f3179704f5889d867e694849038fc535ae6f5966ba790a6694a99295.png" src="../../_images/6b7124b1f3179704f5889d867e694849038fc535ae6f5966ba790a6694a99295.png" />
</div>
</div>
<p>The family of competing models we consider are polynomials of different degrees <span class="math notranslate nohighlight">\(P\)</span>,</p>
<div class="math notranslate nohighlight">
\[ \Large
y(x) = \sum_{k=0}^P\, c_k x^k \; ,
\]</div>
<p>each with <span class="math notranslate nohighlight">\(P+1\)</span> parameters.  The true model that the datasets were generated from have <span class="math notranslate nohighlight">\(P=2\)</span>.</p>
<p>Use sklearn <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">LinearRegression</a> to implement this fit after expanding the features with <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">PolynomialFeatures</a>,</p>
<div class="math notranslate nohighlight">
\[ \Large
x \; \rightarrow\; \{ x^0, x^1, \ldots, x^P \} \; ,
\]</div>
<p>and combining the preprocessing and regression steps into a <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">Pipeline</a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">poly_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degree</span><span class="p">):</span>
    <span class="n">degree_is_zero</span> <span class="o">=</span> <span class="p">(</span><span class="n">degree</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="n">degree_is_zero</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="ow">not</span> <span class="n">degree_is_zero</span><span class="p">))])</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Compare fits with <span class="math notranslate nohighlight">\(P = 0, 1, 2, 14\)</span> to each dataset. Note that <span class="math notranslate nohighlight">\(P=14\)</span> is an extreme case of overfitting to the smaller dataset, with the model passing exactly through each sample with large oscillations between them.  Similarly, <span class="math notranslate nohighlight">\(P=1\)</span> underfits the larger dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plotXy</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>
<span class="n">plotXy</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="mi">14</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e2f5a40daf64b9d8cde4f05844caee40b49c07133df18205a91cb466f57ca8b5.png" src="../../_images/e2f5a40daf64b9d8cde4f05844caee40b49c07133df18205a91cb466f57ca8b5.png" />
</div>
</div>
</section>
<section id="span-style-color-lightgreen-train-test-split-span">
<h3><span style="color:Lightgreen">Train-Test Split</span><a class="headerlink" href="#span-style-color-lightgreen-train-test-split-span" title="Permalink to this heading">#</a></h3>
<p>The <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a> function picks a random fraction of the observed data to hold back when learning the model and then use for latest testing.</p>
<p>Note that since train/test splitting involves random numbers, you will need to pass around a random state object for reproducible results.</p>
<p>The plots below show 20% of the data reserved for testing (red points) with a <span class="math notranslate nohighlight">\(P=2\)</span> fit to the training data superimposed. The primary sklearn test metric for regression problems is the <a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination</a> <span class="math notranslate nohighlight">\(R^2\)</span>, for which the goal is <span class="math notranslate nohighlight">\(R^2 = 1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">test_fraction</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_fraction</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">gen</span><span class="p">)</span>
    <span class="n">train_fit</span> <span class="o">=</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
    <span class="n">plotXy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">train_fit</span><span class="p">)</span>
    <span class="n">test_R2</span> <span class="o">=</span> <span class="n">train_fit</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;$R^2=</span><span class="si">{:.2f}</span><span class="s1">$&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_R2</span><span class="p">),</span> <span class="n">transform</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;x-large&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_test_split</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">train_test_split</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e072786616b50977538488c2debfbdaf1c12a297634dd4994c270b125783c303.png" src="../../_images/e072786616b50977538488c2debfbdaf1c12a297634dd4994c270b125783c303.png" />
</div>
</div>
<p>There is no rigorous procedure for setting an optimum test fraction, and anything between 0.1 and 0.5 would be reasonable (and the sklearn default is 0.25).  A larger test fraction improves the reliability of the test metric but decreases the reliability of the model being tested.  As always, more data always helps and reduces your sensitivity to the training fraction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_fraction_scan</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">test_fractions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">)</span>
    <span class="n">R2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_fractions</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">test_fraction</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_fractions</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(((</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">),</span> <span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">))):</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_fraction</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">gen</span><span class="p">)</span>
            <span class="n">fit</span> <span class="o">=</span> <span class="n">poly_fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
            <span class="n">R2</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_fractions</span><span class="p">,</span> <span class="n">R2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;o:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$N = </span><span class="si">{}</span><span class="s1">$&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ya</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_fractions</span><span class="p">,</span> <span class="n">R2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$N = </span><span class="si">{}</span><span class="s1">$&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">yb</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Test fraction&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Test score $R^2$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">test_fraction_scan</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/70e5242acfe1fb502cb43d9800cefec7ca3d3d5e7416a76a4eb436fb51a2684b.png" src="../../_images/70e5242acfe1fb502cb43d9800cefec7ca3d3d5e7416a76a4eb436fb51a2684b.png" />
</div>
</div>
</section>
<section id="span-style-color-lightgreen-k-folding-span">
<h3><span style="color:Lightgreen">K-Folding</span><a class="headerlink" href="#span-style-color-lightgreen-k-folding-span" title="Permalink to this heading">#</a></h3>
<p>Cross validation goes beyond a simple train-test split by repeating the split multiple times and combining the (correlated) results. There are different strategies for picking the different splits, but <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html">K-folding</a> is a good all-around choice:</p>
<ul class="simple">
<li><p>Specify the number <span class="math notranslate nohighlight">\(k\)</span> of splits (folds) to use.</p></li>
<li><p>The data is split into <span class="math notranslate nohighlight">\(k\)</span> (almost) equal independent subsets.</p></li>
<li><p>Each subset is used for testing once, with the remaining subsets used for training.</p></li>
</ul>
<p>The result is <span class="math notranslate nohighlight">\(k\)</span> different train-test splits using a test fraction <span class="math notranslate nohighlight">\(1/k\)</span>.  For example, with <span class="math notranslate nohighlight">\(N=10\)</span> samples and <span class="math notranslate nohighlight">\(k=3\)</span> folds, the subsets are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kfold</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(np.int64(0), np.int64(1), np.int64(2), np.int64(3)),
 (np.int64(4), np.int64(5), np.int64(6)),
 (np.int64(7), np.int64(8), np.int64(9))]
</pre></div>
</div>
</div>
</div>
<p>The <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html">cross_validate</a> function automates the k-folding and scoring process, and outputs both train and test <span class="math notranslate nohighlight">\(R^2\)</span> scores, as well as CPU times, for each split:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cross_validate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">degree</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">))])</span>
    <span class="n">kfold</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">cross_validate</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))]</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cross_validate</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(0, 1, 2, 3, 4)</th>
      <td>0.000949</td>
      <td>0.000399</td>
      <td>0.404038</td>
      <td>0.731105</td>
    </tr>
    <tr>
      <th>(5, 6, 7, 8, 9)</th>
      <td>0.000559</td>
      <td>0.000299</td>
      <td>-2.232860</td>
      <td>0.632223</td>
    </tr>
    <tr>
      <th>(10, 11, 12, 13, 14)</th>
      <td>0.000536</td>
      <td>0.000270</td>
      <td>-0.352504</td>
      <td>0.518438</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>With enough data, you can use a large number of K-folds but remember that they are highly correlated so you are not increasing the useful information as much as you might think. The sklearn default number of splits is 3.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cross_validate</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)</th>
      <td>0.000862</td>
      <td>0.000358</td>
      <td>0.574221</td>
      <td>0.613270</td>
    </tr>
    <tr>
      <th>(10, 11, 12, 13, 14, 15, 16, 17, 18, 19)</th>
      <td>0.000567</td>
      <td>0.000314</td>
      <td>0.644016</td>
      <td>0.608664</td>
    </tr>
    <tr>
      <th>(20, 21, 22, 23, 24, 25, 26, 27, 28, 29)</th>
      <td>0.000547</td>
      <td>0.000291</td>
      <td>0.559857</td>
      <td>0.614704</td>
    </tr>
    <tr>
      <th>(30, 31, 32, 33, 34, 35, 36, 37, 38, 39)</th>
      <td>0.000526</td>
      <td>0.000290</td>
      <td>0.382087</td>
      <td>0.619587</td>
    </tr>
    <tr>
      <th>(40, 41, 42, 43, 44, 45, 46, 47, 48, 49)</th>
      <td>0.000525</td>
      <td>0.000284</td>
      <td>0.741344</td>
      <td>0.601154</td>
    </tr>
    <tr>
      <th>(50, 51, 52, 53, 54, 55, 56, 57, 58, 59)</th>
      <td>0.000520</td>
      <td>0.000285</td>
      <td>0.415899</td>
      <td>0.625590</td>
    </tr>
    <tr>
      <th>(60, 61, 62, 63, 64, 65, 66, 67, 68, 69)</th>
      <td>0.000699</td>
      <td>0.000328</td>
      <td>0.675124</td>
      <td>0.601310</td>
    </tr>
    <tr>
      <th>(70, 71, 72, 73, 74, 75, 76, 77, 78, 79)</th>
      <td>0.000557</td>
      <td>0.000298</td>
      <td>0.745460</td>
      <td>0.599734</td>
    </tr>
    <tr>
      <th>(80, 81, 82, 83, 84, 85, 86, 87, 88, 89)</th>
      <td>0.000547</td>
      <td>0.000297</td>
      <td>0.366792</td>
      <td>0.618551</td>
    </tr>
    <tr>
      <th>(90, 91, 92, 93, 94, 95, 96, 97, 98, 99)</th>
      <td>0.000484</td>
      <td>0.000269</td>
      <td>0.498276</td>
      <td>0.614549</td>
    </tr>
    <tr>
      <th>(100, 101, 102, 103, 104, 105, 106, 107, 108, 109)</th>
      <td>0.000521</td>
      <td>0.000280</td>
      <td>0.752076</td>
      <td>0.595514</td>
    </tr>
    <tr>
      <th>(110, 111, 112, 113, 114, 115, 116, 117, 118, 119)</th>
      <td>0.000518</td>
      <td>0.000284</td>
      <td>0.604196</td>
      <td>0.610233</td>
    </tr>
    <tr>
      <th>(120, 121, 122, 123, 124, 125, 126, 127, 128, 129)</th>
      <td>0.000521</td>
      <td>0.000341</td>
      <td>-0.292603</td>
      <td>0.630345</td>
    </tr>
    <tr>
      <th>(130, 131, 132, 133, 134, 135, 136, 137, 138, 139)</th>
      <td>0.000577</td>
      <td>0.000317</td>
      <td>0.529028</td>
      <td>0.616538</td>
    </tr>
    <tr>
      <th>(140, 141, 142, 143, 144, 145, 146, 147, 148, 149)</th>
      <td>0.000536</td>
      <td>0.000295</td>
      <td>0.513520</td>
      <td>0.619319</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="span-style-color-lightgreen-hyperparameter-grid-search-span">
<h3><span style="color:Lightgreen">Hyperparameter Grid Search</span><a class="headerlink" href="#span-style-color-lightgreen-hyperparameter-grid-search-span" title="Permalink to this heading">#</a></h3>
<p>The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a> function puts all the pieces together to scan a grid of one or more hyperparameters for a family of models.  For example, the polynomial degree <span class="math notranslate nohighlight">\(P\)</span> corresponds to a pipeline <code class="docutils literal notranslate"><span class="pre">poly__degree</span></code> parameter, which we vary from 0 to 5 below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">cv_summary</span><span class="p">(</span><span class="n">cv</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Summarize the results from a GridSearchCV fit.</span>

<span class="sd">    Summarize a cross-validation grid search in a pandas DataFrame with the</span>
<span class="sd">    following transformations of the full results:</span>
<span class="sd">      - Remove all columns with timing measurements.</span>
<span class="sd">      - Remove the &#39;param_&#39; prefix from column names.</span>
<span class="sd">      - Remove the &#39;_score&#39; suffix from column names.</span>
<span class="sd">      - Round scores to 3 decimal places.</span>

<span class="sd">     If the parameter grid is 1D, then this function also plots the test</span>
<span class="sd">     and training R2 scores versus the parameter.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cv : sklearn.model_selection.GridSearchCV</span>
<span class="sd">        Instance of a GridSearchCV object that has been fit to some data.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pandas.DataFrame</span>
<span class="sd">        Summary table of cross-validation results.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Look up the list of parameters used in the grid.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="c1"># Index results by the test score rank.</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;rank_test_score&#39;</span><span class="p">]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="s1">&#39;rank_test_score&#39;</span><span class="p">])</span>
    <span class="c1"># Remove columns that measure running time.</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_time&#39;</span><span class="p">)])</span>
    <span class="c1"># Remove param_ prefix from column names.</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span><span class="p">[</span><span class="mi">6</span><span class="p">:]</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;param_&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">n</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
    <span class="c1"># Remove _score suffix from column names.</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">n</span><span class="p">[:</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span> <span class="k">if</span> <span class="n">n</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;_score&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">n</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Plot the test and training scores vs the grid parameter when there is only one.</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;mean_train&#39;</span><span class="p">],</span> <span class="s1">&#39;o:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;mean_test&#39;</span><span class="p">],</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="s1">&#39;x-large&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Hyperparameter value&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Score $R^2$&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;mean_test&#39;</span><span class="p">])),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compare_models</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_degree</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
    <span class="n">hyper_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;poly__degree&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)}</span>
    <span class="n">hyper_model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">PolynomialFeatures</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">))])</span>
    <span class="n">kfold</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">)</span>
    <span class="n">cv</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">hyper_model</span><span class="p">,</span> <span class="n">hyper_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cv_summary</span><span class="p">(</span><span class="n">cv</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>With a small dataset, a polynomial fit is very prone to overfitting and the training score continues to rise as <span class="math notranslate nohighlight">\(P\)</span> increases.  However, only the <span class="math notranslate nohighlight">\(P=1\)</span> and <span class="math notranslate nohighlight">\(P=2\)</span> models look at all promising on the test data, but are still worse than guessing the average <span class="math notranslate nohighlight">\(y\)</span> value (which always has <span class="math notranslate nohighlight">\(R^2=0\)</span>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compare_models</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>poly__degree</th>
      <th>split0_test</th>
      <th>split1_test</th>
      <th>split2_test</th>
      <th>mean_test</th>
      <th>std_test</th>
      <th>split0_train</th>
      <th>split1_train</th>
      <th>split2_train</th>
      <th>mean_train</th>
      <th>std_train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.404</td>
      <td>-2.233</td>
      <td>-0.353</td>
      <td>-0.727</td>
      <td>1.109</td>
      <td>0.731</td>
      <td>0.632</td>
      <td>0.518</td>
      <td>0.627</td>
      <td>0.087</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0.474</td>
      <td>-2.819</td>
      <td>-0.801</td>
      <td>-1.049</td>
      <td>1.356</td>
      <td>0.621</td>
      <td>0.628</td>
      <td>0.481</td>
      <td>0.577</td>
      <td>0.068</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>-0.048</td>
      <td>-7.549</td>
      <td>-3.226</td>
      <td>-3.608</td>
      <td>3.074</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>-1.743</td>
      <td>-5.019</td>
      <td>-28.006</td>
      <td>-11.589</td>
      <td>11.685</td>
      <td>0.857</td>
      <td>0.714</td>
      <td>0.625</td>
      <td>0.732</td>
      <td>0.095</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3</td>
      <td>0.408</td>
      <td>-51.581</td>
      <td>-0.324</td>
      <td>-17.166</td>
      <td>24.337</td>
      <td>0.740</td>
      <td>0.708</td>
      <td>0.518</td>
      <td>0.656</td>
      <td>0.098</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5</td>
      <td>-0.837</td>
      <td>-316.856</td>
      <td>-157.921</td>
      <td>-158.538</td>
      <td>129.015</td>
      <td>0.895</td>
      <td>0.716</td>
      <td>0.749</td>
      <td>0.787</td>
      <td>0.077</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/90466ba3eaffebd251883d8a24d8f2ec6a3e1ab346dc9e9304c7be62818dd27d.png" src="../../_images/90466ba3eaffebd251883d8a24d8f2ec6a3e1ab346dc9e9304c7be62818dd27d.png" />
</div>
</div>
<p>With a larger dataset, the training score is stable over a wide range of <span class="math notranslate nohighlight">\(P\ge 1\)</span> and the test score decreases very slowly. You could make a case for either <span class="math notranslate nohighlight">\(P=1\)</span> (less overfitting) or <span class="math notranslate nohighlight">\(P=2\)</span> (better test score) from the graph below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compare_models</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>poly__degree</th>
      <th>split0_test</th>
      <th>split1_test</th>
      <th>split2_test</th>
      <th>mean_test</th>
      <th>std_test</th>
      <th>split0_train</th>
      <th>split1_train</th>
      <th>split2_train</th>
      <th>mean_train</th>
      <th>std_train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0.593</td>
      <td>0.600</td>
      <td>0.509</td>
      <td>0.567</td>
      <td>0.041</td>
      <td>0.610</td>
      <td>0.611</td>
      <td>0.638</td>
      <td>0.620</td>
      <td>0.013</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0.563</td>
      <td>0.590</td>
      <td>0.512</td>
      <td>0.555</td>
      <td>0.032</td>
      <td>0.619</td>
      <td>0.612</td>
      <td>0.639</td>
      <td>0.623</td>
      <td>0.011</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0.607</td>
      <td>0.590</td>
      <td>0.415</td>
      <td>0.537</td>
      <td>0.087</td>
      <td>0.536</td>
      <td>0.540</td>
      <td>0.625</td>
      <td>0.567</td>
      <td>0.041</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.575</td>
      <td>0.584</td>
      <td>0.337</td>
      <td>0.499</td>
      <td>0.114</td>
      <td>0.620</td>
      <td>0.612</td>
      <td>0.688</td>
      <td>0.640</td>
      <td>0.034</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>0.569</td>
      <td>0.586</td>
      <td>0.336</td>
      <td>0.497</td>
      <td>0.114</td>
      <td>0.623</td>
      <td>0.612</td>
      <td>0.689</td>
      <td>0.642</td>
      <td>0.034</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>-0.017</td>
      <td>-0.023</td>
      <td>-0.001</td>
      <td>-0.014</td>
      <td>0.009</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/6ce3fde6c15b39144c0525822186781a28072f6987c2ff6fa610730d208b8cd8.png" src="../../_images/6ce3fde6c15b39144c0525822186781a28072f6987c2ff6fa610730d208b8cd8.png" />
</div>
</div>
</section>
<section id="span-style-color-lightgreen-comparison-with-bayesian-evidence-using-pymc-span">
<h3><span style="color:Lightgreen">Comparison with Bayesian Evidence Using PyMC</span><a class="headerlink" href="#span-style-color-lightgreen-comparison-with-bayesian-evidence-using-pymc-span" title="Permalink to this heading">#</a></h3>
<p>The function below estimates the Bayesian evidence for the data <span class="math notranslate nohighlight">\(D=(X,y)\)</span> given a polynomial model of degree <span class="math notranslate nohighlight">\(P\)</span>, using the same MCMC techniques we saw earlier but implemented within the <a class="reference external" href="https://www.pymc.io">PyMC framework</a>. PyMC is a probabilistic programming library for Python that allows users to build Bayesian models with a simple Python API and fit them using Markov chain Monte Carlo (MCMC) methods. The evidence calculation requires an additional ingredient that we never specified for cross validation: a prior on the <span class="math notranslate nohighlight">\(P + 1\)</span> polynomial coefficients, which has a similar effect to a regularization term in an sklearn linear regression.  We adopt a Gaussian prior on each coefficient with the same scale, chosen to be large enough that the likelihood will dominate the posterior.</p>
<p>The evidence calculation method via the <span style="color:Violet">GMM-based approach</span> implemented in <code class="docutils literal notranslate"><span class="pre">estimate_log_evidence</span></code> below, is mathematically sound as an approximate method for estimating the log marginal likelihood (log evidence).</p>
<p>The logic follows the identity:</p>
<div class="math notranslate nohighlight">
\[ \Large
\log P(D|M) = \log P(D, \Theta|M) - \log P(\Theta|D, M)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\log P(D, \Theta|M)\)</span> is the <span class="math notranslate nohighlight">\(\mathbf{\text{log joint probability}}\)</span> (<span class="math notranslate nohighlight">\(\log \text{likelihood} + \log \text{prior}\)</span>) and <span class="math notranslate nohighlight">\(\log P(\Theta|D, M)\)</span> is the <span class="math notranslate nohighlight">\(\mathbf{\text{log posterior density}}\)</span>.</p>
<p>The code implements a function, <code class="docutils literal notranslate"><span class="pre">calculate_evidence</span></code>, which offers several methods for estimating the log marginal likelihood (<span class="math notranslate nohighlight">\(\log Z = \log P(D|M)\)</span>). The key method for direct evidence estimation is the <span style="color:Violet">Gaussian Mixture Model (GMM) approximation</span>, which we encountered before.</p>
<section id="span-style-color-orange-the-pymc-model-and-mcmc-sampling-span">
<h4><span style="color:Orange">The PyMC Model and MCMC Sampling</span><a class="headerlink" href="#span-style-color-orange-the-pymc-model-and-mcmc-sampling-span" title="Permalink to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><span style="color:LightBlue">Model Definition</span>: A <strong>polynomial regression</strong> model of degree <span class="math notranslate nohighlight">\(d\)</span> is defined in <strong>PyMC</strong>, where the number of parameters is <span class="math notranslate nohighlight">\(P = d+1\)</span>.</p>
<ul class="simple">
<li><p><strong>Prior:</strong> All coefficients are assigned a <strong>Gaussian prior</strong> <span class="math notranslate nohighlight">\(\text{Normal}(0, \sigma_{\text{coef}})\)</span>.</p></li>
<li><p><strong>Likelihood:</strong> The observed data <span class="math notranslate nohighlight">\(y_{\text{data}}\)</span> is modeled as <span class="math notranslate nohighlight">\(\text{Normal}(\mu, \sigma_y)\)</span>, where <span class="math notranslate nohighlight">\(\mu\)</span> is the polynomial fit (<span class="math notranslate nohighlight">\(\mathbf{X} \cdot \Theta\)</span>) and <span class="math notranslate nohighlight">\(\sigma_y\)</span> is the known observation standard deviation.</p></li>
</ul>
</li>
<li><p><span style="color:LightBlue">Sampling</span>: <strong>NUTS (or HMC) MCMC</strong> sampling is performed to generate samples from the <strong>posterior distribution</strong> <span class="math notranslate nohighlight">\(P(\Theta|D, M)\)</span>. These samples are extracted as <code class="docutils literal notranslate"><span class="pre">coef_samples</span></code> (shape: <span class="math notranslate nohighlight">\(N_{\text{samples}} \times P\)</span>).</p></li>
</ol>
</section>
<section id="span-style-color-orange-gmm-based-log-evidence-calculation-span">
<h4><span style="color:Orange">GMM-Based Log Evidence Calculation</span><a class="headerlink" href="#span-style-color-orange-gmm-based-log-evidence-calculation-span" title="Permalink to this heading">#</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">estimate_log_evidence</span></code> function takes the MCMC samples and a pre-evaluated grid to estimate <span class="math notranslate nohighlight">\(\log Z\)</span>.</p>
<section id="a-span-style-color-purple-log-joint-calculation-span-the-numerator">
<h5>A. <span style="color: Purple">Log Joint Calculation</span> (The Numerator)<a class="headerlink" href="#a-span-style-color-purple-log-joint-calculation-span-the-numerator" title="Permalink to this heading">#</a></h5>
<p>The <strong>log joint probability</strong> <span class="math notranslate nohighlight">\(\log P(D, \Theta|M)\)</span> is calculated on a parameter grid (<span class="math notranslate nohighlight">\(\Theta_{\text{grid}}\)</span>) derived from the MCMC samples.</p>
<ol class="arabic simple">
<li><p><span style="color:LightBlue">Grid Construction</span> (<code class="docutils literal notranslate"><span class="pre">create_param_grid</span></code>): A dense grid (<span class="math notranslate nohighlight">\(\Theta_{\text{grid}}\)</span>) is constructed using <strong>percentiles</strong> of the MCMC samples. This efficiently covers the high-probability region of the parameter space.</p></li>
<li><p><span style="color:LightBlue">Log Joint Evaluation</span>: The log joint probability (<code class="docutils literal notranslate"><span class="pre">log_numerator</span></code>) is calculated for every point on the grid <strong>outside the PyMC model</strong> using closed-form expressions for the prior and likelihood:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">log_prior</span></code>: Sum of the log PDFs of the independent Gaussian priors for <span class="math notranslate nohighlight">\(\Theta\)</span>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_lik</span></code>: Sum of the log PDFs of the Gaussian likelihood evaluated at the grid parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_numerator</span></code> = <code class="docutils literal notranslate"><span class="pre">log_prior</span></code> + <code class="docutils literal notranslate"><span class="pre">log_lik</span></code>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="b-span-style-color-purple-log-posterior-estimation-span-the-denominator">
<h5>B. <span style="color: Purple">Log Posterior Estimation</span> (The Denominator)<a class="headerlink" href="#b-span-style-color-purple-log-posterior-estimation-span-the-denominator" title="Permalink to this heading">#</a></h5>
<p>The log posterior density, <span class="math notranslate nohighlight">\(\log P(\Theta|D, M)\)</span>, is approximated using a <strong>Gaussian Mixture Model (GMM)</strong>.</p>
<ol class="arabic simple">
<li><p><span style="color:LightBlue">GMM Fitting</span>: A GMM is fitted to the <strong>MCMC samples</strong> (<span class="math notranslate nohighlight">\(\Theta_{\text{samples}}\)</span>). The GMM is trained on the distribution from which the samples were drawn, which is <span class="math notranslate nohighlight">\(P(\Theta|D, M)\)</span>.</p></li>
<li><p><span style="color:LightBlue">Density Evaluation</span>: The GMM provides an estimate of the log posterior density (<span class="math notranslate nohighlight">\(\mathbf{\text{use\_log\_density}}\)</span>) at the selected grid points using <code class="docutils literal notranslate"><span class="pre">fit.score_samples</span></code>.</p></li>
<li><p><span style="color:LightBlue">Optimization</span>: The method iterates over a range of GMM components (<span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(\text{max\_components}\)</span>). The number of components that minimizes the <strong>spread</strong> (inter-quantile range) of the <span class="math notranslate nohighlight">\(\log Z\)</span> estimates across the grid is chosen as the optimal one.</p></li>
</ol>
</section>
<section id="c-span-style-color-purple-final-estimate-span">
<h5>C. <span style="color: Purple">Final Estimate</span><a class="headerlink" href="#c-span-style-color-purple-final-estimate-span" title="Permalink to this heading">#</a></h5>
<p>The estimate for <span class="math notranslate nohighlight">\(\log Z\)</span> is calculated for each grid point and optimal GMM component:</p>
<div class="math notranslate nohighlight">
\[ \Large
\mathbf{\log \hat{P}(D|M)} = \textrm{log numerator} - \textrm{log density}
\]</div>
<p>The final result is the <strong>median</strong> of these stable estimates, providing a robust approximation of the log evidence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pymc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">arviz</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">az</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">mixture</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pymc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pm_version_check</span>

<span class="c1"># Set up logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># Check PyMC version</span>
<span class="n">pymc_version</span> <span class="o">=</span> <span class="n">pm_version_check</span><span class="o">.</span><span class="n">__version__</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">pymc_version</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;5&quot;</span><span class="p">):</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This code is designed for PyMC 5.x. For PyMC 4.x, use `logp_dlogp` instead of `compile_logp` and `d2logp`. Contact support for the PyMC 4.x version.&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_param_grid</span><span class="p">(</span><span class="n">samples</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n_grid</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a parameter grid from posterior samples for GMM-based evidence estimation.&quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">n_grid</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="n">j</span><span class="p">],</span> <span class="n">quantiles</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
    <span class="n">mesh</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">grid</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mesh</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_log_evidence</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">log_numerator</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="n">max_components</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">grid_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate log evidence using Gaussian Mixture Model (GMM) approximation.&quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">cut</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">log_numerator</span><span class="p">,</span> <span class="mf">100.</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">grid_fraction</span><span class="p">))</span>
    <span class="n">use</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">log_numerator</span> <span class="o">&gt;=</span> <span class="n">cut</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">use_grid</span> <span class="o">=</span> <span class="n">param_grid</span><span class="p">[</span><span class="n">use</span><span class="p">]</span>
    <span class="n">use_log_numerator</span> <span class="o">=</span> <span class="n">log_numerator</span><span class="p">[</span><span class="n">use</span><span class="p">]</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">log_evidence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_components</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">use</span><span class="p">)))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_components</span><span class="p">):</span>
        <span class="n">fit</span> <span class="o">=</span> <span class="n">mixture</span><span class="o">.</span><span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">use_log_density</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">use_grid</span><span class="p">)</span>
        <span class="n">log_evidence</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">use_log_numerator</span> <span class="o">-</span> <span class="n">use_log_density</span>

    <span class="n">lo</span><span class="p">,</span> <span class="n">med</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">log_evidence</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">95</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">spread</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">hi</span> <span class="o">-</span> <span class="n">lo</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">spread</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">use_log_numerator</span><span class="p">,</span> <span class="n">log_evidence</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\log P(D|\Theta,M) + \log P(\Theta|M)$&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\log P(D|M)$&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">lo</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;5th/95th percentiles&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">hi</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">med</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Median&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n_GMM=</span><span class="si">{</span><span class="n">best</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">, logP(D|M)=</span><span class="si">{</span><span class="n">med</span><span class="p">[</span><span class="n">best</span><span class="p">]</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">med</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_evidence</span><span class="p">(</span>
    <span class="n">Xdata</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span>
    <span class="n">ydata</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span>
    <span class="n">degree</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">sigma_y</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">coef_sigma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10.0</span><span class="p">,</span>
    <span class="n">n_mc</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">n_grid</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span><span class="p">,</span>
    <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;waic&quot;</span><span class="p">,</span>
    <span class="n">grid_fraction</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fits a polynomial regression model using PyMC and calculates model evidence</span>
<span class="sd">    approximations using various methods (WAIC, LOO, Laplace, GMM).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Xdata : np.ndarray or list</span>
<span class="sd">        Input features (1D or 2D array of shape (n_samples,) or (n_samples, n_features)).</span>
<span class="sd">    ydata : np.ndarray or list</span>
<span class="sd">        Target values (1D array of shape (n_samples,)).</span>
<span class="sd">    degree : int</span>
<span class="sd">        Polynomial degree (non-negative).</span>
<span class="sd">    sigma_y : float, optional</span>
<span class="sd">        Standard deviation of observation noise (default: 1.0).</span>
<span class="sd">    coef_sigma : float, optional</span>
<span class="sd">        Standard deviation of prior on coefficients (default: 10.0).</span>
<span class="sd">    n_mc : int, optional</span>
<span class="sd">        Total number of MCMC samples (default: 2000).</span>
<span class="sd">    n_grid : int, optional</span>
<span class="sd">        Number of grid points to use along each parameter axis. The full</span>
<span class="sd">        grid contains n_grid ** P points.</span>
<span class="sd">    seed : int, optional</span>
<span class="sd">        Random seed for reproducibility (default: 123).</span>
<span class="sd">    method : str, optional</span>
<span class="sd">        Evidence approximation method: &#39;waic&#39;, &#39;loo&#39;, &#39;laplace&#39;, or &#39;gmm&#39; (default: &#39;waic&#39;).</span>
<span class="sd">    grid_fraction : float, optional</span>
<span class="sd">        Fraction of samples to use for GMM method (default: 0.1).</span>
<span class="sd">    plot : bool, optional</span>
<span class="sd">        Whether to generate a plot for the GMM method (default: False).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dict</span>
<span class="sd">        Dictionary containing:</span>
<span class="sd">        - idata: ArviZ InferenceData object with posterior samples.</span>
<span class="sd">        - method: Selected method.</span>
<span class="sd">        - logZ: Estimated log marginal likelihood.</span>
<span class="sd">        - Additional method-specific results (e.g., waic_result, loo_result).</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; # Generate synthetic data (quadratic polynomial with noise)</span>
<span class="sd">    &gt;&gt;&gt; np.random.seed(123)</span>
<span class="sd">    &gt;&gt;&gt; X = np.linspace(-2, 2, 50)</span>
<span class="sd">    &gt;&gt;&gt; true_coefs = [1, 0.5, 2]  # y = 1 + 0.5x + 2x^2</span>
<span class="sd">    &gt;&gt;&gt; y = np.polyval(true_coefs[::-1], X) + np.random.normal(0, 0.5, size=50)</span>
<span class="sd">    &gt;&gt;&gt; # Fit model and estimate evidence using different methods</span>
<span class="sd">    &gt;&gt;&gt; results_waic = calculate_evidence(X, y, degree=2, method=&quot;waic&quot;)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;WAIC logZ: {results_waic[&#39;logZ&#39;]:.3f}&quot;)</span>
<span class="sd">    &gt;&gt;&gt; results_laplace = calculate_evidence(X, y, degree=2, method=&quot;laplace&quot;)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;Laplace logZ: {results_laplace[&#39;logZ&#39;]:.3f}&quot;)</span>
<span class="sd">    &gt;&gt;&gt; results_gmm = calculate_evidence(X, y, degree=2, method=&quot;gmm&quot;, plot=True)</span>
<span class="sd">    &gt;&gt;&gt; print(f&quot;GMM logZ: {results_gmm[&#39;logZ&#39;]:.3f}&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Input validation</span>
    <span class="n">Xdata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Xdata</span><span class="p">)</span>
    <span class="n">ydata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">ydata</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">Xdata</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Xdata must be 1D or 2D&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ydata</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ydata must be 1D&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">Xdata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">ydata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Xdata and ydata must have same number of samples&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">degree</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Degree must be non-negative&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">sigma_y</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">coef_sigma</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;sigma_y and coef_sigma must be positive&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">method</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;waic&quot;</span><span class="p">,</span> <span class="s2">&quot;loo&quot;</span><span class="p">,</span> <span class="s2">&quot;laplace&quot;</span><span class="p">,</span> <span class="s2">&quot;gmm&quot;</span><span class="p">}:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Method must be &#39;waic&#39;, &#39;loo&#39;, &#39;laplace&#39;, or &#39;gmm&#39;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">grid_fraction</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">grid_fraction</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;grid_fraction must be in (0, 1]&quot;</span><span class="p">)</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Preprocess data</span>
    <span class="k">if</span> <span class="n">Xdata</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">Xdata</span> <span class="o">=</span> <span class="n">Xdata</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vander</span><span class="p">(</span><span class="n">Xdata</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">increasing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Stable polynomial matrix</span>
    <span class="n">n_data</span><span class="p">,</span> <span class="n">n_params</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Warn about potential overfitting</span>
    <span class="k">if</span> <span class="n">n_data</span> <span class="o">&lt;</span> <span class="n">n_params</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Number of data points (</span><span class="si">%d</span><span class="s2">) is less than number of parameters (</span><span class="si">%d</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">n_data</span><span class="p">,</span> <span class="n">n_params</span><span class="p">)</span>

    <span class="c1"># PyMC model</span>
    <span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">X_shared</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="s2">&quot;X_shared&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>  <span class="c1"># Removed mutable=True</span>
        <span class="n">coef</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;coef&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">coef_sigma</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">n_params</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_shared</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
        <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma_y</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">ydata</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">idata</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">draws</span><span class="o">=</span><span class="n">n_mc</span> <span class="o">//</span> <span class="mi">4</span><span class="p">,</span>
                <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                <span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                <span class="n">random_seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                <span class="n">progressbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">idata_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;log_likelihood&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="n">pm</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">SamplingError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Sampling failed: </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;idata&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="n">method</span><span class="p">,</span> <span class="s2">&quot;logZ&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">}</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;idata&quot;</span><span class="p">:</span> <span class="n">idata</span><span class="p">,</span> <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="n">method</span><span class="p">}</span>

    <span class="c1"># Extract samples</span>
    <span class="n">coef_da</span> <span class="o">=</span> <span class="n">idata</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;coef&quot;</span><span class="p">]</span>
    <span class="n">dims_to_stack</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">coef_da</span><span class="o">.</span><span class="n">dims</span> <span class="k">if</span> <span class="n">d</span> <span class="o">!=</span> <span class="s2">&quot;coef_dim_0&quot;</span><span class="p">]</span>
    <span class="n">coef_samples</span> <span class="o">=</span> <span class="n">coef_da</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">dims_to_stack</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
    <span class="k">if</span> <span class="n">coef_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">n_params</span><span class="p">:</span>
        <span class="n">coef_samples</span> <span class="o">=</span> <span class="n">coef_samples</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># Ensure (n_samples, n_params)</span>

    <span class="c1"># Evidence approximations</span>
    <span class="k">if</span> <span class="n">method</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;waic&quot;</span><span class="p">:</span>
        <span class="n">waic_res</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">waic</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">pointwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;waic_result&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">waic_res</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;logZ&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">waic_res</span><span class="o">.</span><span class="n">elpd_waic</span>  <span class="c1"># Rough proxy</span>

    <span class="k">elif</span> <span class="n">method</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;loo&quot;</span><span class="p">:</span>
        <span class="n">loo_res</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">loo</span><span class="p">(</span><span class="n">idata</span><span class="p">,</span> <span class="n">pointwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;loo_result&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loo_res</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;logZ&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">loo_res</span><span class="o">.</span><span class="n">elpd_loo</span>  <span class="c1"># Rough proxy</span>

    <span class="k">elif</span> <span class="n">method</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;gmm&quot;</span><span class="p">:</span>
        <span class="n">param_grid</span> <span class="o">=</span> <span class="n">create_param_grid</span><span class="p">(</span><span class="n">coef_samples</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="n">n_grid</span><span class="p">)</span>
        <span class="n">log_prior</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">param_grid</span> <span class="o">/</span> <span class="n">coef_sigma</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">param_grid</span><span class="o">.</span><span class="n">T</span>
        <span class="n">log_lik</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">ydata</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">log_numerator</span> <span class="o">=</span> <span class="n">log_prior</span> <span class="o">+</span> <span class="n">log_lik</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;logZ&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimate_log_evidence</span><span class="p">(</span>
            <span class="n">coef_samples</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">log_numerator</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="n">grid_fraction</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Ea_P0</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 19 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 19 seconds.
</pre></div>
</div>
<img alt="../../_images/582cd49340dc312bec9435c564cc2eb8b310d6793f6215b67b3c435836b60e7c.png" src="../../_images/582cd49340dc312bec9435c564cc2eb8b310d6793f6215b67b3c435836b60e7c.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: -1.329
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Eb_P0</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 250 draw iterations (4_000 + 1_000 draws total) took 18 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 250 draw iterations (4_000 + 1_000 draws total) took 18 seconds.
</pre></div>
</div>
<img alt="../../_images/ed2cdcec36fa7886a3ed6040c89fe691813cb969d1a2a53159a880a2b29b57c8.png" src="../../_images/ed2cdcec36fa7886a3ed6040c89fe691813cb969d1a2a53159a880a2b29b57c8.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: -7.876
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Ea_P1</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 19 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 19 seconds.
</pre></div>
</div>
<img alt="../../_images/48c3b02ba3e83a547115ea4599c057510799682c5ab4b8efb7fb140d8acd4b0d.png" src="../../_images/48c3b02ba3e83a547115ea4599c057510799682c5ab4b8efb7fb140d8acd4b0d.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: 0.226
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Eb_P1</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 19 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 19 seconds.
</pre></div>
</div>
<img alt="../../_images/69e272f3693d10deecced0d7f8231ad9a727c5b6d621014e49fb5fcaf88f44a6.png" src="../../_images/69e272f3693d10deecced0d7f8231ad9a727c5b6d621014e49fb5fcaf88f44a6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: -4.599
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Ea_P2</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 375 draw iterations (4_000 + 1_500 draws total) took 23 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 375 draw iterations (4_000 + 1_500 draws total) took 23 seconds.
The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
INFO:pymc.stats.convergence:The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
</pre></div>
</div>
<img alt="../../_images/99f1ff7f43b513479bb98365a4228898c1dbbfa65e1a25be75da6b781dba7063.png" src="../../_images/99f1ff7f43b513479bb98365a4228898c1dbbfa65e1a25be75da6b781dba7063.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: 2.400
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">75</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Eb_P2</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 375 draw iterations (4_000 + 1_500 draws total) took 19 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 375 draw iterations (4_000 + 1_500 draws total) took 19 seconds.
The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
INFO:pymc.stats.convergence:The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
</pre></div>
</div>
<img alt="../../_images/626a0e552efcbf4169264cd2bfeffeeb600f60a5dbe945831d3c16a425183ef1.png" src="../../_images/626a0e552efcbf4169264cd2bfeffeeb600f60a5dbe945831d3c16a425183ef1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: -3.184
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xa</span><span class="p">,</span> <span class="n">ya</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Ea_P3</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 300 draw iterations (4_000 + 1_200 draws total) took 20 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 300 draw iterations (4_000 + 1_200 draws total) took 20 seconds.
The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
INFO:pymc.stats.convergence:The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
</pre></div>
</div>
<img alt="../../_images/b0db0b58995b58d9717192b8e95df1ffa98cd79ba6c8101d2b9aaa6f1346888a.png" src="../../_images/b0db0b58995b58d9717192b8e95df1ffa98cd79ba6c8101d2b9aaa6f1346888a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: 4.957
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_gmm</span> <span class="o">=</span> <span class="n">calculate_evidence</span><span class="p">(</span><span class="n">Xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;gmm&quot;</span><span class="p">,</span> <span class="n">n_mc</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">log_Eb_P3</span> <span class="o">=</span> <span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GMM logZ: </span><span class="si">{</span><span class="n">results_gmm</span><span class="p">[</span><span class="s1">&#39;logZ&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
INFO:pymc.sampling.mcmc:Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
INFO:pymc.sampling.mcmc:Multiprocess sampling (4 chains in 4 jobs)
NUTS: [coef]
INFO:pymc.sampling.mcmc:NUTS: [coef]
Sampling 4 chains for 1_000 tune and 250 draw iterations (4_000 + 1_000 draws total) took 21 seconds.
INFO:pymc.sampling.mcmc:Sampling 4 chains for 1_000 tune and 250 draw iterations (4_000 + 1_000 draws total) took 21 seconds.
The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
INFO:pymc.stats.convergence:The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
</pre></div>
</div>
<img alt="../../_images/c21c684cec60bcae135028a2bbd5c4fe927a0d80ab652073930df6cda1ef0fe1.png" src="../../_images/c21c684cec60bcae135028a2bbd5c4fe927a0d80ab652073930df6cda1ef0fe1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GMM logZ: -1.228
</pre></div>
</div>
</div>
</div>
<p>Summarize these estimated log evidence values, <span class="math notranslate nohighlight">\(\log P(D\mid M)\)</span>, for the four models considered, P0-3:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;P0&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">log_Ea_P0</span><span class="p">,</span> <span class="n">log_Eb_P0</span><span class="p">],</span> <span class="s1">&#39;P1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">log_Ea_P1</span><span class="p">,</span> <span class="n">log_Eb_P1</span><span class="p">],</span>
    <span class="s1">&#39;P2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">log_Ea_P2</span><span class="p">,</span> <span class="n">log_Eb_P2</span><span class="p">],</span> <span class="s1">&#39;P3&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">log_Ea_P3</span><span class="p">,</span> <span class="n">log_Eb_P3</span><span class="p">]},</span>
    <span class="n">index</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;N=15&#39;</span><span class="p">,</span> <span class="s1">&#39;N=150&#39;</span><span class="p">))</span>
<span class="n">results</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>P0</th>
      <th>P1</th>
      <th>P2</th>
      <th>P3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>N=15</th>
      <td>-1.33</td>
      <td>0.23</td>
      <td>2.40</td>
      <td>4.96</td>
    </tr>
    <tr>
      <th>N=150</th>
      <td>-7.88</td>
      <td>-4.60</td>
      <td>-3.18</td>
      <td>-1.23</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<hr style="border:1px solid rgba(255, 255, 255, 1); margin: 2em 0;"><p><em><strong><span style="color:violet">EXERCISE</span></strong></em>: Use the table of <span class="math notranslate nohighlight">\(\log P(D\mid M)\)</span> values above to answer the following questions:</p>
<ul class="simple">
<li><p>Which model best explains the <span class="math notranslate nohighlight">\(N=15\)</span> dataset?</p></li>
<li><p>Which model best explains the <span class="math notranslate nohighlight">\(N=150\)</span> dataset?</p></li>
<li><p>Which pairs of models have a Bayes’ factor <span class="math notranslate nohighlight">\(&gt; 100\)</span>, indicating “decisive evidence” favoring one model over the other?</p></li>
<li><p>Does “decisive evidence” that favors one model over another indicate that the favored model is correct?</p></li>
<li><p>Are the model comparisons based on evidence substantially different from those based on cross validation in this example?</p></li>
</ul>
<p>Both the <span class="math notranslate nohighlight">\(N=15\)</span> and <span class="math notranslate nohighlight">\(N=150\)</span> datasets are best explained by the P3 model (since it has the maximum value in the first row of the table), which is probably not surprising given the modest differences of the function over this polynomial degree range.</p>
<p>A Bayes’ factor <span class="math notranslate nohighlight">\(&gt; 10\)</span> represents <span style="color:Orange">strong evidence</span> of one model over another and corresponds to a difference in <span class="math notranslate nohighlight">\(\log P(D\mid M)\)</span> of <span class="math notranslate nohighlight">\(\log 10 \simeq 2.3\)</span>.</p>
<p>Here is a table showing which model pairs pass <span style="color:Orange">strong evidence</span> test</p>
<p>For <span class="math notranslate nohighlight">\(N=15\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">row</span> <span class="o">-</span> <span class="n">row</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[False False  True  True]
 [False False False  True]
 [False False False  True]
 [False False False False]]
</pre></div>
</div>
</div>
</div>
<p>And for <span class="math notranslate nohighlight">\(N=150\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">row</span> <span class="o">-</span> <span class="n">row</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[False  True  True  True]
 [False False False  True]
 [False False False False]
 [False False False False]]
</pre></div>
</div>
</div>
</div>
<p>A Bayes’ factor <span class="math notranslate nohighlight">\(&gt; 100\)</span> represents <span style="color:Orange">decisive evidence</span> of one model over another and corresponds to a difference in <span class="math notranslate nohighlight">\(\log P(D\mid M)\)</span> of <span class="math notranslate nohighlight">\(\log 100 \simeq 4.6\)</span>.</p>
<p>Here is a table showing which model pairs pass <span style="color:Orange">decisive evidence</span> test</p>
<p>For <span class="math notranslate nohighlight">\(N=15\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">row</span> <span class="o">-</span> <span class="n">row</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[False False False  True]
 [False False False  True]
 [False False False False]
 [False False False False]]
</pre></div>
</div>
</div>
</div>
<p>And for <span class="math notranslate nohighlight">\(N=150\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">row</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">row</span> <span class="o">-</span> <span class="n">row</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[False False False  True]
 [False False False  True]
 [False False False False]
 [False False False False]]
</pre></div>
</div>
</div>
</div>
<p>We find that, in both cases, the P3 model is “decisively favored” over P0 and P1 as explanations for the data.</p>
<hr style="border:1px solid rgba(255, 255, 255, 1); margin: 2em 0;"><p>The Bayes’ factor is calculated for a pair of models, without considering the range of all possible models. A high value can either indicate that one model of the pair is particularly good or that the other model is particularly bad (or some combination of these). In this example, P0 is particularly bad. In general, the Bayes’ factor compares models relative to each other, but does not offer any absolute measure of how well either model explains the data.</p>
<p>These evidence-based model comparisons are broadly consistent with the earlier cross-validation comparisons, but with some differences in the details. The advantages of using evidence are clearer for the smaller dataset, where the cross validation results are difficult to interpret and priors have more influence. Ideally, you should use both methods and compare.</p>
<hr style="border:1px solid rgba(255, 255, 255, 1); margin: 2em 0;"></section>
</section>
</section>
<section id="span-style-color-lightgreen-appendix-comparison-with-bayesian-evidence-using-emcee-span-alternative-implementation-for-reference">
<h3><span style="color:Lightgreen">Appendix: Comparison with Bayesian Evidence using Emcee</span> (alternative implementation for reference)<a class="headerlink" href="#span-style-color-lightgreen-appendix-comparison-with-bayesian-evidence-using-emcee-span-alternative-implementation-for-reference" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">emcee</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">mixture</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_param_grid</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a parameter grid from parameter samples.</span>

<span class="sd">    Grids are based on 1D quantiles in each parameter, so are not uniform.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    samples : array</span>
<span class="sd">        2D array with shape (N, P) containing N samples for P parameters.</span>
<span class="sd">    n_grid : int</span>
<span class="sd">        Number of grid points to use along each parameter axis. The full</span>
<span class="sd">        grid contains n_grid ** P points.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    array</span>
<span class="sd">        Array of shape (n_grid ** P, P) with parameters values covering the</span>
<span class="sd">        full grid. Can be reshaped to ([P] * (P+1)) to reconstruct the</span>
<span class="sd">        P-dimensional grid structure.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">P</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">n_grid</span><span class="p">)</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">column</span><span class="p">,</span> <span class="n">quantiles</span><span class="p">)</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">T</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">grid</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">estimate_log_evidence</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">log_numerator</span><span class="p">,</span> <span class="n">max_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                          <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate the log evidence using MCMC samples.</span>

<span class="sd">    The evidence is estimated at each grid point using the ratio of the</span>
<span class="sd">    log_numerator and the empirical density of samples. Only grid points with</span>
<span class="sd">    the largest log_numerator are used for the estimate. The density is</span>
<span class="sd">    estimated with a Gaussian mixture model using the number of components</span>
<span class="sd">    that minimizes the spread of estimates over the selected grid points.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    samples : array</span>
<span class="sd">        2D array with shape (N, P) containing N samples for P parameters.</span>
<span class="sd">    param_grid : array</span>
<span class="sd">        2D array with shape (n_grid ** P, P) to specify a grid that covers</span>
<span class="sd">        the full parameter space. Normally obtained by calling</span>
<span class="sd">        :func:`create_param_grid`.</span>
<span class="sd">    log_numerator : array</span>
<span class="sd">        1D array with shape (n_grid ** P,) with the log_likelihood+log_prior</span>
<span class="sd">        value tabulated on the input parameter grid.</span>
<span class="sd">    max_components : int</span>
<span class="sd">        Maximum number of Gaussian mixture model components to use in</span>
<span class="sd">        estimating the density of samples over the parameter space.</span>
<span class="sd">    grid_fraction : float</span>
<span class="sd">        The fraction of grid points with the highest log_numerator to use</span>
<span class="sd">        for estimating the log_evidence.</span>
<span class="sd">    plot : bool</span>
<span class="sd">        When True, draw a scatter plot of log_numerator vs log_evidence for the</span>
<span class="sd">        requested fraction of grid points, using the best found number of</span>
<span class="sd">        GMM components.</span>
<span class="sd">    seed : int or None</span>
<span class="sd">        Random seed to use for reproducible results.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Estimate of the log evidence.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
    <span class="n">cut</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">log_numerator</span><span class="p">,</span> <span class="mf">100.</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">grid_fraction</span><span class="p">))</span>
    <span class="n">use</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">log_numerator</span> <span class="o">&gt;=</span> <span class="n">cut</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">use_grid</span> <span class="o">=</span> <span class="n">param_grid</span><span class="p">[</span><span class="n">use</span><span class="p">]</span>
    <span class="n">use_log_numerator</span> <span class="o">=</span> <span class="n">log_numerator</span><span class="p">[</span><span class="n">use</span><span class="p">]</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">log_evidence</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">max_components</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">use</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_components</span><span class="p">):</span>
        <span class="n">fit</span> <span class="o">=</span> <span class="n">mixture</span><span class="o">.</span><span class="n">GaussianMixture</span><span class="p">(</span>
            <span class="n">n_components</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">gen</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="n">use_log_density</span> <span class="o">=</span> <span class="n">fit</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">use_grid</span><span class="p">)</span>
        <span class="n">log_evidence</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">use_log_numerator</span> <span class="o">-</span> <span class="n">use_log_density</span>
    <span class="n">lo</span><span class="p">,</span> <span class="n">med</span><span class="p">,</span> <span class="n">hi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">log_evidence</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">95</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">spread</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">hi</span> <span class="o">-</span> <span class="n">lo</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">spread</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">use_log_numerator</span><span class="p">,</span> <span class="n">log_evidence</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$\log P(D\mid \Theta, M) + \log P(\Theta\mid M)$&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$\log P(D\mid M)$&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">lo</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">hi</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">med</span><span class="p">[</span><span class="n">best</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;n_GMM=</span><span class="si">{}</span><span class="s1">, logP(D|M)=</span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">med</span><span class="p">[</span><span class="n">best</span><span class="p">]))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">med</span><span class="p">[</span><span class="n">best</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">calculate_evidence</span><span class="p">(</span>
    <span class="n">Xdata</span><span class="p">,</span>
    <span class="n">ydata</span><span class="p">,</span>
    <span class="n">degree</span><span class="p">,</span>
    <span class="n">sigma_y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">coef_sigma</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span>
    <span class="n">n_mc</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">n_grid</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">grid_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the log evidence for a polynomial regression model using MCMC.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    Xdata : array</span>
<span class="sd">        1D array of input data points.</span>
<span class="sd">    ydata : array</span>
<span class="sd">        1D array of output data points.</span>
<span class="sd">    degree : int</span>
<span class="sd">        Degree of the polynomial model.</span>
<span class="sd">    sigma_y : float</span>
<span class="sd">        Standard deviation of the observation noise.</span>
<span class="sd">    coef_sigma : float</span>
<span class="sd">        Standard deviation of the prior on polynomial coefficients.</span>
<span class="sd">    n_mc : int</span>
<span class="sd">        Number of MCMC samples to generate.</span>
<span class="sd">    n_grid : int</span>
<span class="sd">        Number of grid points per parameter for evidence estimation.</span>
<span class="sd">    grid_fraction : float</span>
<span class="sd">        Fraction of grid points to use for evidence estimation.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Random seed for reproducibility.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        Estimate of the log evidence.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Ensure inputs are numpy arrays with correct shapes</span>
    <span class="n">Xdata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">Xdata</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ydata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">ydata</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Xdata shape: </span><span class="si">{</span><span class="n">Xdata</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, ydata shape: </span><span class="si">{</span><span class="n">ydata</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">Xdata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">ydata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Xdata and ydata must have the same number of samples, got </span><span class="si">{</span><span class="n">Xdata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">ydata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">Xdata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Xdata must have exactly one feature, got </span><span class="si">{</span><span class="n">Xdata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> features&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">Xdata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Xdata is empty&quot;</span><span class="p">)</span>

    <span class="c1"># Use sklearn fit to initialize MCMC chains</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">)</span>
    <span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Xdata</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Polynomial features shape: </span><span class="si">{</span><span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">polyreg</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="n">polyreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">)</span>
    <span class="c1"># Extract coefficients and intercept</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">intercept_</span>
    <span class="c1"># Ensure intercept is a scalar</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">intercept</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">intercept</span> <span class="o">=</span> <span class="n">intercept</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">polyreg</span><span class="o">.</span><span class="n">steps</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># Handle case where coefs is 2D</span>
    <span class="k">if</span> <span class="n">coefs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">coefs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Raw coefs: </span><span class="si">{</span><span class="n">coefs</span><span class="si">}</span><span class="s1">, shape: </span><span class="si">{</span><span class="n">coefs</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># For PolynomialFeatures(degree=1), coefs should be [0, slope]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span> <span class="o">!=</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">degree</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> coefficients for degree=</span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Combine intercept and non-constant coefficients</span>
    <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">intercept</span><span class="p">]),</span> <span class="n">coefs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
    <span class="n">P</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">coef_init</span><span class="p">)</span>  <span class="c1"># Number of parameters (intercept + coefficients for x^1, ..., x^degree)</span>
    <span class="k">if</span> <span class="n">P</span> <span class="o">!=</span> <span class="n">degree</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected P=</span><span class="si">{</span><span class="n">degree</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> for degree=</span><span class="si">{</span><span class="n">degree</span><span class="si">}</span><span class="s2">, got P=</span><span class="si">{</span><span class="n">P</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Best fit coefficients:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">coef_init</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of parameters (P): </span><span class="si">{</span><span class="n">P</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># Prepare polynomial features for model evaluation</span>
    <span class="n">XX</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Xdata</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_joint</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">x_data</span><span class="o">=</span><span class="n">XX</span><span class="p">,</span> <span class="n">y_data</span><span class="o">=</span><span class="n">ydata</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the log joint probability (log likelihood + log prior).&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">coef</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">P</span><span class="p">,):</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
        <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
        <span class="n">log_likelihood</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">sigma_y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">log_likelihood</span> <span class="o">-=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma_y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>
        <span class="n">log_prior</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">coef</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">coef_sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">log_prior</span> <span class="o">-=</span> <span class="n">P</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">coef_sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">log_likelihood</span> <span class="o">+</span> <span class="n">log_prior</span>

    <span class="c1"># Define the log probability function for emcee</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="n">coef</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">log_joint</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span>

    <span class="c1"># Set up the MCMC sampler with emcee</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">n_walkers</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of walkers: </span><span class="si">{</span><span class="n">n_walkers</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">initial_positions</span> <span class="o">=</span> <span class="n">coef_init</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="p">(</span><span class="n">n_walkers</span><span class="p">,</span> <span class="n">P</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Initial positions shape: </span><span class="si">{</span><span class="n">initial_positions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># Verify initial positions</span>
    <span class="k">if</span> <span class="n">initial_positions</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_walkers</span><span class="p">,</span> <span class="n">P</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial positions shape </span><span class="si">{</span><span class="n">initial_positions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> does not match (n_walkers=</span><span class="si">{</span><span class="n">n_walkers</span><span class="si">}</span><span class="s2">, P=</span><span class="si">{</span><span class="n">P</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="n">sampler</span> <span class="o">=</span> <span class="n">emcee</span><span class="o">.</span><span class="n">EnsembleSampler</span><span class="p">(</span><span class="n">n_walkers</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>

    <span class="c1"># Run burn-in</span>
    <span class="n">num_burnin_steps</span> <span class="o">=</span> <span class="mi">2000</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">pos</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">initial_positions</span><span class="p">,</span> <span class="n">num_burnin_steps</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error during burn-in: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    <span class="n">sampler</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="c1"># Run production MCMC</span>
    <span class="n">sampler</span><span class="o">.</span><span class="n">run_mcmc</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">n_mc</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">coef_samples</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">get_chain</span><span class="p">(</span><span class="n">flat</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Build a parameter grid for estimating the evidence</span>
    <span class="n">coef_grid</span> <span class="o">=</span> <span class="n">create_param_grid</span><span class="p">(</span><span class="n">coef_samples</span><span class="p">,</span> <span class="n">n_grid</span><span class="o">=</span><span class="n">n_grid</span><span class="p">)</span>

    <span class="c1"># Evaluate log(likelihood) + log(prior) on the parameter grid</span>
    <span class="n">log_numerator_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">log_joint</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span> <span class="k">for</span> <span class="n">coef</span> <span class="ow">in</span> <span class="n">coef_grid</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">estimate_log_evidence</span><span class="p">(</span>
        <span class="n">coef_samples</span><span class="p">,</span>
        <span class="n">coef_grid</span><span class="p">,</span>
        <span class="n">log_numerator_grid</span><span class="p">,</span>
        <span class="n">grid_fraction</span><span class="o">=</span><span class="n">grid_fraction</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Ea_P0 = calculate_evidence(Xa, ya, 0, n_mc=2000, n_grid=200, grid_fraction=0.5)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Eb_P0 = calculate_evidence(Xb, yb, 0, n_mc=2000, n_grid=200, grid_fraction=0.5)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Ea_P1 = calculate_evidence(Xa, ya, 1, n_mc=2000, n_grid=75, grid_fraction=0.3)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Eb_P1 = calculate_evidence(Xb, yb, 1, n_mc=2000, n_grid=75, grid_fraction=0.3)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Ea_P2 = calculate_evidence(Xa, ya, 2, n_mc=1500, n_grid=75, grid_fraction=0.02)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Eb_P2 = calculate_evidence(Xb, yb, 2, n_mc=1500, n_grid=75, grid_fraction=0.02)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Ea_P3 = calculate_evidence(Xa, ya, 3, n_mc=700, n_grid=60, grid_fraction=0.0005)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#log_Eb_P3 = calculate_evidence(Xb, yb, 3, n_mc=700, n_grid=60, grid_fraction=0.0005)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="span-style-color-orange-acknowledgments-span">
<h2><span style="color:Orange">Acknowledgments</span><a class="headerlink" href="#span-style-color-orange-acknowledgments-span" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Initial version: Mark Neubauer</p></li>
</ul>
<p>© Copyright 2025</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./_sources/lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Artificial Intelligence and Machine Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="../homework/Homework_08.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Homework 08: Cross Validation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-model-selection-with-cross-validation-span"><span style="color:Orange">Model Selection With Cross Validation</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-train-vs-validate-vs-test-data-span"><span style="color:Lightgreen">Train vs. Validate vs. Test Data</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-cross-validation-cv-process-span"><span style="color:Lightgreen">Cross Validation (CV) Process</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-overfitting-and-generalization-span"><span style="color:Lightgreen">Overfitting and Generalization</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-train-test-split-span"><span style="color:Lightgreen">Train-Test Split</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-k-folding-span"><span style="color:Lightgreen">K-Folding</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-hyperparameter-grid-search-span"><span style="color:Lightgreen">Hyperparameter Grid Search</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-comparison-with-bayesian-evidence-using-pymc-span"><span style="color:Lightgreen">Comparison with Bayesian Evidence Using PyMC</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-the-pymc-model-and-mcmc-sampling-span"><span style="color:Orange">The PyMC Model and MCMC Sampling</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-gmm-based-log-evidence-calculation-span"><span style="color:Orange">GMM-Based Log Evidence Calculation</span></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#a-span-style-color-purple-log-joint-calculation-span-the-numerator">A. <span style="color: Purple">Log Joint Calculation</span> (The Numerator)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#b-span-style-color-purple-log-posterior-estimation-span-the-denominator">B. <span style="color: Purple">Log Posterior Estimation</span> (The Denominator)</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#c-span-style-color-purple-final-estimate-span">C. <span style="color: Purple">Final Estimate</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-appendix-comparison-with-bayesian-evidence-using-emcee-span-alternative-implementation-for-reference"><span style="color:Lightgreen">Appendix: Comparison with Bayesian Evidence using Emcee</span> (alternative implementation for reference)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mark Neubauer
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>