{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2dYwE1SdgJE"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEtwAZXDdgJH"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing, pipeline, model_selection, linear_model, mixture\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcJU1ZsMdgJI"
      },
      "source": [
        "## <span style=\"color:Orange\">Model Selection With Cross Validation</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avx9CKBjdgJJ"
      },
      "source": [
        "Recall that there are three main types of learning:\n",
        " - Learning model parameters from data.\n",
        "\n",
        " - Learning to predict new data (unsupervised learning).\n",
        "\n",
        " - Learning to predict target features in new data (supervised learning).\n",
        "\n",
        "All three types of learning require an assumed model with associated parameters, and predicting new data is only possible after learning model parameters from old data.\n",
        "\n",
        "Since all types of learning assume a model, they must all solve the meta-problem of comparing competing models. The Bayesian evidence $P(D\\mid M)$ is our primary quantitative tool for comparing how well different models explain the same data.  When we are primarily interested in a model's ability to generalize and predict new data, ___<span style=\"color:violet\">cross validation</span>___ is a useful alternative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbxXc814dgJJ"
      },
      "source": [
        "### <span style=\"color:Lightgreen\">Train vs. Validate vs. Test Data</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxPfZ4A-dgJK"
      },
      "source": [
        "During an ML model development and evaluation, various types of data are utilized. Below, we review these types of data, their purpose and the differences between each in how they are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY5E335xdgJK"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/CrossValidation-splits.png\" width=800 align=left></img><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQUZ52xAdgJK"
      },
      "source": [
        "<span style=\"color:Tan\">Training Dataset</span>: The sample of data used to fit the model.\n",
        "\n",
        "* The actual dataset that we use to train the ML model under development (weights and biases in the case of a Neural Network). The model sees and learns from this data. Training data are collections of examples or samples that are used to \"teach\" or \"train\" the model. The model uses a training data set to understand the patterns and relationships within the data, thereby learning to make predictions or decisions without being explicitly programmed to perform a specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p5N0LtsdgJL"
      },
      "source": [
        "<span style=\"color:Tan\">Validation Dataset</span>: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset (i.e. a \"trained model\") while tuning model hyperparameters. The evaluation becomes more biased as more skill on the validation dataset is incorporated into the model configuration.\n",
        "\n",
        "* It is still possible to tune and control the model at this stage. Working on validation data is used to assess the model performance and fine-tune the parameters of the model. This becomes an iterative process wherein the model learns from the training data and is then validated and fine-tuned on the validation set. A validation dataset tells us how well the model is learning and adapting, allowing for adjustments and optimizations to be made to the model's parameters or hyperparameters before it's finally put to the test. So the validation set affects a model, but only indirectly.\n",
        "\n",
        "  * Note that not all models require validation sets. Some experts consider that ML models with no hyperparameters or those that do not have tuning options do not need a validation set. Still, in most practical applications, validation sets play a crucial role in ensuring the model's robustness and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "785oxx3udgJL"
      },
      "source": [
        "<span style=\"color:Tan\">Test Dataset</span>: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
        "\n",
        "* The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). It is a separate sample, an unseen data set, to provide an unbiased final evaluation of a model. Its primary purpose is to offer a fair and final assessment of how the model would perform when it encounters new data in a live, operational environment.\n",
        "\n",
        "  * The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner).\n",
        "\n",
        "  * Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world on data it has never seen before. The inputs in the test data are similar to the previous stages but not the same data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG_n8gKkdgJL"
      },
      "source": [
        "### <span style=\"color:Lightgreen\">Cross Validation (CV) Process</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nvwZgp6dgJL"
      },
      "source": [
        "Cross validation is a process that provides the ability to estimate model performance on unseen data not used while training. It is a systematic process that can involve tuning the model hyperparameters, testing different properties of the overall datasets, and iterating the training process. There are many variations of the CV process - we will study primarily the <span style=\"color:violet\">K-folds method</span> in this lecture. We will show some examples using random sampling (called \"folds\") of these types of data to illustrate the process. For simplicity, we will focus on the test and train data and not consider validation data explicitly in the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib_iRLUodgJL"
      },
      "source": [
        "The figure below summarizes the data splitting and process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zePqy6zidgJM"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/CrossValidation-process.png\" width=600 align=left></img><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqIHZht_dgJM"
      },
      "source": [
        "The basic idea of ___<span style=\"color:violet\">cross validation</span>___ is to:\n",
        "1. split the observed data into separate training and test datasets\n",
        "\n",
        "2. learn the model from the training dataset\n",
        "\n",
        "3. measure the model's ability to predict the test dataset\n",
        "\n",
        "4. repeat the steps above with different splits (\"folds\") and combine the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfwLbdj3dgJM"
      },
      "source": [
        "### <span style=\"color:Lightgreen\">Overfitting and Generalization</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vYIPydOdgJM"
      },
      "source": [
        "Generate some data consisting of 2D points along the path of a projectile, where $x$ is measured with negligible error and $y$ has a known error $\\sigma_y$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxnc5wUCdgJM"
      },
      "outputs": [],
      "source": [
        "xlo, xhi = 0., 1.\n",
        "poly_coefs = np.array([-1., 2., -1.])\n",
        "f = lambda X: np.dot(poly_coefs, [X ** 0, X ** 1, X ** 2])\n",
        "sigma_y = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRxJwLuwdgJM"
      },
      "outputs": [],
      "source": [
        "def generate(N, seed=123):\n",
        "    gen = np.random.RandomState(seed=seed)\n",
        "    X = gen.uniform(xlo, xhi, size=N)\n",
        "    y = f(X) + gen.normal(scale=sigma_y, size=N)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee8EuirvdgJM"
      },
      "source": [
        "Compare results with different numbers of samples from the same model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpkeI-FgdgJM"
      },
      "outputs": [],
      "source": [
        "Xa, ya = generate(N=15)\n",
        "Xb, yb = generate(N=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "l1jFUEkRdgJM",
        "outputId": "d891845d-e237-4c56-f93b-95b24cab361b"
      },
      "outputs": [],
      "source": [
        "def plotXy(X, y, ax=None, *fits):\n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X, y, s=25, lw=0)\n",
        "    x_grid = np.linspace(xlo, xhi, 100)\n",
        "    ax.plot(x_grid, f(np.array(x_grid)), '-', lw=10, alpha=0.2)\n",
        "    for fit in fits:\n",
        "        y_fit = fit.predict(x_grid.reshape(-1, 1))\n",
        "        ax.plot(x_grid, y_fit, lw=2, alpha=1)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ylo, yhi = np.percentile(y, (0, 100))\n",
        "    dy = yhi - ylo\n",
        "    ax.set_ylim(ylo - 0.1 * dy, yhi + 0.1 * dy)\n",
        "\n",
        "_, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
        "plotXy(Xa, ya, ax[0])\n",
        "plotXy(Xb, yb, ax[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWJDczcPdgJM"
      },
      "source": [
        "The family of competing models we consider are polynomials of different degrees $P$,\n",
        "\n",
        "$$ \\Large\n",
        "y(x) = \\sum_{k=0}^P\\, c_k x^k \\; ,\n",
        "$$\n",
        "\n",
        "each with $P+1$ parameters.  The true model that the datasets were generated from have $P=2$.\n",
        "\n",
        "Use sklearn [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to implement this fit after expanding the features with [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html),\n",
        "\n",
        "$$ \\Large\n",
        "x \\; \\rightarrow\\; \\{ x^0, x^1, \\ldots, x^P \\} \\; ,\n",
        "$$\n",
        "\n",
        "and combining the preprocessing and regression steps into a [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDOC5L5wdgJM"
      },
      "outputs": [],
      "source": [
        "def poly_fit(X, y, degree):\n",
        "    degree_is_zero = (degree == 0)\n",
        "    model = pipeline.Pipeline([\n",
        "        ('poly', preprocessing.PolynomialFeatures(degree=degree, include_bias=degree_is_zero)),\n",
        "        ('linear', linear_model.LinearRegression(fit_intercept=not degree_is_zero))])\n",
        "    return model.fit(X.reshape(-1, 1), y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLDOr9P3dgJN"
      },
      "source": [
        "Compare fits with $P = 0, 1, 2, 14$ to each dataset. Note that $P=14$ is an extreme case of overfitting to the smaller dataset, with the model passing exactly through each sample with large oscillations between them.  Similarly, $P=1$ underfits the larger dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "0k2AXseAdgJN",
        "outputId": "bda45fe3-42f8-4afe-8bfa-adfbb91e0df6"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
        "plotXy(Xa, ya, ax[0], poly_fit(Xa, ya, 0), poly_fit(Xa, ya, 1), poly_fit(Xa, ya, 2), poly_fit(Xa, ya, 14))\n",
        "plotXy(Xb, yb, ax[1], poly_fit(Xb, yb, 0), poly_fit(Xb, yb, 1), poly_fit(Xb, yb, 2), poly_fit(Xb, yb, 14))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jm-sAB8dgJN"
      },
      "source": [
        "### <span style=\"color:Lightgreen\">Train-Test Split</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAFkgvpSdgJN"
      },
      "source": [
        "The [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function picks a random fraction of the observed data to hold back when learning the model and then use for latest testing.\n",
        "\n",
        "Note that since train/test splitting involves random numbers, you will need to pass around a random state object for reproducible results.\n",
        "\n",
        "The plots below show 20% of the data reserved for testing (red points) with a $P=2$ fit to the training data superimposed. The primary sklearn test metric for regression problems is the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$, for which the goal is $R^2 = 1$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "_dRYJUifdgJN",
        "outputId": "51885e0b-290e-4ab8-eeba-d88d7b281b78"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X, y, degree, test_fraction=0.2, ax=None, seed=123):\n",
        "    gen = np.random.RandomState(seed=seed)\n",
        "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "        X, y, test_size=test_fraction, random_state=gen)\n",
        "    train_fit = poly_fit(X_train, y_train, degree)\n",
        "    plotXy(X, y, ax, train_fit)\n",
        "    test_R2 = train_fit.score(X_test.reshape(-1, 1), y_test)\n",
        "    ax.scatter(X_test, y_test, marker='x', color='r', s=40, zorder=10)\n",
        "    ax.text(0.7, 0.1, '$R^2={:.2f}$'.format(test_R2), transform=ax.transAxes, fontsize='x-large', color='r')\n",
        "\n",
        "_, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
        "train_test_split(Xa, ya, 2, ax=ax[0])\n",
        "train_test_split(Xb, yb, 2, ax=ax[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90VlpFFzdgJN"
      },
      "source": [
        "There is no rigorous procedure for setting an optimum test fraction, and anything between 0.1 and 0.5 would be reasonable (and the sklearn default is 0.25).  A larger test fraction improves the reliability of the test metric but decreases the reliability of the model being tested.  As always, more data always helps and reduces your sensitivity to the training fraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "we_RNnoVdgJN",
        "outputId": "46b0b661-2a20-4e3e-801e-0417b652c6d0"
      },
      "outputs": [],
      "source": [
        "def test_fraction_scan(degree=2, seed=123):\n",
        "    gen = np.random.RandomState(seed=seed)\n",
        "    test_fractions = np.arange(0.05, 0.6, 0.025)\n",
        "    R2 = np.empty((2, len(test_fractions)))\n",
        "    for i, test_fraction in enumerate(test_fractions):\n",
        "        for j, (X, y) in enumerate(((Xa, ya), (Xb, yb))):\n",
        "            X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "                X, y, test_size=test_fraction, random_state=gen)\n",
        "            fit = poly_fit(X_train, y_train, degree)\n",
        "            R2[j, i] = fit.score(X_test.reshape(-1, 1), y_test)\n",
        "    plt.plot(test_fractions, R2[0], 'o:', label='$N = {}$'.format(len(ya)))\n",
        "    plt.plot(test_fractions, R2[1], 'o:', label='$N = {}$'.format(len(yb)))\n",
        "    plt.xlabel('Test fraction')\n",
        "    plt.ylabel('Test score $R^2$')\n",
        "    plt.ylim(-2, 1)\n",
        "    plt.legend()\n",
        "\n",
        "test_fraction_scan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y94F5fF_dgJN"
      },
      "source": [
        "### <span style=\"color:Lightgreen\">K-Folding</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbLI897ddgJN"
      },
      "source": [
        "Cross validation goes beyond a simple train-test split by repeating the split multiple times and combining the (correlated) results. There are different strategies for picking the different splits, but [K-folding](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) is a good all-around choice:\n",
        " - Specify the number $k$ of splits (folds) to use.\n",
        "\n",
        " - The data is split into $k$ (almost) equal independent subsets.\n",
        "\n",
        " - Each subset is used for testing once, with the remaining subsets used for training.\n",
        "\n",
        "The result is $k$ different train-test splits using a test fraction $1/k$.  For example, with $N=10$ samples and $k=3$ folds, the subsets are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdaLYLqZdgJN",
        "outputId": "68d62d1c-ce5e-4675-e366-2b5868d68b42"
      },
      "outputs": [],
      "source": [
        "kfold = model_selection.KFold(n_splits=3)\n",
        "[tuple(split[1]) for split in kfold.split(range(10))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOt6pet2dgJN"
      },
      "source": [
        "The [cross_validate](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) function automates the k-folding and scoring process, and outputs both train and test $R^2$ scores, as well as CPU times, for each split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ2FZUwqdgJO",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def cross_validate(X, y, degree, n_splits):\n",
        "    model = pipeline.Pipeline([\n",
        "        ('poly', preprocessing.PolynomialFeatures(degree=degree)),\n",
        "        ('linear', linear_model.LinearRegression(fit_intercept=True))])\n",
        "    kfold = model_selection.KFold(n_splits=n_splits)\n",
        "    scores = model_selection.cross_validate(\n",
        "        model, X.reshape(-1, 1), y, cv=kfold, return_train_score=True)\n",
        "    index = [tuple(split[1]) for split in kfold.split(X.reshape(-1, 1))]\n",
        "    return pd.DataFrame(scores, index=index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OLS_m7EBdgJO",
        "outputId": "1577ae73-2eb4-49c6-b8e6-eb2d9b7efa4f"
      },
      "outputs": [],
      "source": [
        "cross_validate(Xa, ya, degree=2, n_splits=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cmfTlkZdgJO"
      },
      "source": [
        "With enough data, you can use a large number of K-folds but remember that they are highly correlated so you are not increasing the useful information as much as you might think. The sklearn default number of splits is 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "6F7XyQWXdgJO",
        "outputId": "24490ae3-71e7-4628-93a5-c424be49f497"
      },
      "outputs": [],
      "source": [
        "cross_validate(Xb, yb, degree=2, n_splits=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWmZGMZ8dgJT"
      },
      "source": [
        "### <span style=\"color:Lightgreen\">Hyperparameter Grid Search</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYIAp-7ldgJT"
      },
      "source": [
        "The [GridSearchCV]() function puts all the pieces together to scan a grid of one or more hyperparameters for a family of models.  For example, the polynomial degree $P$ corresponds to a pipeline `poly__degree` parameter, which we vary from 0 to 5 below. The full output from `GridSearchCV` is detailed but unwieldy so we use the MLS `grid_search_summary()` function to create a summary table and plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GXZ8R-8dgJT"
      },
      "outputs": [],
      "source": [
        "def cv_summary(cv):\n",
        "    \"\"\"Summarize the results from a GridSearchCV fit.\n",
        "\n",
        "    Summarize a cross-validation grid search in a pandas DataFrame with the\n",
        "    following transformations of the full results:\n",
        "      - Remove all columns with timing measurements.\n",
        "      - Remove the 'param_' prefix from column names.\n",
        "      - Remove the '_score' suffix from column names.\n",
        "      - Round scores to 3 decimal places.\n",
        "\n",
        "     If the parameter grid is 1D, then this function also plots the test\n",
        "     and training R2 scores versus the parameter.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv : sklearn.model_selection.GridSearchCV\n",
        "        Instance of a GridSearchCV object that has been fit to some data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Summary table of cross-validation results.\n",
        "    \"\"\"\n",
        "    # Look up the list of parameters used in the grid.\n",
        "    params = list(cv.cv_results_['params'][0].keys())\n",
        "    # Index results by the test score rank.\n",
        "    index = cv.cv_results_['rank_test_score']\n",
        "    df = pd.DataFrame(cv.cv_results_, index=index).drop(columns=['params', 'rank_test_score'])\n",
        "    # Remove columns that measure running time.\n",
        "    df = df.drop(columns=[n for n in df.columns.values if n.endswith('_time')])\n",
        "    # Remove param_ prefix from column names.\n",
        "    df = df.rename(lambda n: n[6:] if n.startswith('param_') else n, axis='columns')\n",
        "    # Remove _score suffix from column names.\n",
        "    df = df.rename(lambda n: n[:-6] if n.endswith('_score') else n, axis='columns')\n",
        "    if len(params) == 1:\n",
        "        # Plot the test and training scores vs the grid parameter when there is only one.\n",
        "        plt.plot(df[params[0]], df['mean_train'], 'o:', label='train')\n",
        "        plt.plot(df[params[0]], df['mean_test'], 'o-', label='test')\n",
        "        plt.legend(fontsize='x-large')\n",
        "        plt.xlabel('Hyperparameter value')\n",
        "        plt.ylabel('Score $R^2$')\n",
        "        plt.ylim(max(-2, np.min(df['mean_test'])), 1)\n",
        "    return df.sort_index().round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZANN-xkVdgJT",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def compare_models(X, y, max_degree=5, n_splits=3, seed=123):\n",
        "    hyper_grid = {'poly__degree': range(max_degree + 1)}\n",
        "    hyper_model = pipeline.Pipeline([\n",
        "        ('poly', preprocessing.PolynomialFeatures()),\n",
        "        ('linear', linear_model.LinearRegression(fit_intercept=True))])\n",
        "    kfold = model_selection.KFold(n_splits=n_splits)\n",
        "    cv = model_selection.GridSearchCV(hyper_model, hyper_grid, cv=kfold, return_train_score=True)\n",
        "    cv.fit(X.reshape(-1, 1), y)\n",
        "    return cv_summary(cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq6VIge-dgJT"
      },
      "source": [
        "With a small dataset, a polynomial fit is very prone to overfitting and the training score continues to rise as $P$ increases.  However, only the $P=1$ and $P=2$ models look at all promising on the test data, but are still worse than guessing the average $y$ value (which always has $R^2=0$):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "EZ5jmJHEdgJT",
        "outputId": "c3a7316b-4526-46c8-cda5-3c848330a27e"
      },
      "outputs": [],
      "source": [
        "compare_models(Xa, ya)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0okyliydgJT"
      },
      "source": [
        "With a larger dataset, the training score is stable over a wide range of $P\\ge 1$ and the test score decreases very slowly. You could make a case for either $P=1$ (less overfitting) or $P=2$ (better test score) from the graph below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "6x347mjYdgJT",
        "outputId": "906ac31f-62d6-4d2c-fda8-0404bc21a76f"
      },
      "outputs": [],
      "source": [
        "compare_models(Xb, yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI26lkf7dgJT",
        "outputId": "9a9093fa-4e49-4172-fe73-01593602b0f5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "!pip install git+https://github.com/google/edward2\n",
        "import edward2 as ed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yy9XEyzdgJT"
      },
      "source": [
        "### <span style=\"color:Lightgreen\">Comparison with the Bayesian Evidence</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjFompqRdgJT"
      },
      "source": [
        "The function below estimates the Bayesian evidence for the data $D=(X,y)$ given a polynomial model of degree $P$, using the same MCMC techniques we saw earlier. The evidence calculation requires an additional ingredient that we never specified for cross validation: a prior on the $P + 1$ polynomial coefficients, which has a similar effect to a regularization term in an sklearn linear regression.  We adopt a Gaussian prior on each coefficient with the same scale, chosen to be large enough that the likelihood will dominate the posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve1FfukehnnC"
      },
      "outputs": [],
      "source": [
        "def create_param_grid(samples, n_grid=50):\n",
        "    \"\"\"Create a parameter grid from parameter samples.\n",
        "\n",
        "    Grids are based on 1D quantiles in each parameter, so are not uniform.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    samples : array\n",
        "        2D array with shape (N, P) containing N samples for P parameters.\n",
        "    n_grid : int\n",
        "        Number of grid points to use along each parameter axis.  The full\n",
        "        grid contains n_grid ** P points.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    array\n",
        "        Array of shape (n_grid ** P, P) with parameters values covering the\n",
        "        full grid.  Can be reshaped to ([P] * (P+1)) to reconstruct the\n",
        "        P-dimensional grid structure.\n",
        "    \"\"\"\n",
        "    samples = np.asarray(samples)\n",
        "    N, P = samples.shape\n",
        "    # Create a grid that is equally spaced in quantiles of each column.\n",
        "    quantiles = np.linspace(0, 100, n_grid)\n",
        "    grid = [np.percentile(column, quantiles) for column in samples.T]\n",
        "    # Flatten to an array P-tuples that cover the grid.\n",
        "    return np.moveaxis(np.stack(\n",
        "        np.meshgrid(*grid), axis=-1), (0,1), (1,0)).reshape(-1, P)\n",
        "\n",
        "\n",
        "def estimate_log_evidence(samples, param_grid, log_numerator, max_components=5,\n",
        "                          grid_fraction=0.1, plot=True, seed=123):\n",
        "    \"\"\"Estimate the log evidence using MCMC samples.\n",
        "\n",
        "    The evidence is estimated at each grid point using the ratio of the\n",
        "    log_numerator and the empirical density of samples. Only grid points with\n",
        "    the largest log_numerator are used for the estimate. The density is\n",
        "    estimated with a Gaussian mixture model using the a number of components\n",
        "    that minimizes the spread of estimates over the selected grid points.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    samples : array\n",
        "        2D array with shape (N, P) containing N samples for P parameters.\n",
        "    param_grid : array\n",
        "        2D array with shape (n_grid ** P, P) to specify a grid that covers\n",
        "        the full parameter space. Normally obtained by calling\n",
        "        :func:`create_param_grid`.\n",
        "    log_numerator : array\n",
        "        1D array with shape (n_grid ** P,) with the log_likelihood+log_prior\n",
        "        value tabulated on the input parameter grid.\n",
        "    max_components : int\n",
        "        Maximum number of Gaussian mixture model components to use in\n",
        "        estimating the density of samples over the parameter space.\n",
        "    grid_fraction : float\n",
        "        The fraction of grid points with the highest log_numerator to use\n",
        "        for estimating the log_evidence.\n",
        "    plot : bool\n",
        "        When True, draw a scatter plot of log_numerator vs log_evidence for the\n",
        "        requested fraction of grid points, using the best found number of\n",
        "        GMM components.\n",
        "    seed : int or None\n",
        "        Random seed to use for reproducible results.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Estimate of the log evidence.\n",
        "    \"\"\"\n",
        "    samples = np.asarray(samples)\n",
        "    # Only use grid points with the highest log_numerator value.\n",
        "    cut = np.percentile(log_numerator, 100. * (1 - grid_fraction))\n",
        "    use = np.where(log_numerator >= cut)[0]\n",
        "    use_grid = param_grid[use]\n",
        "    use_log_numerator = log_numerator[use]\n",
        "    # Loop over the number of GMM components.\n",
        "    gen = np.random.RandomState(seed=seed)\n",
        "    log_evidence = np.empty((max_components, len(use)))\n",
        "    for i in range(max_components):\n",
        "        # Fit the samples to a Gaussian mixture model.\n",
        "        fit = mixture.GaussianMixture(\n",
        "            n_components=i + 1, random_state=gen).fit(samples)\n",
        "        # Evaluate the density on the grid.\n",
        "        use_log_density = fit.score_samples(use_grid)\n",
        "        # Estimate the log(evidence) from each grid point.\n",
        "        log_evidence[i] = use_log_numerator - use_log_density\n",
        "    # Calculate the median and 90% spread.\n",
        "    lo, med, hi = np.percentile(log_evidence, (5, 50, 95), axis=-1)\n",
        "    spread = 0.5 * (hi - lo)\n",
        "    best = np.argmin(spread)\n",
        "    if plot:\n",
        "        plt.scatter(use_log_numerator, log_evidence[best], s=10, lw=0)\n",
        "        plt.xlabel('$\\log P(D\\mid \\Theta, M) + \\log P(\\Theta\\mid M)$')\n",
        "        plt.ylabel('$\\log P(D\\mid M)$')\n",
        "        plt.axhline(lo[best], ls='--', c='k')\n",
        "        plt.axhline(hi[best], ls='--', c='k')\n",
        "        plt.axhline(med[best], ls='-', c='k')\n",
        "        plt.title('n_GMM={}, logP(D|M)={:.3f}'.format(best + 1, med[best]))\n",
        "        plt.show()\n",
        "    return med[best]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwVluLJFdgJT",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "def calculate_evidence(\n",
        "    Xdata,\n",
        "    ydata,\n",
        "    degree,\n",
        "    sigma_y=sigma_y,\n",
        "    coef_sigma=10.0,\n",
        "    n_mc=5000,\n",
        "    n_grid=50,\n",
        "    grid_fraction=0.1,\n",
        "    seed=123,\n",
        "):\n",
        "    # Use sklearn fit to initialize MCMC chains\n",
        "    fit = poly_fit(Xdata, ydata, degree).steps[1][1]\n",
        "    coef_init = fit.coef_.astype(np.float32)\n",
        "    if degree > 0:\n",
        "        coef_init = np.insert(coef_init, 0, fit.intercept_)\n",
        "    print('Best fit coefficients:', np.round(coef_init, 3))\n",
        "    P = len(coef_init)\n",
        "\n",
        "    # Build the graph for this inference\n",
        "    #\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        tf.random.set_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Build the inference model\n",
        "        #\n",
        "        X = tf.compat.v1.placeholder(tf.float32, name=\"X\")\n",
        "        XP = [X ** p for p in range(degree + 1)]\n",
        "        XX = tf.stack(XP, axis=1, name=\"XX\")\n",
        "\n",
        "        def model(P, x_data):\n",
        "            coef = ed.Normal(loc=0.0, scale=tf.fill((P,), coef_sigma), name=\"coef\")\n",
        "            y_true = tf.reshape(tf.matmul(x_data, tf.reshape(coef, [-1, 1])), [-1])\n",
        "            y = ed.Normal(loc=y_true, scale=sigma_y, name=\"y\")\n",
        "            return y\n",
        "\n",
        "        log_joint = ed.make_log_joint_fn(model)\n",
        "\n",
        "        def target_log_prob_fn(coef):\n",
        "            \"\"\"Target log-probability as a function of states.\"\"\"\n",
        "            return log_joint(P, coef=coef, x_data=XX, y=ydata)\n",
        "\n",
        "        # Define HMC kernel\n",
        "        #\n",
        "        num_burnin_steps = 2000\n",
        "        step_size = 1e-2\n",
        "        num_leapfrog_steps = 10\n",
        "\n",
        "        hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "            target_log_prob_fn=target_log_prob_fn,\n",
        "            step_size=step_size,\n",
        "            num_leapfrog_steps=num_leapfrog_steps\n",
        "        )\n",
        "\n",
        "        hmc_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
        "            inner_kernel=hmc_kernel,\n",
        "            num_adaptation_steps=int(num_burnin_steps * 0.8)\n",
        "        )\n",
        "\n",
        "        # Prepare for MCMC sampling\n",
        "        #\n",
        "        def get_shape(n, x):\n",
        "            x = np.asarray(x)\n",
        "            return (n, x.shape[0]) if len(x.shape) > 0 else (n,)\n",
        "\n",
        "        mc_coef = tf.reshape(\n",
        "            tf.reduce_mean(\n",
        "                tf.compat.v1.random_normal(get_shape(1000, coef_init),\n",
        "                                           mean=coef_init),\n",
        "                axis=0\n",
        "            ),\n",
        "            (P,),\n",
        "        )\n",
        "\n",
        "        states, kernel_results = tfp.mcmc.sample_chain(\n",
        "            num_results=n_mc,\n",
        "            num_burnin_steps=num_burnin_steps,\n",
        "            current_state=[mc_coef],\n",
        "            kernel=hmc_kernel\n",
        "        )\n",
        "\n",
        "        # Prepare to tabulate log_likelihood + log_prior on a coefficient grid.\n",
        "        #\n",
        "        coef_in        = tf.compat.v1.placeholder(tf.float32)\n",
        "        coef_out       = ed.Normal(loc=0.0, scale=tf.fill(tf.shape(coef_in), coef_sigma))\n",
        "        y_in           = tf.compat.v1.placeholder(tf.float32)\n",
        "        y_true_out     = tf.transpose(tf.matmul(XX, tf.transpose(coef_in)))\n",
        "        y_out          = ed.Normal(loc=y_true_out, scale=sigma_y)\n",
        "        log_likelihood = tf.reduce_sum(y_out.distribution.log_prob(y_in), axis=-1)\n",
        "        log_prior      = tf.reduce_sum(coef_out.distribution.log_prob(coef_in), axis=-1)\n",
        "        log_numerator  = log_likelihood + log_prior\n",
        "\n",
        "        init_op = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "    with tf.compat.v1.Session(graph=graph) as sess:\n",
        "        # Run the inference using HMC to generate samples.\n",
        "        #\n",
        "        init_op.run()\n",
        "        states_, results_ = sess.run([states, kernel_results], feed_dict={X: Xdata})\n",
        "        coef_samples = states_[0]\n",
        "\n",
        "        # Build a parameter grid for estimating the evidence.\n",
        "        #\n",
        "        coef_grid = create_param_grid(coef_samples, n_grid=n_grid)\n",
        "\n",
        "        # Evaluate log(likelihood) + log(prior) on the parameter grid.\n",
        "        #\n",
        "        log_numerator_grid = sess.run(\n",
        "            log_numerator, feed_dict={X: Xdata, y_in: ydata, coef_in: coef_grid}\n",
        "        )\n",
        "\n",
        "    return estimate_log_evidence(\n",
        "        coef_samples, coef_grid, log_numerator_grid, grid_fraction=grid_fraction\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "ThnoiuiydgJU",
        "outputId": "c22953ed-346f-4546-884b-557352581763"
      },
      "outputs": [],
      "source": [
        "log_Ea_P0 = calculate_evidence(Xa, ya, 0, grid_fraction=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "xQUu1eXYdgJU",
        "outputId": "ae227eef-c2cd-425f-95ad-2e81ba5b8bcd"
      },
      "outputs": [],
      "source": [
        "log_Eb_P0 = calculate_evidence(Xb, yb, 0, grid_fraction=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "oBTUD9VRdgJU",
        "outputId": "fc6b8413-2b86-4bab-b8cc-d970a35390e3"
      },
      "outputs": [],
      "source": [
        "log_Ea_P1 = calculate_evidence(Xa, ya, 1, grid_fraction=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "Y9hHYVAbdgJU",
        "outputId": "46a68c08-eac8-4073-9335-77bce0578e6b"
      },
      "outputs": [],
      "source": [
        "log_Eb_P1 = calculate_evidence(Xb, yb, 1, grid_fraction=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "NQXu59j4dgJU",
        "outputId": "db05705e-5015-4df5-94d4-bc9d74dbe7a3"
      },
      "outputs": [],
      "source": [
        "log_Ea_P2 = calculate_evidence(Xa, ya, 2, grid_fraction=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "dg2lz-hcdgJU",
        "outputId": "4c023a06-d743-41e7-ddcf-c626b3a36750"
      },
      "outputs": [],
      "source": [
        "log_Eb_P2 = calculate_evidence(Xb, yb, 2, grid_fraction=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "EABpkqsTdgJU",
        "outputId": "c9d2190a-5cb2-437a-9d3d-acfe8f3b2c06"
      },
      "outputs": [],
      "source": [
        "log_Ea_P3 = calculate_evidence(Xa, ya, 3, grid_fraction=0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "uZ_VPxKHdgJU",
        "outputId": "c4988552-83e1-427e-f9ab-e4487e05e2d7"
      },
      "outputs": [],
      "source": [
        "log_Eb_P3 = calculate_evidence(Xb, yb, 3, grid_fraction=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj4B4vQkdgJU"
      },
      "source": [
        "Summarize these estimated log evidence values, $\\log P(D\\mid M)$, for the four models considered, P0-3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "suLBd-BEdgJU",
        "outputId": "d75d3210-3f12-4cf9-8495-1c39f16f87f2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({\n",
        "    'P0': [log_Ea_P0, log_Eb_P0], 'P1': [log_Ea_P1, log_Eb_P1],\n",
        "    'P2': [log_Ea_P2, log_Eb_P2], 'P3': [log_Ea_P3, log_Eb_P3]},\n",
        "    index=('N=15', 'N=150'))\n",
        "results.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tNjlpPBdgJU",
        "solution2": "hidden",
        "solution2_first": true
      },
      "source": [
        "___<span style=\"color:violet\">EXERCISE</span>___: Use the table of $\\log P(D\\mid M)$ values above to answer the following questions:\n",
        " - Which model best explains the $N=15$ dataset?\n",
        "\n",
        " - Which model best explains the $N=150$ dataset?\n",
        "\n",
        " - Which pairs of models have a Bayes' factor $> 100$, indicating \"decisive evidence\" favoring one model over the other?\n",
        "\n",
        " - Does \"decisive evidence\" that favors one model over another indicate that the favored model is correct?\n",
        "\n",
        " - Are the model comparisons based on evidence substantially different from those based on cross validation in this example?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEn6z6wodgJU",
        "solution2": "hidden"
      },
      "source": [
        "The $N=15$ dataset is best explained by the P1 model (since it has the maximum value in the first row of the table).\n",
        "\n",
        "The $N=150$ dataset is best explained by the P2 model (since it has the maximum value in the first row of the table).\n",
        "\n",
        "A Bayes' factor $> 100$ corresponds to a difference in $\\log P(D\\mid M)$ of $\\log 100 \\simeq 4.6$.\n",
        "\n",
        "Here is a table showing which model pairs pass this test for $N=15$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URqqaNS4dgJU",
        "outputId": "b703bc6a-c46e-4f44-fe5e-ddd8d470ae9d",
        "solution2": "hidden"
      },
      "outputs": [],
      "source": [
        "row = results.iloc[0].values\n",
        "print(np.exp(row - row.reshape(-1, 1)) > 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0FT6U4dgJU",
        "solution2": "hidden"
      },
      "source": [
        "And for $N=150$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BBY8PlwdgJV",
        "outputId": "cad19b1a-e052-41aa-b3dd-c420d9371f7a",
        "solution2": "hidden"
      },
      "outputs": [],
      "source": [
        "row = results.iloc[1].values\n",
        "print(np.exp(row - row.reshape(-1, 1)) > 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWSRQElZdgJV",
        "solution2": "hidden"
      },
      "source": [
        "We find that, in both cases, the P1, P2, and P3 models are all \"decisively favored\" over P0 as explanations for the data.\n",
        "\n",
        "The Bayes' factor is calculated for a pair of models, without considering the range of all possible models. A high value can either indicate that one model of the pair is particularly good or that the other model is particularly bad (or some combination of these). In this example, P0 is particularly bad. In general, the Bayes' factor compares models relative to each other, but does not offer any absolute measure of how well either model explains the data.\n",
        "\n",
        "These evidence-based model comparisons are broadly consistent with the earlier cross-validation comparisons, but with some differences in the details. The advantages of using evidence are clearer for the smaller dataset, where the cross validation results are difficult to interpret and priors have more influence. Ideally, you should use both methods and compare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWRtTZDgdgJV"
      },
      "source": [
        "---\n",
        "## <span style=\"color:Orange\">Acknowledgments</span>\n",
        "\n",
        "* Initial version: Mark Neubauer\n",
        "\n",
        "Â© Copyright 2023"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
