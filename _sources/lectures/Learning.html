

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Learning &#8212; PHYS 503</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_sources/lectures/Learning';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Cross Validation" href="CrossValidation.html" />
    <link rel="prev" title="Learning &amp; Cross Validation" href="../Week_09.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="PHYS 503 - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="PHYS 503 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    <span style="color:Blue">Instrumentation Physics: Applications of Machine Learning</span>
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Machine Learning and Data Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_01.html"><span style="color: blue;"><b>Course Introduction</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1vq4b3zxrhEMJbfeCH52hufBbXvTwhufEXdSs8mWY2ec/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="JupyterNumpy.html">Jupyter Notebooks and Numerical Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="Pandas.html">Handling Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_01.html">Homework 01: Numerical python and data handling</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_02.html"><span style="color: blue;"><b>Visualizing &amp; Finding Structure in Data</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1_LstEfghjdZUheyrqbjx4PK9y1J0cN-_hOdCInTldp8/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Visualization.html">Visualizing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Clustering.html">Finding Structure in Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_02.html">Homework 02: Visualization and Expectation-Maximization</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_03.html"><span style="color: blue;"><b>Dimensionality, Linearity and Kernel Functions</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1x4bWQr7kEAh6Z6L7iaLdaNiY6SDTHtvHxzvjFM1wYnE/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Dimensionality.html">Measuring and Reducing Dimensionality</a></li>
<li class="toctree-l2"><a class="reference internal" href="Nonlinear.html">Adapting Linear Methods to Non-Linear Data and Kernel Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_03.html">Homework 03: K-means and Principle Component Analysis</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability and Statistics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_04.html"><span style="color: blue;"><b>Probability Theory</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1qW-gCHY3bQMmB0-klM0crTD9020UG3DTlT_awlOhy2A/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityTheory.html">Probability Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="ProbabilityDistributions.html">Important Probability Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_04.html">Homework 04: Probability Theory and Common Distributions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_05.html"><span style="color: blue;"><b>Kernel Density Estimation and Statistics</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1XZoeBdXzhcfezIbUrH0a9-4-QmM-5iNksLCB7X4q1wI/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Density.html">Estimating Probability Density from Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="Statistics.html">Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_05.html">Homework 05: Kernel Density Estimation, Covariance and Correlation</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Bayesian Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_06.html"><span style="color: blue;"><b>Bayesian Statistics and Markov Chain Monte Carlo</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1L_lz0WbrrUu9qDPnKxYu5S_fCXKjl01Mrs2RnHN5_6E/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayes.html">Bayesian Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="MCMC.html">Markov Chain Monte Carlo in Practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_06.html">Homework 06: Bayesian Statistics and MCMC</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_01.html"><span style="color: blue;"><b>Project 01</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_HiggsTauTau.html">Higgs Boson Decaying to Tau Leptons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_ExoticParticles.html">Searching for Exotic Particles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_GalaxyZoo.html">Galaxy Zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_NuclearGeometryQGP.html">Nuclear Geometry and Characterization of the Quark Gluon Plasma</a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_01_DarkEnergySurvey.html">Dark Energy Survey</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_07.html"><span style="color: blue;"><b>Stochastic Processes, Markov Chains &amp; Variational Inference</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/11Mzc9rBUcnEh_D3SKeDUoCW-iwIA9ilcxsx_4yuu32A/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Markov.html">Stochastic Processes and Markov-Chain Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="Variational.html">Variational Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_07.html">Homework 07: Markov Chains</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_08.html"><span style="color: blue;"><b>Optimization and Model Selection</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1KshmOwKTWptL-3PASHrW6WT2PkH2ltU-XwoYOflhKQk/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Optimization.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="ModelSelection.html">Bayesian Model Selection</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supervised Learning &amp; Cross Validation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../Week_09.html"><span style="color: blue;"><b>Learning &amp; Cross Validation</b></span></a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1a2cjkREM0LYxRjrLwrfHPTotM0n_CgVrZ_WQjEyc_Jc/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="CrossValidation.html">Cross Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_08.html">Homework 08: Cross Validation</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Artificial Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_10.html"><span style="color: blue;"><b>Supervised Learning &amp; Artificial Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1vyg7eSo5XaUAtYDwxmLY5qeUrwKxKqJcEEHPB40yVpE/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="Supervised.html">Supervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="NeuralNetworks.html">Artificial Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_09.html">Homework 09: Artificial Neural Networks</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_11.html"><span style="color: blue;"><b>Deep Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1RnFI0k15C_m2j43QtFDGCFRBCcQ-Rx6EG-9U3EumOHc/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="DeepLearning.html">Deep Learning</a></li>



<li class="toctree-l2"><a class="reference internal" href="../homework/Homework_10.html">Homework 10: Forecasting Projectile Motion with Recurrent Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_12.html"><span style="color: blue;"><b>Graph Neural Networks</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1fxZdnCU_8pWocQbjMQ5HUOSUL3MgbCyg3xFWxfeNaeY/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="GraphNeuralNetworks.html">Graph Neural Networks</a></li>





</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Project_02.html"><span style="color: blue;"><b>Project 02</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_HiggsTauTau.html"><em><strong><span style="color:Yellow">Higgs Boson Decaying to Tau Leptons</span></strong></em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_ExoticParticles.html"><em><strong><span style="color:Yellow">Searching for Exotic Particles</span></strong></em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_GalaxyZoo.html"><em><strong><span style="color:Yellow">Galaxy Zoo</span></strong></em></a></li>
<li class="toctree-l2"><a class="reference internal" href="../projects/Project_02_NuclearGeometryQGP.html">Nuclear Geometry and Characterization of the Quark Gluon Plasma</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_13.html"><span style="color: blue;"><b>Unsupervised Learning, Uncertainties and Anomaly Detection</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1jGxr3j5t7Ahi3Ai6501dVJXOIzvlYMGhe9ZDqHlcfjo/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="UnsupervisedLearning.html">Unsupervised Learning</a></li>

</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Accelerated Machine Learning and Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Week_14.html"><span style="color: blue;"><b>Accelerated Machine Learning and Inference</b></span></a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference external" href="https://docs.google.com/presentation/d/1-_9DcO71v6fQN1kNhKRH2d2iSjhs_Ddi2wLxiXy6RNg/edit?usp=sharing">Slides</a></li>
<li class="toctree-l2"><a class="reference internal" href="AcceleratedML.html">Accelerated Machine Learning and Inference</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/illinois-ipaml/MachineLearningForPhysics/blob/main/_sources/lectures/Learning.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/illinois-ipaml/MachineLearningForPhysics" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/_sources/lectures/Learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-overview-span"><span style="color:Orange">Overview</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-types-of-learning-span"><span style="color:Orange">Types of Learning</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-supervised-learning-span"><span style="color:LightGreen">Supervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-unsupervised-learning-span"><span style="color:LightGreen">Unsupervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-semi-supervised-learning-span"><span style="color:LightGreen">Semi-Supervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-reinforcement-learning-span"><span style="color:LightGreen">Reinforcement Learning</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-learning-in-a-probabilistic-context-span"><span style="color:Orange">Learning in a Probabilistic Context</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-learning-a-model-span"><span style="color:LightGreen">Learning a Model</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><span style="color:LightGreen">Unsupervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><span style="color:LightGreen">Supervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-terminology"><span style="color:LightGreen">Terminology</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-model-selection-revisited-span"><span style="color:LightGreen">Model Selection Revisited</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="learning">
<h1>Learning<a class="headerlink" href="#learning" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<section id="span-style-color-orange-overview-span">
<h2><span style="color:Orange">Overview</span><a class="headerlink" href="#span-style-color-orange-overview-span" title="Permalink to this heading">#</a></h2>
<p>There is no consensus on the precise definitions of data science, machine learning, deep learning and artificial intelligence. For our purposes, we consider the following definitions:</p>
<ul class="simple">
<li><p><em><strong><span style="color:violet">Data Science</span></strong></em> (DS): a cross-disciplinary field that employs scientific approaches, processes, algorithms and systems used to extract meaning and insights from data.</p></li>
<li><p><em><strong><span style="color:violet">Artificial Intelligence</span></strong></em> (AI): a field of research aiming to develop artificial systems with human-level learning and reasoning abilities, possessing the qualities of intentionality, intelligence and adaptability.</p></li>
<li><p><em><strong><span style="color:violet">Machine Learning</span></strong></em> (ML): a subset of the field of AI which involves algorithms with the ability to learn without being explicitly programmed. These algorithms learn from data to improve their accuracy, adaptability and utility.</p></li>
</ul>
<p><em><span style="color:LightGreen">A little ML history</span></em>: Arthur Samuel is an American computer scientist who is credited for coining the term, “machine learning” with his research in computer systems at UIUC (he initiated the <a class="reference external" href="https://en.wikipedia.org/wiki/ILLIAC">ILLIAC project</a>) then at IBM where he developed the first checkers program on IBM’s first commercial computer in 1959. Robert Nealey, a self-proclaimed checkers master, played the game on an IBM 7094 computer in 1962, and he lost to the computer. Compared to what can be done today, this feat seems trivial, but it’s considered a major milestone in the field of artificial intelligence.</p>
<ul class="simple">
<li><p><em><strong><span style="color:violet">Artificial neural networks</span></strong></em> (ANNs): comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node.</p></li>
<li><p><em><strong><span style="color:violet">Deep Learning</span></strong></em> (DL): a subset of ML in which artificial neural networks adapt and learn from large datasets. The “deep” in deep learning is just referring to the number of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the input and the output—can be considered a deep learning algorithm or a deep neural network. A neural network that only has three layers is just a basic neural network. You can think of deep learning as “scalable machine learning” that eliminates some of the human intervention required (through flexible frameworks) and enables the use of larger data sets to continually improve the model performance.</p></li>
</ul>
<p>The figure below summarizes these definitions and relationships:</p>
<p><a class="reference internal" href="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-AI.png"><img alt="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-AI.png" class="align-left" src="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-AI.png" style="width: 800px;" /></a></img><br></p>
</section>
<section id="span-style-color-orange-types-of-learning-span">
<h2><span style="color:Orange">Types of Learning</span><a class="headerlink" href="#span-style-color-orange-types-of-learning-span" title="Permalink to this heading">#</a></h2>
<p><a class="reference internal" href="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-TypesOfLearning.png"><img alt="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-TypesOfLearning.png" class="align-left" src="https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-TypesOfLearning.png" style="width: 800px;" /></a></img><br></p>
<section id="span-style-color-lightgreen-supervised-learning-span">
<h3><span style="color:LightGreen">Supervised Learning</span><a class="headerlink" href="#span-style-color-lightgreen-supervised-learning-span" title="Permalink to this heading">#</a></h3>
<p><span style="color:Violet">Supervised learning</span>, also known as supervised machine learning, is where machines are taught by example. It is defined by its use of labeled datasets to train algorithms to classify new data or predict outcomes accurately. As input data is fed into the model, the model adjusts its weights until it has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids overfitting or underfitting. Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, naïve bayes, linear regression, logistic regression, random forest, and support vector machine (SVM).</p>
<p>There are two main categories of supervised learning that are mentioned below:</p>
<ul class="simple">
<li><p><em><strong><span style="color:Tan">Classification</span></strong></em>: Classification is a process of categorizing data or objects into predefined categories based on their features or attributes and determining to what category new observations belong.</p></li>
<li><p><em><strong><span style="color:Tan">Regression</span></strong></em>: Regression is a process to estimate the relationships among variables when the output variable is a real or continuous value.</p></li>
</ul>
<p><em><strong><span style="color:LightBlue">Advantages of Supervised Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>Supervised Learning models can have high accuracy as they are trained on labelled data.</p></li>
<li><p>The process of decision-making in supervised learning models is often interpretable.</p></li>
<li><p>It can often be used in pre-trained models which saves time and resources when developing new models from scratch.</p></li>
</ul>
<p><em><strong><span style="color:LightBlue">Disadvantages of Supervised Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>It has limitations in knowing patterns and may struggle with unseen or unexpected patterns that are not present in the training data.</p></li>
<li><p>It can be time-consuming and costly as it relies on labeled data only.</p></li>
<li><p>It may lead to poor generalizations based on new data.</p></li>
</ul>
</section>
<section id="span-style-color-lightgreen-unsupervised-learning-span">
<h3><span style="color:LightGreen">Unsupervised Learning</span><a class="headerlink" href="#span-style-color-lightgreen-unsupervised-learning-span" title="Permalink to this heading">#</a></h3>
<p><span style="color:Violet">Unsupervised learning</span> is a machine learning technique in which an algorithm discovers patterns and relationships using unlabeled data. Unlike supervised learning, unsupervised learning doesn’t involve providing the algorithm with labeled target outputs. The primary goal of Unsupervised learning is often to discover hidden patterns, similarities, or clusters within the data, which can then be used for various purposes, such as data exploration, visualization, dimensionality reduction, and more.</p>
<p>There are two main categories of unsupervised learning that we have already studied extensively:</p>
<ul class="simple">
<li><p><em><strong><span style="color:Tan">Clustering</span></strong></em>: Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them.</p></li>
<li><p><em><strong><span style="color:Tan">Dimensionality Reduction</span></strong></em>: Dimensionality reduction is a technique used to reduce the number of features in a dataset while retaining as much of the important information as possible. In other words, it is a process of transforming high-dimensional data into a lower-dimensional space that still preserves the essence of the original data. This can be done for a variety of reasons, such as to reduce the complexity of a model, to improve the performance of a learning algorithm, or to make it easier to visualize the data.</p></li>
</ul>
<p><em><strong><span style="color:LightBlue">Advantages of Unsupervised Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>It helps to discover hidden patterns and various relationships between the data.</p></li>
<li><p>Used for tasks such as anomaly detection and data exploration. Techniques such as autoencoders and dimensionality reduction that can be used to extract meaningful features from raw data.</p></li>
<li><p>It does not require labeled data and reduces the effort of data labeling.</p></li>
</ul>
<p><em><strong><span style="color:LightBlue">Disadvantages of Unsupervised Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>Without using labels, it may be difficult to predict the quality of the model’s output.</p></li>
<li><p>Cluster Interpretability may not be clear and may not have meaningful interpretations.</p></li>
</ul>
</section>
<section id="span-style-color-lightgreen-semi-supervised-learning-span">
<h3><span style="color:LightGreen">Semi-Supervised Learning</span><a class="headerlink" href="#span-style-color-lightgreen-semi-supervised-learning-span" title="Permalink to this heading">#</a></h3>
<p><span style="color:Violet">Semi-supervised learning</span> is a type of machine learning that falls in between supervised and unsupervised learning. It is a method that uses a small amount of labeled data and a large amount of unlabeled data to train a model. The goal of semi-supervised learning is to learn a function that can accurately predict the output variable based on the input variables, similar to supervised learning. However, unlike supervised learning, the algorithm is trained on a dataset that contains both labeled and unlabeled data. Semi-supervised learning is particularly useful when there is a large amount of unlabeled data available, but it’s too expensive or difficult to label all of it.</p>
<p><em><strong><span style="color:LightBlue">Advantages of Semi-supervised Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>It leads to better generalization as compared to supervised learning, as it takes both labeled and unlabeled data.</p></li>
<li><p>Can be applied to a wide range of data.</p></li>
</ul>
<p><em><strong><span style="color:LightBlue">Disdvantages of Semi-supervised Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>Semi-supervised methods can be more complex to implement compared to other approaches.</p></li>
<li><p>It still requires some labeled data that might not always be available or easy to obtain.</p></li>
<li><p>The unlabeled data can impact the model performance accordingly.</p></li>
</ul>
</section>
<section id="span-style-color-lightgreen-reinforcement-learning-span">
<h3><span style="color:LightGreen">Reinforcement Learning</span><a class="headerlink" href="#span-style-color-lightgreen-reinforcement-learning-span" title="Permalink to this heading">#</a></h3>
<p><span style="color:Violet">Reinforcement learning</span> is a learning method that interacts with an environment by producing actions and discovering errors. It is the science of decision making - learning the optimal behavior in an environment to obtain maximum reward. Trial, error, and delay are the most relevant characteristics of reinforcement learning. These methods allows machines to become autonomous, self-learners that automatically determine the ideal behaviour within specific context in order to maximize performance. This type of learning is crucial for applications that involve decision-making in unpredictable environments.</p>
<p><em><strong><span style="color:LightBlue">Advantages of Reinforcement Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>It has autonomous decision-making that is well-suited for tasks and that can learn to make a sequence of decisions without human guidance, like robotics and game-playing. For promising science and engineering applications, it can be used for beam controls in particle acclerators, steering of high-temperature plasma in fusion systems, just to name a few.</p></li>
<li><p>This technique is preferred to achieve long-term results that are very difficult to achieve.</p></li>
<li><p>It is used to solve a complex problems that cannot be solved by conventional techniques.</p></li>
</ul>
<p><em><strong><span style="color:LightBlue">Disadvantages of Reinforcement Machine Learning</span></strong></em>:</p>
<ul class="simple">
<li><p>Training Reinforcement Learning agents can be computationally expensive and time-consuming.</p></li>
<li><p>Reinforcement learning is not preferable to solving simple problems.</p></li>
<li><p>It needs a lot of data and a lot of computation, which makes it impractical and costly.</p></li>
</ul>
</section>
</section>
<section id="span-style-color-orange-learning-in-a-probabilistic-context-span">
<h2><span style="color:Orange">Learning in a Probabilistic Context</span><a class="headerlink" href="#span-style-color-orange-learning-in-a-probabilistic-context-span" title="Permalink to this heading">#</a></h2>
<section id="span-style-color-lightgreen-learning-a-model-span">
<h3><span style="color:LightGreen">Learning a Model</span><a class="headerlink" href="#span-style-color-lightgreen-learning-a-model-span" title="Permalink to this heading">#</a></h3>
<p>We have focused recently on two questions:</p>
<ul class="simple">
<li><p><strong>Assuming a model, which parameters best explain my data?</strong></p></li>
<li><p><strong>Given competing models, what are the relative odds that they explain my data?</strong></p></li>
</ul>
<p>In the framework of Bayesian inference, we answer the first question by estimating the posterior</p>
<div class="math notranslate nohighlight">
\[ \Large
P(\Theta_M\mid D, M) = \frac{P(D\mid \Theta_M, M)\, P(\Theta_M, M)}{P(D\mid M)}
\]</div>
<p>for the assumed model <span class="math notranslate nohighlight">\(M\)</span> and its parameters <span class="math notranslate nohighlight">\(\Theta_M\)</span>.  We answer the second question by estimating the posterior ratio:</p>
<div class="math notranslate nohighlight">
\[ \Large
\frac{P(M_1\mid D)}{P(M_2\mid D)} = \frac{P(D\mid M_1)\, P(M_1)}{P(D\mid M_2)\, P(M_2)} \; .
\]</div>
<p>In either case, the fundamental object is the joint probability,</p>
<div class="math notranslate nohighlight">
\[ \Large
P(D, \Theta_M, M),
\]</div>
<p>or, with competing models, <span class="math notranslate nohighlight">\(M_1\)</span> and <span class="math notranslate nohighlight">\(M_2\)</span>, the pair of joint probabilities,</p>
<div class="math notranslate nohighlight">
\[ \Large
P(D, \Theta_{M_1}, M_1) \quad , \quad
P(D, \Theta_{M_2}, M_2) \; .
\]</div>
<p>These are the fundamental objects since any conditional or marginalized probability can be derived from them.  Note that the observed random variables <span class="math notranslate nohighlight">\(D\)</span> are given, but the unobserved (latent) random variables <span class="math notranslate nohighlight">\(\Theta_M\)</span> require that we make a choice of model(s).</p>
</section>
<section id="id1">
<h3><span style="color:LightGreen">Unsupervised Learning</span><a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The questions above concern models and their parameters.  However, we can also ask questions about “new” data, where “new” means either not yet observed or else already observed but omitted from our analysis.  For example:</p>
<ul class="simple">
<li><p><em><strong><span style="color:Violet">KEY QUESTION</span></strong></em>: <strong>Given observed data <span class="math notranslate nohighlight">\(D\)</span>, how likely is unobserved data <span class="math notranslate nohighlight">\(D'\)</span>?</strong></p></li>
</ul>
<p>This is the fundamental question of <span style="color:Violet">unsupervised learning</span>, and can be framed in probabilistic language starting from the joint probability</p>
<div class="math notranslate nohighlight">
\[ \Large
P(D, D', \Theta_M, M)
\]</div>
<p>assuming the model <span class="math notranslate nohighlight">\(M\)</span> with parameters <span class="math notranslate nohighlight">\(\Theta_M\)</span>.</p>
<p>To answer this question, we must estimate:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \Large
\begin{aligned}
P(D'\mid D, M) &amp;= \int d\Theta_M\, P(D',\Theta_M\mid D, M) \\
&amp;= \int d\Theta_M\, P(D'\mid D,\Theta_M, M)\,P(\Theta_M\mid D, M)\\
&amp;= \int d\Theta_M\, \frac{P(D',D\mid \Theta_M, M)}{P(D\mid \Theta_M, M)}\, P(\Theta_M\mid D, M) \;. \\
\end{aligned}
\end{split}\]</div>
<p>If we assume that <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(D'\)</span> are statistically independent datasets, then</p>
<div class="math notranslate nohighlight">
\[ \Large
P(D',D\mid \Theta_M, M) = P(D'\mid\Theta_M, M)\, P(D\mid \Theta_M, M) \; ,
\]</div>
<p>and we can simplify</p>
<div class="math notranslate nohighlight">
\[ \Large
P(D'\mid D, M) = \int d\Theta_M\, P(D'\mid\Theta_M, M)\, P(\Theta_M\mid D, M) \; .
\]</div>
<p>Note that in order to evaluate the RHS, we must have already learned the model <span class="math notranslate nohighlight">\(M\)</span> and determined the posterior <span class="math notranslate nohighlight">\(P(\Theta_M\mid D, M)\)</span> of its parameters.</p>
</section>
<section id="id2">
<h3><span style="color:LightGreen">Supervised Learning</span><a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>If we split the features of our data <span class="math notranslate nohighlight">\(D\)</span> into two categories, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, we can ask a new question about unobserved data <span class="math notranslate nohighlight">\(D'\)</span> that splits into <span class="math notranslate nohighlight">\(X'\)</span> and <span class="math notranslate nohighlight">\(Y'\)</span>:</p>
<ul class="simple">
<li><p><em><strong><span style="color:Violet">KEY QUESTION</span></strong></em>: <strong>Given observed data <span class="math notranslate nohighlight">\((X,Y)\)</span> from <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(X'\)</span> from <span class="math notranslate nohighlight">\(D'\)</span>, how likely is the remaining unobserved data <span class="math notranslate nohighlight">\(Y'\)</span>?</strong></p></li>
</ul>
<p>This is the fundamental question of <span style="color:Violet">supervised learning</span>, and the relevant joint probability is now</p>
<div class="math notranslate nohighlight">
\[ \Large
P(D, D', \Theta_M, M) = P(X, Y, X', Y', \Theta_M, M)
\]</div>
<p>for the assumed model <span class="math notranslate nohighlight">\(M\)</span> with parameters <span class="math notranslate nohighlight">\(\Theta_M\)</span>.</p>
<p>To answer this question, we can estimate:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \Large
\begin{aligned}
P(Y'\mid X, Y, X', M) &amp;= \int d\Theta_M\, ~P(Y',\Theta_M\mid X, Y, X', M) \\
&amp;= \int d\Theta_M\, ~P(Y'\mid X, Y, X',\Theta_M, M)\, ~P(\Theta_M\mid X, Y, X', M) \\
&amp;= \int d\Theta_M\, \frac{P(X, Y, X', Y'\mid \Theta_M, M)}{P(X, Y, X'\mid \Theta_M, M)}\,
~P(\Theta_M\mid X, Y, X', M) \; .
\end{aligned}
\end{split}\]</div>
<p>If we again assume that <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(D'\)</span> are statistically independent, then</p>
<div class="math notranslate nohighlight">
\[ \Large
P(X,Y,X',Y'\mid\Theta_M,M) = P(X',Y'\mid\Theta_M,M)\, ~P(X,Y\mid\Theta_M,M) \; ,
\]</div>
<p>and (after integrating out <span class="math notranslate nohighlight">\(X'\)</span>),</p>
<div class="math notranslate nohighlight">
\[ \Large
P(X,Y,Y'\mid\Theta_M,M) = P(Y'\mid\Theta_M,M)\, ~P(X,Y\mid\Theta_M,M) \; .
\]</div>
<p>We can then simplify:</p>
<div class="math notranslate nohighlight">
\[ \Large
P(Y'\mid X, Y, X', M) = \int d\Theta_M\,
\frac{P(X', Y'\mid \Theta_M, M)}{P(X'\mid \Theta_M, M)}\, ~P(\Theta_M\mid X, Y, X', M) \; .
\]</div>
<p>Note that this formulation of the problem has <span class="math notranslate nohighlight">\(P(\Theta_M\mid X, Y, X', M)\)</span> on the RHS, which indicates that we must re-learn the model <span class="math notranslate nohighlight">\(M\)</span> each time we are given new data <span class="math notranslate nohighlight">\(X'\)</span>.</p>
<p>An alternative formulation reveals that, while valid, this is not necessary: start from the unsupervised result above, with <span class="math notranslate nohighlight">\(D=(X,Y)\)</span> and <span class="math notranslate nohighlight">\(D'=(X',Y')\)</span>,</p>
<div class="math notranslate nohighlight">
\[ \Large
P(X',Y'\mid X,Y, M) = \int d\Theta_M\, ~P(X',Y'\mid\Theta_M, M)\, ~P(\Theta_M\mid X,Y, M)
\]</div>
<p>then integrate out <span class="math notranslate nohighlight">\(X'\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \Large
\begin{aligned}
P(X'\mid X,Y, M) &amp;= \int d\Theta_M\, \left[ \int dX'\, ~P(X',Y'\mid\Theta_M, M)\right] 
~P(\Theta_M\mid X,Y, M) \\
&amp;= \int d\Theta_M\, ~P(Y'\mid\Theta_M, M)\, ~P(\Theta_M\mid X,Y, M) \; .
\end{aligned}
\end{split}\]</div>
<p>We can now answer our original question using</p>
<div class="math notranslate nohighlight">
\[\begin{split} \Large
\begin{aligned}
P(Y'\mid X,Y,X',M) &amp;= \frac{P(P(X',Y'\mid X,Y, M)}{P(X'\mid X,Y, M)} \\
&amp;= \frac{\int d\Theta_M\, ~P(X',Y'\mid\Theta_M, M)\, ~P(\Theta_M\mid X,Y, M)}
{\int d\Theta_M\, ~P(Y'\mid\Theta_M, M)\, ~P(\Theta_M\mid X,Y, M)} \; .
\end{aligned}
\end{split}\]</div>
<p>Note how this formulation allows us to learn the model <span class="math notranslate nohighlight">\(M\)</span> once with the original data <span class="math notranslate nohighlight">\(D=(X,Y)\)</span> but requires two separate marginalizations (integrals) over the model parameters <span class="math notranslate nohighlight">\(\Theta_M\)</span>.</p>
</section>
<section id="span-style-color-lightgreen-terminology">
<h3><span style="color:LightGreen">Terminology<a class="headerlink" href="#span-style-color-lightgreen-terminology" title="Permalink to this heading">#</a></h3>
<p>The data <span class="math notranslate nohighlight">\(D\)</span> used to learn the model used in unsupervised or supervised learning is referred as the <span style="color:Violet">training data</span>.  In supervised learning, the features appearing in <span class="math notranslate nohighlight">\(X\)</span> are the <span style="color:Violet">input features</span> and <span class="math notranslate nohighlight">\(Y\)</span> are referred to as the <span style="color:Violet">target features</span>.</p>
<p>We use different terminology (and approaches to modeling) for supervised learning depending on the type of target features we wish to learn:</p>
<ul class="simple">
<li><p><span style="color:Tan">regression</span> <span class="math notranslate nohighlight">\(~~\)</span> : predict continuous-value target features.</p></li>
<li><p><span style="color:Tan">classification</span>: predict discrete-valued target features.</p></li>
</ul>
<p>Note that the target features might be a mix of continuous and discrete features, so this terminology is incomplete.</p>
<p>Most of the high-profile machine learning applications from Google, Facebook, etc, involve classification rather than regression, so proportionally more effort has gone into developing and optimizing classification algorithms. However, most scientific applications are more naturally expressed as regression problems: this presents both a challenge and an opportunity to the scientific ML community!  Also note that a regression problem can always be converted into a classification problem by binning the output (assuming you don’t need infinite accuracy), which can be surprisingly effective.</p>
<p>All unsupervised and supervised learning algorithms involve priors, <span class="math notranslate nohighlight">\(P(\Theta_M\mid M)\)</span> but they are not always stated explicitly. Sometimes priors are expressed implicitly via terms that are referred to as <span style="color:Violet">regularization conditions</span>.</p>
</section>
<section id="span-style-color-lightgreen-model-selection-revisited-span">
<h3><span style="color:LightGreen">Model Selection Revisited</span><a class="headerlink" href="#span-style-color-lightgreen-model-selection-revisited-span" title="Permalink to this heading">#</a></h3>
<p>When learning a model <span class="math notranslate nohighlight">\(M\)</span>, our quantitative measure of how well it explains the data <span class="math notranslate nohighlight">\(D\)</span> is the evidence <span class="math notranslate nohighlight">\(P(D\mid M)\)</span>.</p>
<p>Since unsupervised and supervised learning requires first learning the model, we could use the same measure, but a different measure is also useful: how well does <span class="math notranslate nohighlight">\(M\)</span> explain unobserved data <span class="math notranslate nohighlight">\(D'\)</span>?  In other words, how well does the learned model generalize and offer predictive power?  In order to answer this question, in practice, you must hold back some of your observed data when learning the model and can then measure how well the model “postdicts” the held back data. The held back data is referred to as the <span style="color:Violet">test sample</span> and this process is known as <span style="color:Violet">cross validation</span>.</p>
<p>In cases where the model and its parameters have some physical reality (for example, projectile motion modeled with Newtonian physics and parameterized by <span class="math notranslate nohighlight">\(g\)</span>, …), these measures are essentially the same since the model is dictated by some independent reality and not chosen specifically to explain the observed data <span class="math notranslate nohighlight">\(D\)</span>.</p>
<p>However, when predicting future data is the main goal and there is no first-principles model available, the model and its parameters are essentially unconstrained and these two measures can easily diverge.  In particular, optimizing how well the model explains the observed data leads to <span style="color:Violet">over-fitting</span> and poor ability to <span style="color:Violet">generalize</span> to new data.</p>
<p>For example, suppose the observed data <span class="math notranslate nohighlight">\(D\)</span> consists of <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(x_i\)</span> of a single feature <span class="math notranslate nohighlight">\(x\)</span>, then a model <span class="math notranslate nohighlight">\(M\)</span> with the likelihood (<span class="math notranslate nohighlight">\(\delta_D\)</span> is the Dirac delta function)</p>
<div class="math notranslate nohighlight">
\[ \Large
P(x\mid \Theta_M, M) = \frac{1}{N}\, \sum_{i=1}^N \delta_D(x - x_i)
\]</div>
<p>trivially explains the data perfectly with the parameters</p>
<div class="math notranslate nohighlight">
\[ \Large
\Theta_M = \{ x_1, x_2, \ldots, x_N \} \; .
\]</div>
<p>This purely empirical approach to model building is an extreme case of over-fitting and offers no generalization power. (Note that the likelihood above is a kernel density estimate with a Dirac delta function kernel).</p>
</section>
</section>
<hr class="docutils" />
<section id="span-style-color-orange-acknowledgments-span">
<h2><span style="color:Orange">Acknowledgments</span><a class="headerlink" href="#span-style-color-orange-acknowledgments-span" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Initial version: Mark Neubauer</p></li>
</ul>
<p>© Copyright 2023</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./_sources/lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../Week_09.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span style="color: blue;"><b>Learning &amp; Cross Validation</b></span></p>
      </div>
    </a>
    <a class="right-next"
       href="CrossValidation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Cross Validation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-overview-span"><span style="color:Orange">Overview</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-types-of-learning-span"><span style="color:Orange">Types of Learning</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-supervised-learning-span"><span style="color:LightGreen">Supervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-unsupervised-learning-span"><span style="color:LightGreen">Unsupervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-semi-supervised-learning-span"><span style="color:LightGreen">Semi-Supervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-reinforcement-learning-span"><span style="color:LightGreen">Reinforcement Learning</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-learning-in-a-probabilistic-context-span"><span style="color:Orange">Learning in a Probabilistic Context</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-learning-a-model-span"><span style="color:LightGreen">Learning a Model</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><span style="color:LightGreen">Unsupervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><span style="color:LightGreen">Supervised Learning</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-terminology"><span style="color:LightGreen">Terminology</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-lightgreen-model-selection-revisited-span"><span style="color:LightGreen">Model Selection Revisited</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#span-style-color-orange-acknowledgments-span"><span style="color:Orange">Acknowledgments</span></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Mark Neubauer
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>