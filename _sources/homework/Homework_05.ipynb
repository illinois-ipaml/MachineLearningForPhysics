{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05: Kernel Density Estimation, Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for Getting, Loading and Locating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_data(url: str):\n",
    "    local_path = './tmp_data'\n",
    "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
    "    rc = None\n",
    "    while rc is None:\n",
    "      line = p.stderr.readline().strip('\\n')\n",
    "      if len(line) > 0:\n",
    "        print(line)\n",
    "      rc = p.poll()\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement the core of the E- and M-steps for the [Gaussian mixture model (GMM)](http://scikit-learn.org/stable/modules/mixture.html) method. Note the similarities with the E- and M-steps of the K-means method.\n",
    "\n",
    "First, implement the function below to evaluate the [multidimensional Gaussian probability density](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) for arbitrary mean $\\vec{\\mu}$ and covariance matrix $C$ (refer to the lecture for more details on the notation used here):\n",
    "\n",
    "$$ \\Large\n",
    "G(\\vec{x} ; \\vec{\\mu}, C) = \\left(2\\pi\\right)^{-D/2}\\,\\left| C\\right|^{-1/2}\\,\n",
    "\\exp\\left[  -\\frac{1}{2} \\left(\\vec{x} - \\vec{\\mu}\\right)^T C^{-1} \\left(\\vec{x} - \\vec{\\mu}\\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mu, C):\n",
    "    \"\"\"Evaluate the Gaussian probability density.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        1D array of D feature values for a single sample\n",
    "    mu : array\n",
    "        1D array of D mean feature values for this component.\n",
    "    C : array\n",
    "        2D array with shape (D, D) of covariance matrix elements for this component.\n",
    "        Must be positive definite (and therefore symmetric).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Probability density.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    mu = np.asarray(mu)\n",
    "    C = np.asarray(C)\n",
    "    D = len(x)\n",
    "    assert x.shape == (D,) and mu.shape == (D,)\n",
    "    assert C.shape == (D, D)\n",
    "    assert np.allclose(C.T, C)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [1], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[2]]), 1 / np.sqrt(4 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [0], [[1]]), np.exp(-0.5) / np.sqrt(2 * np.pi))\n",
    "\n",
    "assert np.allclose(Gaussian_pdf([0, 0], [0, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, -1], [1, -1], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[4, 0], [0, 1]]), 1 / (4 * np.pi))\n",
    "\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, +1], [+1, 1]]), 5) == 0.07778\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, -1], [-1, 1]]), 5) == 0.07778\n",
    "assert np.round(np.log(Gaussian_pdf([1, 0, -1], [1, 2, 3], [[4, -1, 0], [-1, 1, 0], [0, 0, 2]])), 5) == -10.31936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the E-step in the function below. This consists of calculating the relative probability that each sample $\\vec{x}_n$ ($n$-th row of $X$) belongs to each component $k$:\n",
    "\n",
    "$$ \\Large\n",
    "p_{nk} = \\frac{\\omega_k G(\\vec{x}_n; \\vec{\\mu}_k, C_k)}\n",
    "{\\sum_{j=1}^K\\, \\omega_j G(\\vec{x}_n; \\vec{\\mu}_j, C_j)}\n",
    "$$\n",
    "\n",
    "Note that these relative probabilities (also called *responsibilities*) sum to one over components $k$ for each sample $n$.  Also note that we consider the parameters ($\\omega_k$, $\\vec{\\mu}_k$, $C_k$) of each component fixed during this step. *Hint: use your `Gaussian_pdf` function here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, w, mu, C):\n",
    "    \"\"\"Perform a GMM E-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    w : array with shape (K,)\n",
    "        Per-component weights.\n",
    "    mu : array with shape (K, D)\n",
    "        Array of mean vectors for each component.\n",
    "    C : array with shape (K, D, D).\n",
    "        Array of covariance matrices for each component.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(w)\n",
    "    assert w.shape == (K,)\n",
    "    assert mu.shape == (K, D)\n",
    "    assert C.shape == (K, D, D)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "w = np.full(4, 0.25)\n",
    "mu = np.array([[-2], [-1], [0], [1]])\n",
    "C = np.ones((4, 1, 1))\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.258,  0.134,  0.058,  0.021,  0.006],\n",
    "     [ 0.426,  0.366,  0.258,  0.152,  0.077],\n",
    "     [ 0.258,  0.366,  0.426,  0.414,  0.346],\n",
    "     [ 0.058,  0.134,  0.258,  0.414,  0.57 ]])\n",
    "\n",
    "X = np.zeros((1, 3))\n",
    "w = np.ones((2,))\n",
    "mu = np.zeros((2, 3))\n",
    "C = np.zeros((2, 3, 3))\n",
    "diag = range(3)\n",
    "C[:, diag, diag] = 1\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.5], [ 0.5]])\n",
    "\n",
    "X = np.array([[0,0,0], [1,0,0]])\n",
    "mu = np.array([[0,0,0], [1,0,0]])\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.622,  0.378], [ 0.378,  0.622]])\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "subsample = X.reshape(K, (N//K), D)\n",
    "mu = subsample.mean(axis=1)\n",
    "C = np.empty((K, D, D))\n",
    "w = gen.uniform(size=K)\n",
    "w /= w.sum()\n",
    "for k in range(K):\n",
    "    C[k] = np.cov(subsample[k], rowvar=False)\n",
    "#print(repr(np.round(E_step(X, w, mu, C)[:, :5], 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C)[:, :5], 3) ==\n",
    "    [[ 0.422,  0.587,  0.344,  0.279,  0.19 ],\n",
    "     [ 0.234,  0.11 ,  0.269,  0.187,  0.415],\n",
    "     [ 0.291,  0.194,  0.309,  0.414,  0.279],\n",
    "     [ 0.053,  0.109,  0.077,  0.12 ,  0.116]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the M-step in the function below.  During this step, we consider the relative weights $p_{nk}$ from the previous step fixed and instead update the parameters of each component (which were fixed in the previous step), using:\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{aligned}\n",
    "\\omega_k &= \\frac{1}{N}\\, \\sum_{n=1}^N\\, p_{nk} \\\\\n",
    "\\vec{\\mu}_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\vec{x}_n}{\\sum_{n=1}^N\\, p_{nk}} \\\\\n",
    "C_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\left( \\vec{x}_n - \\vec{\\mu}_k\\right) \\left( \\vec{x}_n - \\vec{\\mu}_k\\right)^T}\n",
    "{\\sum_{n=1}^N\\, p_{nk}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Make sure you understand why the last expression yields a matrix rather than a scalar dot product before jumping into the code. (If you would like a numpy challenge, try implementing this function without any loops, e.g., with `np.einsum`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(X, p):\n",
    "    \"\"\"Perform a GMM M-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    p : array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple w, mu, C of arrays with shapes (K,), (K, D) and (K, D, D) giving\n",
    "        the updated component parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(p)\n",
    "    assert p.shape == (K, N)\n",
    "    assert np.allclose(p.sum(axis=0), 1)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "p = np.full(20, 0.25).reshape(4, 5)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C, 5)))\n",
    "assert np.all(np.round(w, 5) == 0.25)\n",
    "assert np.all(np.round(mu, 5) == 0.0)\n",
    "assert np.all(np.round(C, 5) == 0.5)\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "p = gen.uniform(size=(K, N))\n",
    "p /= p.sum(axis=0)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C[0], 5)))\n",
    "assert np.all(\n",
    "    np.round(w, 5) == [ 0.25216,  0.24961,  0.24595,  0.25229])\n",
    "assert np.all(\n",
    "    np.round(mu, 5) ==\n",
    "    [[ 0.06606,  0.06   , -0.00413,  0.01562,  0.00258],\n",
    "     [ 0.02838,  0.01299,  0.01286,  0.03068, -0.01714],\n",
    "     [ 0.03157,  0.04558, -0.01206,  0.03493, -0.0326 ],\n",
    "     [ 0.05467,  0.06293, -0.01779,  0.04454,  0.00065]])\n",
    "assert np.all(\n",
    "    np.round(C[0], 5) ==\n",
    "    [[ 0.98578,  0.01419, -0.03717,  0.01403,  0.0085 ],\n",
    "     [ 0.01419,  0.95534, -0.02724,  0.03201, -0.00648],\n",
    "     [-0.03717, -0.02724,  0.90722,  0.00313,  0.0299 ],\n",
    "     [ 0.01403,  0.03201,  0.00313,  1.02891,  0.0813 ],\n",
    "     [ 0.0085 , -0.00648,  0.0299 ,  0.0813 ,  0.922  ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented the core of the GMM algorithm. Next you will use KMeans as a means of seeding the GMM model fit. First we include two helpful functions `draw_ellipses` and `GMM_parplot` to help display results. The details of these methods are not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "from matplotlib.collections import EllipseCollection\n",
    "\n",
    "def draw_ellipses(w, mu, C, nsigmas=2, color='red', outline=None, filled=True, axis=None):\n",
    "    \"\"\"Draw a collection of ellipses.\n",
    "\n",
    "    Uses the low-level EllipseCollection to efficiently draw a large number\n",
    "    of ellipses. Useful to visualize the results of a GMM fit via\n",
    "    GMM_pairplot() defined below.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array\n",
    "        1D array of K relative weights for each ellipse. Must sum to one.\n",
    "        Ellipses with smaller weights are rendered with greater transparency\n",
    "        when filled is True.\n",
    "    mu : array\n",
    "        Array of shape (K, 2) giving the 2-dimensional centroids of\n",
    "        each ellipse.\n",
    "    C : array\n",
    "        Array of shape (K, 2, 2) giving the 2 x 2 covariance matrix for\n",
    "        each ellipse.\n",
    "    nsigmas : float\n",
    "        Number of sigmas to use for scaling ellipse area to a confidence level.\n",
    "    color : matplotlib color spec\n",
    "        Color to use for the ellipse edge (and fill when filled is True).\n",
    "    outline : None or matplotlib color spec\n",
    "        Color to use to outline the ellipse edge, or no outline when None.\n",
    "    filled : bool\n",
    "        Fill ellipses with color when True, adjusting transparency to\n",
    "        indicate relative weights.\n",
    "    axis : matplotlib axis or None\n",
    "        Plot axis where the ellipse collection should be drawn. Uses the\n",
    "        current default axis when None.\n",
    "    \"\"\"\n",
    "    # Calculate the ellipse angles and bounding boxes using SVD.\n",
    "    U, s, _ = np.linalg.svd(C)\n",
    "    angles = np.degrees(np.arctan2(U[:, 1, 0], U[:, 0, 0]))\n",
    "    widths, heights = 2 * nsigmas * np.sqrt(s.T)\n",
    "    # Initialize colors.\n",
    "    color = colorConverter.to_rgba(color)\n",
    "    if filled:\n",
    "        # Use transparency to indicate relative weights.\n",
    "        ec = np.tile([color], (len(w), 1))\n",
    "        ec[:, -1] *= w\n",
    "        fc = np.tile([color], (len(w), 1))\n",
    "        fc[:, -1] *= w ** 2\n",
    "    # Data limits must already be defined for axis.transData to be valid.\n",
    "    axis = axis or plt.gca()\n",
    "    if outline is not None:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=4,\n",
    "            transOffset=axis.transData, facecolors='none', edgecolors=outline))\n",
    "    if filled:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=2,\n",
    "            transOffset=axis.transData, facecolors=fc, edgecolors=ec))\n",
    "    else:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=2.5,\n",
    "            transOffset=axis.transData, facecolors='none', edgecolors=color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_pairplot(data, w, mu, C, limits=None, entropy=False):\n",
    "    \"\"\"Display 2D projections of a Gaussian mixture model fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas DataFrame\n",
    "        N samples of D-dimensional data.\n",
    "    w : array\n",
    "        1D array of K relative weights for each ellipse. Must sum to one.\n",
    "    mu : array\n",
    "        Array of shape (K, 2) giving the 2-dimensional centroids of\n",
    "        each ellipse.\n",
    "    C : array\n",
    "        Array of shape (K, 2, 2) giving the 2 x 2 covariance matrix for\n",
    "        each ellipse.\n",
    "    limits : array or None\n",
    "        Array of shape (D, 2) giving [lo,hi] plot limits for each of the\n",
    "        D dimensions. Limits are determined by the data scatter when None.\n",
    "    \"\"\"\n",
    "    colnames = data.columns.values\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    if entropy:\n",
    "        n_components = len(w)\n",
    "        # Pick good colors to distinguish the different clusters.\n",
    "        cmap = ListedColormap(\n",
    "            sns.color_palette('husl', n_components).as_hex())\n",
    "        # Calculate the relative probability that each sample belongs to each cluster.\n",
    "        # This is equivalent to fit.predict_proba(X)\n",
    "        lnprob = np.zeros((n_components, N))\n",
    "        for k in range(n_components):\n",
    "            lnprob[k] = scipy.stats.multivariate_normal.logpdf(X, mu[k], C[k])\n",
    "        lnprob += np.log(w)[:, np.newaxis]\n",
    "        prob = np.exp(lnprob)\n",
    "        prob /= prob.sum(axis=0)\n",
    "        prob = prob.T\n",
    "        # Assign each sample to its most probable cluster.\n",
    "        labels = np.argmax(prob, axis=1)\n",
    "        color = cmap(labels)\n",
    "        if n_components > 1:\n",
    "            # Calculate the relative entropy (0-1) as a measure of cluster assignment ambiguity.\n",
    "            relative_entropy = -np.sum(prob * np.log(prob), axis=1) / np.log(n_components)\n",
    "            color[:, :3] *= (1 - relative_entropy).reshape(-1, 1)        \n",
    "    # Build a pairplot of the results.\n",
    "    fs = 5 * min(D - 1, 3)\n",
    "    fig, axes = plt.subplots(D - 1, D - 1, sharex='col', sharey='row',\n",
    "                             squeeze=False, figsize=(fs, fs))\n",
    "    for i in range(1, D):\n",
    "        for j in range(D - 1):\n",
    "            ax = axes[i - 1, j]\n",
    "            if j >= i:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            # Plot the data in this projection.\n",
    "            if entropy:\n",
    "                ax.scatter(X[:, j], X[:, i], s=5, c=color, cmap=cmap)\n",
    "                draw_ellipses(\n",
    "                    w, mu[:, [j, i]], C[:, [[j], [i]], [[j, i]]],\n",
    "                    color='w', outline='k', filled=False, axis=ax)\n",
    "            else:\n",
    "                ax.scatter(X[:, j], X[:, i], s=10, alpha=0.3, c='k', lw=0)\n",
    "                draw_ellipses(\n",
    "                    w, mu[:, [j, i]], C[:, [[j], [i]], [[j, i]]],\n",
    "                    color='red', outline=None, filled=True, axis=ax)\n",
    "            # Overlay the fit components in this projection.\n",
    "            # Add axis labels and optional limits.\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(colnames[i])\n",
    "                if limits: ax.set_ylim(limits[i])\n",
    "            if i == D - 1:\n",
    "                ax.set_xlabel(colnames[j])\n",
    "                if limits: ax.set_xlim(limits[j])\n",
    "    plt.subplots_adjust(hspace=0.02, wspace=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple wrapper that uses KMeans to initialize the relative probabilities to all be either zero or one, based on each sample's cluster assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_fit(data, n_components, nsteps, init='random', seed=123):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    p = np.zeros((n_components, N))\n",
    "    if init == 'kmeans':\n",
    "        # Use KMeans to divide the data into clusters.\n",
    "        fit = cluster.KMeans(n_clusters=n_components, random_state=gen, n_init=10).fit(data)\n",
    "        # Initialize the relative weights using cluster membership.\n",
    "        # The initial weights are therefore all either 0 or 1.\n",
    "        for k in range(n_components):\n",
    "            p[k, fit.labels_ == k] = 1\n",
    "    else:\n",
    "        # Assign initial relative weights in quantiles of the first feature.\n",
    "        # This is not a good initialization strategy, but shows how well\n",
    "        # GMM converges from a poor starting point.\n",
    "        x0 = X[:, 0]\n",
    "        edges = np.percentile(x0, np.linspace(0, 100, n_components + 1))\n",
    "        for k in range(n_components):\n",
    "            quantile = (edges[k] <= x0) & (x0 <= edges[k + 1])\n",
    "            p[k, quantile] = 1.\n",
    "    # Normalize relative weights.\n",
    "    p /= p.sum(axis=0)\n",
    "    # Perform an initial M step to initialize the component params.\n",
    "    w, mu, C = M_step(X, p)\n",
    "    # Loop over iterations.\n",
    "    for i in range(nsteps):\n",
    "        p = E_step(X, w, mu, C)\n",
    "        w, mu, C = M_step(X, p)\n",
    "    # Plot the results.\n",
    "    GMM_pairplot(data, w, mu, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this out on the 3D `blobs_data` and notice that it converges close to the correct solution after 8 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/blobs_data.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_data = pd.read_hdf(locate_data('blobs_data.hf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0)\n",
    "GMM_fit(blobs_data, 3, nsteps=4)\n",
    "GMM_fit(blobs_data, 3, nsteps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is even faster if you use KMeans to initialize the relative weights (which is why most implementations do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0, init='kmeans')\n",
    "GMM_fit(blobs_data, 3, nsteps=1, init='kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A density estimator should provide a probability density function $P(\\vec{x})$ that is normalized over its feature space $\\vec{x}$\n",
    "\n",
    "$$ \\Large\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) = 1 \\; .\n",
    "$$\n",
    "\n",
    "In this problem you will verify this normalization for KDE using two different numerical approaches for the integral.\n",
    "\n",
    "First, implement the function below to accept a 1D KDE fit object and estimate its normalization integral using the trapezoid rule with the specified grid. *Hint: the `np.trapz` function will be useful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c7c730306185db01f0a5f377b61660a4",
     "grade": false,
     "grade_id": "cell-f08e5071751cdabd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def check_grid_normalization(fit, xlo, xhi, ngrid):\n",
    "    \"\"\"Check 1D denstity estimator fit result normlization using grid quadrature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.KernelDensity fit object\n",
    "        Result of fit to 1D dataset.\n",
    "    xlo : float\n",
    "        Low edge of 1D integration range.\n",
    "    xhi : float\n",
    "        High edge of 1D integration range.\n",
    "    ngrid : int\n",
    "        Number of equally spaced grid points covering [xlo, xhi],\n",
    "        including both end points.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ec1ffc451d8c87dc51acb674601e07b",
     "grade": true,
     "grade_id": "cell-d535958d6d6e7dc6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']).values)\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 5), 3) == 1.351\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 10), 3) == 1.019\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 20), 3) == 0.986\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x2']).values)\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 5), 3) == 1.108\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 10), 3) == 0.993\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 20), 3) == 0.971\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x1']).values)\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 5), 3) == 1.311\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 10), 3) == 0.954\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 20), 3) == 1.028\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 100), 3) == 1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to estimate a multidimensional fit normalization using [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration):\n",
    "\n",
    "$$ \\Large\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) \\simeq \\frac{V}{N_{mc}}\\, \\sum_{j=1}^{N_{mc}} P(\\vec{x}_j) = V \\langle P\\rangle \\; ,\n",
    "$$\n",
    "\n",
    "where the $\\vec{x}_j$ are uniformly distributed over the integration domain and $V$ is the integration domain volume. Note that `trapz` gives more accurate results for a fixed number of $P(\\vec{x})$ evaluations, but MC integration is much easier to generalize to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "039324c2b4b3e33fe81db69b161bfb2f",
     "grade": false,
     "grade_id": "cell-6a773ed33ecb1799",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def check_mc_normalization(fit, xlo, xhi, nmc, seed=123):\n",
    "    \"\"\"Check denstity estimator fit result normlization using MC integration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.KernelDensity fit object\n",
    "        Result of fit to arbitrary dataset of dimension D.\n",
    "    xlo : array\n",
    "        1D array of length D with low limits of integration domain along each dimension.\n",
    "    xhi : array\n",
    "        1D array of length D with high limits of integration domain along each dimension.\n",
    "    nmc : int\n",
    "        Number of random MC integration points within the domain to use.\n",
    "    \"\"\"\n",
    "    xlo = np.asarray(xlo)\n",
    "    xhi = np.asarray(xhi)\n",
    "    assert xlo.shape == xhi.shape\n",
    "    assert np.all(xhi > xlo)\n",
    "    D = len(xlo)\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # Use gen.uniform() in your solution, not gen.rand(), for consistent random numbers.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "276d5e5fe9b26b0e892d8f1327a6fe22",
     "grade": true,
     "grade_id": "cell-7b392286cb763415",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### A correct solution should pass these tests.\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']).values)\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10), 3) == 1.129\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 100), 3) == 1.022\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 1000), 3) == 1.010\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10000), 3) == 0.999\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x2']).values)\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10), 3) == 1.754\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 100), 3) == 1.393\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 1000), 3) == 0.924\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10000), 3) == 1.019\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.values)\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10), 3) == 2.797\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 100), 3) == 0.613\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 1000), 3) == 1.316\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10000), 3) == 1.139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will study the accuracy of Monte Carlo integration in each of four different expressions, each with some physical significance, shown in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Expression # | Function &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp;  &nbsp;|Interval|Notes|\n",
    "|---|---|---|---|\n",
    "|1. |${x = \\int_0^1 7-10t\\ dt} $| $t$ is time; $t = [0, 1]$ s |Gives position at <br>time $t$ for this system|\n",
    "|2. |${\\Delta S}$ ${= \\int_{300}^{400}\\frac{mc}{T}\\ dT }$|$m$ is mass; $m=1$ kg<br> $c$ is specific heat capacity; $c = 4190$ J/kg K<br>$T$ is temperature; $T = [300, 400]$ K|Change in entropy for <br>thermal processes|\n",
    "|3. |$\\Phi = \\int_1^2 \\frac{Q}{4 \\pi \\epsilon_o r^2} dr$|$r$ is distance; $r = [1, 2]$ m<br>$\\epsilon_o$ is the Permittivity of Free Space<br>$Q$ is the charge; $Q = 1$ C|$\\Phi$ is the electric potential energy <br>gained by moving along line $r$|\n",
    "|4. |$I = \\int_0^\\infty \\frac{2 \\pi h c^2}{\\lambda^5(e^{hc/\\lambda k T} - 1)}\\ d\\lambda$|$h$ is Planck's constant<br> $c$ is speed of light <br> $k$ is Boltzmann's Constant <br> $T$ is the absolute temperature; T = 400K <br> $\\lambda$ is wavelength; $\\lambda = [0, \\infty]$ m|Planck's radiation law; <br>Integrating gives Stefan Boltzmann Law|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytically integrate each for the region and values provided, and record your answer in the `analytical_result` variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_result_expr1 = None # replace the None's with your results\n",
    "analytical_result_expr2 = None\n",
    "analytical_result_expr3 = None\n",
    "analytical_result_expr4 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show your work in the cell below, either in a picture file for written derivations or in Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the each expression to be integrated as a python function.  \n",
    "\n",
    "> For example, if I want to integrate the expression\n",
    "> \n",
    "> $$ \\Large \n",
    "> F = \\int 3x^2 + 17\\ dx\n",
    "> $$\n",
    "> \n",
    "> then my integrand is\n",
    "> \n",
    "> $$ \\Large\n",
    "> f(x) = 3x^2 + 17\n",
    "> $$\n",
    "> \n",
    "> and I would write the following function:\n",
    "> ```\n",
    "> def integrand(x):\n",
    ">     f_x = 3*np.power(x, 2) + 17\n",
    ">     return f_x\n",
    "> ```\n",
    "> \n",
    "> This function takes `x` as my function argument, and returns the calculated value `f_x`.  Note that I am not yet evaluating the limits of my integrand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful constants:\n",
    "pi = np.pi #unitless\n",
    "c = 2.99E8 #m/s\n",
    "h = 6.62607015E-34 #J\n",
    "k = 1.380649E-23 #J/K\n",
    "epsilon = 8.854187817E-12 #F/m\n",
    "sigma = 5.6704E-8 #W/(m^2 K^4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrand(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly choose values for `x` from a distribution between the limits of the definite integral. \n",
    "\n",
    ">*Hint*: if one of your limits is $\\infty$, it is okay to approximate it with a large number.  Another way to do it is to plot [x, f(x)] and visually estimate the most important region of your integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_limit = # this can be a float\n",
    "upper_limit = # this can be a float\n",
    "num_x_vals  = # this must be an integer value less than or equal to 10^8\n",
    "x_vals = np.random.uniform(lower_limit, upper_limit, num_x_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the `f_x` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = integrand(x_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_int = ((upper_limit - lower_limit)*np.sum(y))/(num_x_vals - 1)\n",
    "print(f\"The Monte Carlo approximation is {approx_int}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the error between the `approx_int` and the `analytical_result` variables using one or more of the metrics discussed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = None # replace with your calculation\n",
    "print(f\"The Mean Squared Error is {mse}\")\n",
    "\n",
    "pe = None # replace with your calculation\n",
    "print(f\"The Percent Error is {pe}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to visualize how the error decreases as the number of random trials `num_x_vals` increases.  Write code to the do the following:\n",
    "\n",
    "* Using the error metric you decided on above, write a for-loop that calculates the error as a function of the number of points you sampled.  For example, calculate the error when you summed two values of $\\langle F^N \\rangle$, then calculate the error for three summed values of $\\langle F^N \\rangle$, and so on until you have calculated the errors for the full range of $\\langle F^N \\rangle$.\n",
    "\n",
    "* IMPORTANT: You do not need to re-do the experiment to calculate this analysis; if you do it will slow down your for-loop and potentially crash your notebook kernel.  Instead, you will want to reuse all of the integrand values are stored in the `y` variable.  Python indexing into this list using the `y[:N]` functionality will give you the first `N` values in this list.  The first `N` values can then be used to calculate a $\\langle F^N \\rangle$ value for the first `N` samples.\n",
    "\n",
    "* Make a figure showing how the error changes with the number of values in the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data = [] \n",
    "# Write code here to fill error_data with the percentage error corresponding to each of the number of points you sampled in the MC integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(2, 2000000, 1999998, endpoint=True), error_data)\n",
    "plt.xlabel(\"Number of Values in Sum\")\n",
    "plt.ylabel(\"Percent Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Violet\">Answer the following questions</span>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model vs Simulation: In your own words, describe the difference between a model and a simulation.  Give your own example of a model, and how you would simulate it.\n",
    "\n",
    "> **Answer:**\n",
    "\n",
    "- Markov Chain: In your own words, describe a Markov Chain and its properties. Give your own example of a stochastic system and how you would implement a Markov Chain for it.\n",
    "\n",
    "> **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "Â© Copyright 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
