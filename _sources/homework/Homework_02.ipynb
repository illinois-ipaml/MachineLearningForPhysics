{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 02: Visualization and Expectation-Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for Getting, Loading and Locating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_data(url: str):\n",
    "    local_path = './tmp_data'\n",
    "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
    "    rc = None\n",
    "    while rc is None:\n",
    "      line = p.stderr.readline().strip('\\n')\n",
    "      if len(line) > 0:\n",
    "        print(line)\n",
    "      rc = p.poll()\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/pong_data.hf5')\n",
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/pong_targets.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_data    = pd.read_hdf(locate_data('pong_data.hf5'))\n",
    "pong_targets = pd.read_hdf(locate_data('pong_targets.hf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often coerce data for machine learning into a 2D array of samples x features, but this sometimes obscures the natural structure of the data.  In these cases, it is helpful to use special-purpose visualizations that know about this natural structure.\n",
    "\n",
    "Samples of `pong_data` encode 2D trajectories of a ping-pong ball. Implement the function below to transform one sample into a 2D array of trajectory coordinates suitable for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c934e01fd4a96c19584462d05a95952d",
     "grade": false,
     "grade_id": "cell-1afe0bd891726c3f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_to_trajectory(sample):\n",
    "    \"\"\"Reshape a pong data sample of length N into an (x,y) array of shape (2, N/2).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array\n",
    "        Numpy array of length N.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Array of length (2, N/2) where the first index is 0, 1 for x, y coordinates.\n",
    "    \"\"\"\n",
    "    assert len(sample) % 2 == 0, 'len(sample) is not divisible by two.'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f037259e5735396677ba52e4ab302146",
     "grade": true,
     "grade_id": "cell-4aaab2d2c9d30114",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "sample = np.arange(10)\n",
    "xy = sample_to_trajectory(sample)\n",
    "assert xy.shape == (2, 5)\n",
    "assert np.array_equal(xy[0], [0, 1, 2, 3, 4])\n",
    "assert np.array_equal(xy[1], [5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function below to plot some samples as trajectories, color coded by their target \"grp\" value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(nrows=50):\n",
    "    for row in range(nrows):\n",
    "        x, y = sample_to_trajectory(pong_data.iloc[row].values)\n",
    "        grp = int(pong_targets['grp'][row])\n",
    "        color = 'krb'[grp]\n",
    "        plt.plot(x, y, ls='-', c=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be obvious from this plot why the `y0` column is all zeros.\n",
    "\n",
    "Implement the function below using `sns.distplot` to show the distribution of `x0` values for each value (0, 1, 2) of the target `grp`.  Your plot should resemble this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Homework02-compare_groups.png\" width=600 align=left></img><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6429097519fc63c0079e5d1943d8a1ac",
     "grade": true,
     "grade_id": "cell-d56bc8b63f3187a3",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_groups():\n",
    "    \"\"\"Plot 1D distributions of the x0 feature for each group (grp=0,1,2).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "compare_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some insight into the `th0` target, implement the function below to add a `slope` column to `pong_data` that calculates the initial trajectory slope, then make a scatter plot of `slope` versus `th0` using `sns.jointplot`.  Your plot should resemble this one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Homework02-compare_slope_th0.png\" width=600 align=left></img><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "db0173f2f974ed15c0f09b0f6a4581b9",
     "grade": true,
     "grade_id": "cell-29bd0c402fddbaad",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_slope_th0():\n",
    "    \"\"\"Display scatter plot of calculated initial slope vs target th0.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "compare_slope_th0()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal (aka Gaussian) distribution is one of the fundamental probability densities that we will encounter often.\n",
    "\n",
    "Implement the function below using `np.random.multivariate_normal` to generate random samples from an arbitrary multidimensional normal distribution, for a specified random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bf21c36638bfbb445fb128a25d3da151",
     "grade": false,
     "grade_id": "cell-645c2d95e468fa4f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_normal(mu, C, n, seed=123):\n",
    "    \"\"\"Generate random samples from a normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : array\n",
    "        1D array of mean values of length N.\n",
    "    C : array\n",
    "        2D array of covariances of shape (N, N). Must be a positive-definite matrix.\n",
    "    n : int\n",
    "        Number of random samples to generate.\n",
    "    seed : int\n",
    "        Random number seed to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        2D array of shape (n, N) where each row is a random N-dimensional sample.\n",
    "    \"\"\"\n",
    "    assert len(mu.shape) == 1, 'mu must be 1D.'\n",
    "    assert C.shape == (len(mu), len(mu)), 'C must be N x N.'\n",
    "    assert np.all(np.linalg.eigvals(C) > 0), 'C must be positive definite.'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2eac9e62e50813fb9095f4f3d3fbdf3e",
     "grade": true,
     "grade_id": "cell-37d0ac7edfe0a08b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "mu = np.array([-1., 0., +1.])\n",
    "C = np.identity(3)\n",
    "C[0, 1] = C[1, 0] = -0.9\n",
    "Xa = generate_normal(mu, C, n=500, seed=1)\n",
    "Xb = generate_normal(mu, C, n=500, seed=1)\n",
    "Xc = generate_normal(mu, C, n=500, seed=2)\n",
    "assert np.array_equal(Xa, Xb)\n",
    "assert not np.array_equal(Xb, Xc)\n",
    "X = generate_normal(mu, C, n=2000, seed=3)\n",
    "assert np.allclose(np.mean(X, axis=0), mu, rtol=0.001, atol=0.1)\n",
    "assert np.allclose(np.cov(X, rowvar=False), C, rtol=0.001, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a generated 3D dataset using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d():\n",
    "    mu = np.array([-1., 0., +1.])\n",
    "    C = np.identity(3)\n",
    "    C[0, 1] = C[1, 0] = -0.9\n",
    "    X = generate_normal(mu, C, n=2000, seed=3)\n",
    "    df = pd.DataFrame(X, columns=('x0', 'x1', 'x2'))\n",
    "    sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about [correlation and covariance](https://en.wikipedia.org/wiki/Covariance_and_correlation), then implement the function below to create a 2x2 covariance matrix given values of $\\sigma_x$, $\\sigma_y$ and the correlation coefficient $\\rho$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "667ffdaeffcd1aa12d19fc9c6de061fb",
     "grade": false,
     "grade_id": "cell-9cbf2451fd307362",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_2d_covariance(sigma_x, sigma_y, rho):\n",
    "    \"\"\"Create and return the 2x2 covariance matrix specified by the input args.\n",
    "    \"\"\"\n",
    "    assert (sigma_x > 0) and (sigma_y > 0), 'sigmas must be > 0.'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "41c12e7e71547460685fc9d5beec95a5",
     "grade": true,
     "grade_id": "cell-adc05e1d4a9028e7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.array_equal(create_2d_covariance(1., 1.,  0.0), [[1.,  0.], [ 0., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1.,  0.0), [[4.,  0.], [ 0., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1.,  0.5), [[4.,  1.], [ 1., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1., -0.5), [[4., -1.], [-1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell that uses your `create_2d_covariance` and `generate_normal` functions to compare the 2D normal distributions with $\\rho = 0$ (blue), $\\rho = +0.9$ (red) and $\\rho = -0.9$ (green):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rhos():\n",
    "    mu = np.zeros(2)\n",
    "    sigma_x, sigma_y = 2., 1.\n",
    "    for rho, cmap in zip((0., +0.9, -0.9), ('Blues', 'Reds', 'Greens')):\n",
    "        C = create_2d_covariance(sigma_x, sigma_y, rho)\n",
    "        X = generate_normal(mu, C, 10000)\n",
    "        sns.kdeplot(x=X[:, 0], y=X[:, 1], cmap=cmap)\n",
    "    plt.xlim(-4, +4)\n",
    "    plt.ylim(-2, +2)\n",
    "        \n",
    "compare_rhos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm) is used to implement many machine learning methods, including several we have already studied like K-means and soon to be studied (e.g. factor analysis and weighted PCA.)\n",
    "\n",
    "The basic idea of EM is to optimize a goal function that depends on two disjoint sets of parameters by alternately updating one set and then the other, using a scheme that is guaranteed to improve the goal function (although generally to a local rather than global optimum). The alternating updates are called the E-step and M-step.\n",
    "\n",
    "The K-means is one of the simplest uses of EM, so is a good way to get some hands-on experience.\n",
    "\n",
    "Implement the function below to perform a K-means E-step. Hint: you might find [np.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html) and [np.argmin](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.argmin.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, centers):\n",
    "    \"\"\"Perform a K-means E-step.\n",
    "    \n",
    "    Assign each sample to the cluster with the nearest center, using the\n",
    "    Euclidean norm to measure distance between a sample and a cluster center.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    centers : array with shape (n, D)\n",
    "        Centers of the the n clusters in D dimensions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    integer array with shape (N,)\n",
    "        Cluster index of each sample, in the range 0 to n-1.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    n = len(centers)\n",
    "    assert centers.shape[1] == D\n",
    "    indices = np.empty(N, int)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = gen.normal(size=(100, 2))\n",
    "centers = np.array([[0., 0.], [0., 10.]])\n",
    "X[50:] += centers[1]\n",
    "indices = E_step(X, centers)\n",
    "assert np.all(indices[:50] == 0)\n",
    "assert np.all(indices[50:] == 1)\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = gen.normal(size=(20, 2))\n",
    "centers = gen.uniform(size=(5, 2))\n",
    "indices = E_step(X, centers)\n",
    "assert np.array_equal(indices, [4, 1, 4, 4, 1, 0, 1, 0, 2, 1, 2, 4, 0, 1, 0, 1, 0, 1, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to perform a K-means M-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(X, indices, n):\n",
    "    \"\"\"Perform a K-means M-step.\n",
    "    \n",
    "    Calculate the center of each cluster as the geometric mean of its assigned samples.\n",
    "    \n",
    "    The centers of any clusters without any assigned samples should be set to the origin.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    indices : integer array with shape (N,)\n",
    "        Cluster index of each sample, in the range 0 to n-1.\n",
    "    n : int\n",
    "        Number of clusters.  Must be <= N.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array with shape (n, D)\n",
    "        Centers of the the n clusters in D dimensions.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert indices.shape == (N,)\n",
    "    assert n <= N\n",
    "    centers = np.zeros((n, D))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() \n",
    "        \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = np.ones((20, 2))\n",
    "indices = np.zeros(20, int)\n",
    "centers = M_step(X, indices, 1)\n",
    "assert np.all(centers == 1.)\n",
    "\n",
    "n = 5\n",
    "indices = gen.randint(n, size=len(X))\n",
    "centers = M_step(X, indices, n)\n",
    "assert np.all(centers == 1.)\n",
    "\n",
    "X = gen.uniform(size=X.shape)\n",
    "centers = M_step(X, indices, n)\n",
    "assert np.allclose(\n",
    "    np.round(centers, 3),\n",
    "    [[ 0.494,  0.381], [ 0.592,  0.645],\n",
    "     [ 0.571,  0.371], [ 0.234,  0.634],\n",
    "     [ 0.250,  0.386]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented the core of the K-means algorithm.  Try it out with this simple wrapper, which makes a scatter plot of the first two columns after each iteration. The sklearn wrapper combines the result of several random starting points and has other refinements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_fit(data, n_clusters, nsteps, seed=123):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    assert n_clusters <= N\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # Pick random samples as the initial centers.\n",
    "    shuffle = gen.permutation(N)\n",
    "    centers = X[shuffle[:n_clusters]]\n",
    "    # Perform an initial E step to assign samples to clusters.\n",
    "    indices = E_step(X, centers)\n",
    "    # Loop over iterations.\n",
    "    for i in range(nsteps):\n",
    "        centers = M_step(X, indices, n_clusters)\n",
    "        indices = E_step(X, centers)\n",
    "    # Plot the result.\n",
    "    cmap = np.array(sns.color_palette())\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=cmap[indices % len(cmap)])\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker='+', c='k', s=400, lw=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out on some randomly generated 2D data with 3 separate clusters (using the handy [make_blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X, _ = make_blobs(n_samples=500, n_features=2, centers=[[-3,-3],[0,3],[3,-3]], random_state=gen)\n",
    "data = pd.DataFrame(X, columns=('x0', 'x1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple test, you should find a good solution after two iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_fit(data, n_clusters=3, nsteps=0);\n",
    "KMeans_fit(data, n_clusters=3, nsteps=1);\n",
    "KMeans_fit(data, n_clusters=3, nsteps=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "Â© Copyright 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
