# <span style="color: blue;"><b>Optimization and Model Selection</b></span>

## *Overview*
You will learn about optimization methods and Bayesian model selection

## *Goals*
* Learn about Optimization and Stochastic Gradient Descent
* Learn about Bayesian Model Selection

## *Lecture Materials*
* [Slides](https://docs.google.com/presentation/d/1KshmOwKTWptL-3PASHrW6WT2PkH2ltU-XwoYOflhKQk/edit?usp=sharing)
* {doc}`lectures/Optimization`
* {doc}`lectures/ModelSelection`

## *Homework Assignment*
* {doc}`homework/Homework_08`

## *Supplemental Readings*
* [Convex Functions](https://en.wikipedia.org/wiki/Convex_function)
* [Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen's_inequality)
* [Finite Difference Equations](https://en.wikipedia.org/wiki/Finite_difference)
* [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)
* [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function)
* [Nelder-Mead method](https://en.wikipedia.org/wiki/Nelderâ€“Mead_method)
* [Conjugate Gradient Method](https://en.wikipedia.org/wiki/Conjugate_gradient_method)
* [Newton's CG method](https://en.wikipedia.org/wiki/Newton's_method_in_optimization)
* [Powell's method](https://en.wikipedia.org/wiki/Powell's_method)
* [BFGS method](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)
* [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)
* [Adam Optimizer](https://arxiv.org/abs/1412.6980)
* [Softmax Function](https://en.wikipedia.org/wiki/Softmax_function)
* [Jeffreys proposed scale to interpret a Bayes Factor](https://en.wikipedia.org/wiki/Bayes_factor#Interpretation)