{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Markov Chain Monte Carlo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov-chain Monte Carlo** (MCMC) is an algorithm to generate random samples from an un-normalized probability density.  In other words, you want sample from $P(\\vec{z})$ but can only evaluate $f(\\vec{z})$ where the two are related by\n",
    "\n",
    "$$ \\Large\n",
    "P(\\vec{z}) = \\frac{f(\\vec{z})}{\\int d\\vec{z}\\,f(\\vec{z})} \\; .\n",
    "$$\n",
    "\n",
    "Note that $0 \\le P(\\vec{z}) \\le 1$ requires that $f(\\vec{z}) \\ge 0$ everywhere and that the integral has a non-zero finite value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">Examples</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with some simple motivating examples before diving into the Bayesian applications and the theory of Markov chains.\n",
    "\n",
    "The function\n",
    "\n",
    "$$ \\Large\n",
    "f(z) = \\begin{cases}\n",
    "\\sqrt{1 - z^4} & |z| < 1 \\\\\n",
    "0 & |z| \\ge 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "is never negative and has a finite integral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotf(zlim=1.2):\n",
    "    z = np.linspace(-zlim, +zlim, 250)\n",
    "    plt.plot(z, np.sqrt(np.maximum(0, 1 - z ** 4)))\n",
    "    plt.xlim(-zlim, +zlim)\n",
    "\n",
    "plotf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the normalization integral cannot be evaluated analytically (it is related to the [complete elliptic integral of the first kind](https://en.wikipedia.org/wiki/Elliptic_integral#Complete_elliptic_integral_of_the_first_kind)), so this is a good candidate for MCMC sampling using the MLS `MCMC_sample` function (which wraps [emcee](http://dfm.io/emcee/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "import emcee\n",
    "\n",
    "def wrap(func, **kwargs):\n",
    "    \"\"\"Prepare an arbitrary function to use with emcee sampling.\n",
    "\n",
    "    Emcee expects its parameters in a single list, but it is sometimes more\n",
    "    convenient to write a function in terms of named parameters and\n",
    "    hyperparameters. This method uses introspection to wrap an arbitrary\n",
    "    function with named parameters so that it has the signature expected\n",
    "    by emcee.\n",
    "\n",
    "    For example:\n",
    "\n",
    "        def f(x,y,a,b): ...\n",
    "        wrap(f, x=[1], y=[2], a=3, b=4, c=3, d=4)\n",
    "\n",
    "    returns a tuple (wrapped, ['x','y'], [1,2], {'c':3, 'd':4}) where:\n",
    "\n",
    "      - wrapped([p,q]) calls f(x=p,y=q,a=3,b=4)\n",
    "      - [1,2] are the initial values to use for parameters named ['x','y'].\n",
    "      - {'c':3, 'd':4} are the input kwargs with args of f() removed.\n",
    "\n",
    "    The square brackets identify floating arguments and specify their initial\n",
    "    value. An optional callable to evaluate a log-prior can also be passed,\n",
    "    for example:\n",
    "\n",
    "        wrap(f, x=[1,px], y=[2,py], a=3, b=4, c=3, d=4)\n",
    "\n",
    "    where px(x) and py(y) return the (un-normalized) log of the priors on\n",
    "    x and y to use during posterior sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        The function that should be prepared. It is assumed to have only\n",
    "        numerical arguments that accept any floating point values.\n",
    "    **kwargs : keyword arguments\n",
    "        All arguments of func must be included and assigned a value.\n",
    "        Arguments assigned a floating point value are considered fixed\n",
    "        during sampling.  Arguments assigned a floating point value\n",
    "        within a list, e.g., [1.2], will be sampled using the initial\n",
    "        value provided.  Sampled arguments can optionally also specify\n",
    "        a log-prior distribution using, e.g. [1.2, lnprior], where lnprior\n",
    "        is a function of the sampled argument that returns the log prior\n",
    "        probability density (which does not need to be normalized).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple (wrapped, names, values, kwargs). See example above for details.\n",
    "    \"\"\"\n",
    "    fixed = {}\n",
    "    names, values, lnpriors = [], [], []\n",
    "    funcsig = inspect.signature(func)\n",
    "    try:\n",
    "        funcargs = {name: kwargs[name] for name in funcsig.parameters}\n",
    "    except KeyError:\n",
    "        raise ValueError('Missing arguments.')\n",
    "    bound = funcsig.bind(**funcargs)\n",
    "    bound.apply_defaults()\n",
    "    NoPrior = lambda x: 0.\n",
    "    for name, value in bound.arguments.items():\n",
    "        if isinstance(value, list):\n",
    "            names.append(name)\n",
    "            values.append(value.pop(0))\n",
    "            lnpriors.append(value.pop(0) if value else NoPrior)\n",
    "            if value:\n",
    "                raise ValueError('Invalid syntax for argument {}.'.format(name))\n",
    "        else:\n",
    "            fixed[name] = value\n",
    "    partial = functools.partial(func, **fixed)\n",
    "    def wrapped(theta):\n",
    "        if len(theta) != len(names):\n",
    "            raise ValueError('expected list of {} values.'.format(len(names)))\n",
    "        result = 0.\n",
    "        for lnprior, value in zip(lnpriors, theta):\n",
    "            result += lnprior(value)\n",
    "            if not np.isfinite(result):\n",
    "                # theta is not allowed by this prior.\n",
    "                return -np.inf\n",
    "        args = dict(zip(names, theta))\n",
    "        result += partial(**args)\n",
    "        return result\n",
    "    # Remove function args from kwargs.\n",
    "    for name in funcargs:\n",
    "        kwargs.pop(name, None)\n",
    "    return wrapped, names, values, kwargs\n",
    "\n",
    "\n",
    "def sample(func, names, values, nwalkers=20, nsamples=1000, abs_rms=1e-4,\n",
    "           frac_rms=1e-3, burnin=100, random_state=None):\n",
    "    \"\"\"Generate MCMC samples of the un-normalized PDF func() using emcee.\n",
    "\n",
    "    Can be used standalone but intended to work with :func:`wrap`.\n",
    "\n",
    "    Initial values for each walker are Gaussian samples centered on the\n",
    "    input values with an RMS of max(abs_rms, frac_rms * values).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        Evaluate the log PDF to sample. Passed a single list of parameter\n",
    "        values. Can be prepared using :func:`wrap`.\n",
    "    names : iterable\n",
    "        List of names for each floating parameter.  Used to label columns\n",
    "        in the returned DataFrame. Can be prepared using :func:`wrap`.\n",
    "    values : iterable\n",
    "        List of initial values for each floating parameter.  Used to center\n",
    "        random initial values for each walker. Can be prepared using\n",
    "        :func:`wrap`.\n",
    "    nwalkers : int\n",
    "        The number of emcee walkers to use.\n",
    "    nsamples : int\n",
    "        The total number of samples to return, after combining walkers\n",
    "        and trimming initial burnin.\n",
    "    abs_rms : float\n",
    "        Used to set walker initial values.  See above for details.\n",
    "    rel_rms : float\n",
    "        Used to set walker initial values.  See above for details.\n",
    "    burnin : int\n",
    "        The number of samples to remove from each walker's chain.\n",
    "    random_state : np.random.RandomState or None\n",
    "        The random state to use for reproducible chains.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Generated samples in a dataframe, using the inputs names for columns.\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState()\n",
    "    # Generate sampler starting points.\n",
    "    ndim = len(names)\n",
    "    values = np.array(values, float)\n",
    "    initial = np.tile(values, (nwalkers, 1))\n",
    "    rms = np.maximum(abs_rms, frac_rms * values)\n",
    "    initial += rms * random_state.normal(size=(nwalkers, ndim))\n",
    "    # Initialize and run sampler.\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, func)\n",
    "    n_per_chain = 1 + nsamples // nwalkers + burnin\n",
    "    sampler.run_mcmc(initial, n_per_chain, rstate0=random_state.get_state())\n",
    "    # Remove burnin and return results in a DataFrame.\n",
    "    chain = sampler.chain[:, burnin:].reshape(-1, ndim)[:nsamples]\n",
    "    return pd.DataFrame(chain, columns=names)\n",
    "\n",
    "\n",
    "def MCMC_sample(func, **kwargs):\n",
    "    \"\"\"Generate random samples from an un-normalized PDF.\n",
    "\n",
    "    See :func:`wrap` and :func:`sample` for details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func : callable\n",
    "        Function to evaluate log(f(...)) where f(...) is proportional\n",
    "        to the desired probability density.  Will be wrapped to\n",
    "        determine which arguments are sampled and which are fixed.\n",
    "    **kwargs : keyword arguments\n",
    "        Used to configure the wrapping of func and the sampler.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Generated samples in a dataframe, with one named column per\n",
    "        sampled argument of the input function.\n",
    "    \"\"\"\n",
    "    # Wrap the input function.\n",
    "    wrapped, names, values, kwargs = wrap(func, **kwargs)\n",
    "    # Generate emcee samples.\n",
    "    return sample(wrapped, names, values, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logf(z):\n",
    "    return 0.5 * np.log(1 - z ** 4) if np.abs(z) < 1 else -np.inf\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "samples = MCMC_sample(logf, z=[0], nsamples=20000, random_state=gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation `z=[0]` identifies `z` as the parameter we want to sample (starting the value 0). The result is a Pandas DataFrame of generated samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated samples are (approximately) drawn from the normalized $P(z)$ corresponding to the $f(z)$ provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples['z'], range=(-1,1), bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are MCMC samples good for? They allow us to estimate the expectation value of an arbitrary $g(z)$ using [importance sampling](https://en.wikipedia.org/wiki/Importance_sampling):\n",
    "\n",
    "$$ \\Large\n",
    "\\langle g(\\vec{z})\\rangle_P \\equiv \\int d\\vec{z}\\, g(\\vec{z})\\, P(\\vec{z})\n",
    "\\simeq \\frac{1}{N} \\sum_{i=1}^N g(\\vec{z}_i) \\; ,\n",
    "$$\n",
    "\n",
    "where $\\vec{z}_1, \\vec{z}_2, \\ldots$ are the MCMC samples.\n",
    "\n",
    "For example, to estimate the expectation value of $g(z) = z^2$ (aka the variance) with the samples generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(samples['z'] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation values of more complex functions are equally easy, for example, $g(z) = \\sin(\\pi z)^2$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sin(np.pi * samples['z']) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also build an empirical estimate of the normalized probability density $P(\\vec{z})$ using any density estimation method, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.01).fit(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotfit(zlim=1.2, Pmin=0.1):\n",
    "    z = np.linspace(-zlim, +zlim, 250)\n",
    "    f = np.sqrt(np.maximum(0, 1 - z ** 4))\n",
    "    P = np.exp(fit.score_samples(z.reshape(-1, 1)))\n",
    "    plt.plot(z, f, label='$f(z)$')\n",
    "    plt.fill_between(z, P, alpha=0.5, label='$P(z)$')\n",
    "    ratio = f / P\n",
    "    sel = P > Pmin\n",
    "    plt.plot(z[sel], ratio[sel], '.', label='$P(z)/f(z)$')\n",
    "    mean_ratio = np.mean(ratio[sel])\n",
    "    print('mean P(z)/f(z) = {:.3f}'.format(mean_ratio))\n",
    "    plt.axhline(mean_ratio, ls='--', c='k')\n",
    "    plt.xlim(-zlim, +zlim)\n",
    "    plt.legend(loc='upper left', ncol=3)\n",
    "\n",
    "plotfit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated $P(z)$ does not look great, but since it is just a scaled version of the smooth un-normalized $f(z)$, their ratio at each $z$ is an estimate of the integral:\n",
    "\n",
    "$$ \\Large\n",
    "\\frac{f(z)}{P(z)} = \\int dz\\,f(z) \\; .\n",
    "$$\n",
    "\n",
    "Therefore the mean of $f(z) / P(z)$ provides an estimate of the normalization integral.  In practice, we restrict this mean to $z$ values where $P(z)$ is above some minimum to avoid regions where the empirical density estimate is poorly determined.\n",
    "\n",
    "In the example above, the true value of the integral rounds to 1.748 so our numerical accuracy is roughly 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try a multidimensional example:\n",
    "\n",
    "$$ \\Large\n",
    "f(\\vec{z}, \\vec{z}_0, r) =\n",
    "\\begin{cases}\n",
    "\\exp\\left(-|\\vec{z} - \\vec{z}_0|^2/2\\right) & |\\vec{z}| < r \\\\\n",
    "0 & |\\vec{z}| \\ge r\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This function describes an un-normalized Gaussian PDF centered at $\\vec{z}_0$ and clipped outside $|\\vec{z}| < r$. The normalization integral has no analytic solution except in the limits $\\vec{z}_0\\rightarrow \\vec{z}$ or $r\\rightarrow\\infty$.\n",
    "\n",
    "To generate MCMC samples in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logf(x, y, x0, y0, r):\n",
    "    z = np.array([x, y])\n",
    "    z0 = np.array([x0, y0])\n",
    "    return -0.5 * np.sum((z - z0) ** 2) if np.sum(z ** 2) < r ** 2 else -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables to sample are assigned initial values in square brackets and all other arguments are treated as fixed hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = MCMC_sample(logf, x=[0], y=[0], x0=1, y0=-2, r=3, nsamples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated samples now have two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot shows a 2D Gaussian distribution clipped to a circle and offset from its center, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples['x'], samples['y'], s=10)\n",
    "plt.scatter(1, -2, marker='+', s=500, lw=5, c='white')\n",
    "plt.gca().add_artist(plt.Circle((0, 0), 3, lw=4, ec='red', fc='none'))\n",
    "plt.gca().set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multidimensional samples, we can estimate expectation values of marginal PDFs just as easily as the full joint PDF.  In our 2D example, the marginal PDFs are:\n",
    "\n",
    "$$ \\Large\n",
    "P_X(x) = \\int dy\\, P(x, y) \\quad , \\quad P_Y(y) = \\int dx\\, P(x, y) \\; .\n",
    "$$\n",
    "\n",
    "For example, the expectation value of $g(x)$ with respect to $P_X$ is:\n",
    "\n",
    "$$ \\Large\n",
    "\\langle g\\rangle \\equiv \\int dx\\, g(x) P_X(x) = \\int dx\\, g(x) \\int dy\\, P(x, y) = \\int dx dy\\, g(x)\\, P(x,y) \\; .\n",
    "$$\n",
    "\n",
    "In other words, the expectation value with respect to a marginal PDF is equal to the expectation with respect to the full joint PDF.\n",
    "\n",
    "For example, the expectation value of $g(x) = x$ (aka the mean) with respect to $P_X(x)$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(samples['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the density of a marginal PDF by simply dropping the columns that are integrated out before plugging into a density estimator.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitX = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(samples.drop(columns='y').values)\n",
    "fitY = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(samples.drop(columns='x').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotfitXY(r=3):\n",
    "    xy = np.linspace(-r, +r, 250)\n",
    "    Px = np.exp(fitX.score_samples(xy.reshape(-1, 1)))\n",
    "    Py = np.exp(fitY.score_samples(xy.reshape(-1, 1)))\n",
    "    plt.plot(xy, Px, label='$P_X(x)$')\n",
    "    plt.plot(xy, Py, label='$P_Y(y)$')\n",
    "    plt.legend()\n",
    "    \n",
    "plotfitXY()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">Bayesian Inference with MCMC</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced MCMC above as a general purpose algorithm for sampling any un-normalized PDF, without any reference to Bayesian (or frequentist) statistics. We also never specified whether $\\vec{z}$ was something observed (data) or latent (parameters and hyperparameters), because it doesn't matter to MCMC.\n",
    "\n",
    "However, MCMC is an excellent tool for performing numerical inferences using the generalized Bayes' rule we met earlier:\n",
    "\n",
    "$$ \\Large\n",
    "P(\\Theta_M\\mid D, M) = \\frac{P(D\\mid \\Theta_M, M)\\,P(\\Theta_M\\mid M)}{P(D\\mid M)}\n",
    "$$\n",
    "\n",
    "In particular, the normalizing denominator (aka the \"evidence\"):\n",
    "\n",
    "$$ \\Large\n",
    "P(D\\mid M) = \\int d\\Theta_M' P(D\\mid \\Theta_M', M)\\, P(\\Theta_M'\\mid M)\n",
    "$$\n",
    "\n",
    "is often not practical to calculate, so we can only calculate the un-normalized numerator\n",
    "\n",
    "$$ \\Large\n",
    "P(D\\mid \\Theta_M, M)\\,P(\\Theta_M\\mid M) \\; ,\n",
    "$$\n",
    "\n",
    "which combines the *likelihood of the data* and the *prior probability of the model*.\n",
    "\n",
    "If we treat the observed data $D$ and hyperparameters $M$ as fixed, then the appropriate function to plug into an MCMC is:\n",
    "\n",
    "$$ \\Large\n",
    "\\log f(\\Theta) = \\log P(D\\mid \\Theta_M, M) + \\log P(\\Theta_M\\mid M) \\; .\n",
    "$$\n",
    "\n",
    "The machinery described above then enables us to generate samples $\\Theta_1, \\Theta_2, \\ldots$ drawn from the *posterior* distribution, and therefore make interesting statements about probabilities involving model parameters.\n",
    "\n",
    "The likelihood function depends on the data and model, so could be anything, but we often assume Gaussian errors in the data, which leads to the multivariate Gaussian PDF we met earlier ($d$ is the number of data features):\n",
    "\n",
    "$$ \\Large\n",
    "P(\\vec{x}\\mid \\Theta_M, M) =\n",
    "\\left(2\\pi\\right)^{-d/2}\\,\\left| C\\right|^{-1/2}\\,\n",
    "\\exp\\left[  -\\frac{1}{2} \\left(\\vec{x} - \\vec{\\mu}\\right)^T C^{-1} \\left(\\vec{x} - \\vec{\\mu}\\right) \\right]\n",
    "$$\n",
    "\n",
    "In the most general case, $\\vec{\\mu}$ and $C$ are functions of everything: the data $D$, the parameters $\\Theta_M$ and the hyperparameters $M$.\n",
    "\n",
    "When we have $N$ independent observations, $\\vec{x}_1, \\vec{x}_2, \\ldots$, their combined likelihood is the product of each sample's likelihood:\n",
    "\n",
    "$$ \\Large\n",
    "P(\\vec{x}_1, \\vec{x}_2, \\ldots\\mid \\Theta_M, M) = \\prod_{i=1}^N\\, P(\\vec{x}_i\\mid \\Theta_M, M)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider fitting a straight line $y = m x + b$, with parameters $m$ and $b$, to data with two features $x$ and $y$. The relevant log-likelihood function is:\n",
    "\n",
    "$$ \\Large\n",
    "\\log{\\cal L}(m, b; D) = -\\frac{N}{2}\\log(2\\pi\\sigma_y^2)\n",
    "-\\frac{1}{2\\sigma_y^2} \\sum_{i=1}^N\\, (y_i - m x_i - b)^2 \\; ,\n",
    "$$\n",
    "\n",
    "where the error in $y$, $\\sigma_y$, is a fixed hyperparameter. Note that the first term is the Gaussian PDF normalization factor.\n",
    "\n",
    "First generate some data on a straight line with measurement errors in $y$ (so our assumed model is correct):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.RandomState(seed=123)\n",
    "N, m_true, b_true, sigy_true = 10, 0.5, -0.2, 0.1\n",
    "x_data = gen.uniform(-1, +1, size=N)\n",
    "y_data = m_true * x_data + b_true + gen.normal(scale=sigy_true, size=N)\n",
    "\n",
    "plt.errorbar(x_data, y_data, sigy_true, fmt='o', markersize=5)\n",
    "plt.plot([-1, +1], [-m_true+b_true,+m_true+b_true], 'r:')\n",
    "plt.xlabel('x'); plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the log-likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike(x, y, m, b, sigy):\n",
    "    N = len(x)\n",
    "    norm = 0.5 * N * np.log(2 * np.pi * sigy ** 2)\n",
    "    return -0.5 * np.sum((y - m * x - b) ** 2) / sigy ** 2 - norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, generate some MCMC samples of the posterior \n",
    "\n",
    "$$ \\Large\n",
    "P(m, b\\mid D, M)\n",
    "$$\n",
    "\n",
    "assuming uniform priors \n",
    "\n",
    "$$ \\Large\n",
    "P(b,m\\mid \\sigma_y) = 1\n",
    "$$\n",
    "\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = MCMC_sample(loglike, m=[m_true], b=[b_true],\n",
    "                      x=x_data, y=y_data, sigy=sigy_true, nsamples=10000, random_state=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=samples, x='m', y='b', xlim=(0.2,0.8), ylim=(-0.3,0.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "___<span style=\"color:violet\">EXERCISE</span>___ We always require a starting point to generate MCMC samples. In this example, we used the true parameter values as starting points:\n",
    "```\n",
    "m=[m_true], b=[b_true]\n",
    "```\n",
    "What happens if you chose different starting points?  Try changing the starting values by $\\pm 0.1$ and see how this affects the resulting means and standard deviations for $m$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "samples = MCMC_sample(loglike, m=[m_true+0.1], b=[b_true+0.1],\n",
    "                      x=x_data, y=y_data, sigy=sigy_true, nsamples=10000, random_state=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "samples.describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "samples = MCMC_sample(loglike, m=[m_true-0.1], b=[b_true-0.1],\n",
    "                      x=x_data, y=y_data, sigy=sigy_true, nsamples=10000, random_state=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "samples.describe(percentiles=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The changes are small compared with the offsets ($\\pm 0.1$) and the standard deviations in each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MCMC_sample` function can apply independent (i.e., factorized) priors on each parameter:\n",
    "\n",
    "$$ \\Large\n",
    "P(\\Theta\\mid M) = \\prod_j P(\\theta_j\\mid M)\n",
    "$$\n",
    "\n",
    "Define the two most commonly used independent priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopHat(lo, hi):\n",
    "    \"\"\"Return un-normalized log(prior) for x in [lo,hi]\"\"\"\n",
    "    return lambda x: 0 if (lo <= x <= hi) else -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gauss(mu, sigma):\n",
    "    \"\"\"Return un-normalized log(prior) for x ~ N(mu,sigma)\"\"\"\n",
    "    return lambda x: -0.5 * ((x - mu) / sigma) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply a prior, we replace `z=[value]` with `z=[value,logprior]`.  For example, suppose we believe that $0.4 \\le m \\le 0.7$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = MCMC_sample(loglike, m=[m_true,TopHat(0.4,0.7)], b=[b_true],\n",
    "                      x=x_data, y=y_data, sigy=sigy_true, nsamples=10000, random_state=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=samples, x='m', y='b', xlim=(0.2,0.8), ylim=(-0.3,0.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add a prior on $b$.  For example, suppose a previous measurement found $b = -0.20 \\pm 0.02$ (in which case, the new data is not adding much information about $b$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = MCMC_sample(loglike, m=[m_true,TopHat(0.4,0.7)], b=[b_true,Gauss(-0.20,0.02)],\n",
    "                      x=x_data, y=y_data, sigy=sigy_true, nsamples=10000, random_state=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=samples, x='m', y='b', xlim=(0.2,0.8), ylim=(-0.3,0.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "___<span style=\"color:violet\">EXERCISE</span>___ Suppose we know that all $y_i$ values have the same error $\\sigma_y$ but we do not know its value.\n",
    " - Generate samples of $(m, b, \\sigma_y)$ using `m=[m_true], b=[b_true], sigy=[sigy_true]`.\n",
    " - Look at the samples with an `sns.pairplot`.\n",
    " - Which panel shows the marginalized posterior $P(\\sigma_y\\mid D)$? Do you understand its peculiar shape?\n",
    " - Add a prior on $\\sigma_y$ to fix this peculiar shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "samples = MCMC_sample(loglike, m=[m_true], b=[b_true], sigy=[sigy_true],\n",
    "                      x=x_data, y=y_data, nsamples=10000, random_state=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "samples = MCMC_sample(loglike, m=[m_true], b=[b_true], sigy=[sigy_true, TopHat(0.01,1)],\n",
    "                      x=x_data, y=y_data, nsamples=10000, random_state=gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in-depth case study of the many subtleties in fitting a straight line, read this 55-page [article by Hogg, Bovy and Lang](https://arxiv.org/abs/1008.4686)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "© Copyright 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
