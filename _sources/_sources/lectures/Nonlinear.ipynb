{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapting Linear Methods to Non-Linear Data and Kernel Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess\n",
    "from sklearn import cluster, decomposition, metrics, manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for Getting, Loading and Locating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_data(url: str):\n",
    "    local_path = './tmp_data'\n",
    "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
    "    rc = None\n",
    "    while rc is None:\n",
    "      line = p.stderr.readline().strip('\\n')\n",
    "      if len(line) > 0:\n",
    "        print(line)\n",
    "      rc = p.poll()\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Get Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/circles_data.hf5')\n",
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/circles_targets.hf5')\n",
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/ess_data.hf5')\n",
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/ess_targets.hf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Load Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_data    = pd.read_hdf(locate_data('circles_data.hf5'))\n",
    "circles_targets = pd.read_hdf(locate_data('circles_targets.hf5'))\n",
    "ess_data        = pd.read_hdf(locate_data('ess_data.hf5'))\n",
    "ess_targets     = pd.read_hdf(locate_data('ess_targets.hf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:violet\">EXAMPLE</span>: <span style=\"color:Orange\">The Cure of Dimensionality</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already encountered the \"curse of dimensionality\" in the context of dimensionality reduction, but sometimes a large dimensionality can actually be a cure. As a motivating example, consider the 2D data plotted below which clearly contains two clusters with highly nonlinear shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circles(labels, data=circles_data):\n",
    "    X = data.values\n",
    "    cmap = sns.color_palette('colorblind', 2)\n",
    "    c = [cmap[label] for label in labels]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=c)\n",
    "    plt.gca().set_aspect(1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_circles(labels=circles_targets['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "The plot above is colored using the true labels stored as column `y` of `circles_targets`.\n",
    "\n",
    "___<span style=\"color:violet\">EXERCISE</span>___: Use KMeans to fit this data as two clusters and plot the results using `plot_circles(labels=fit.labels_)`.  Are the results surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "fit = cluster.KMeans(n_clusters=2, n_init=10).fit(circles_data)\n",
    "plot_circles(labels = fit.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "The clusters found by KMeans are not what we want, but also not surprising given that KMeans partitions samples with a simple dividing line (or hyperplane in higher dimensions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "___<span style=\"color:violet\">EXERCISE</span>___: Create a new dataset called `circles_3d` that is a copy of `circles_data` but with a new feature added:\n",
    "\n",
    "$$ \\Large\n",
    "x_2 = x_0^2 + x_1^2 \\; .\n",
    "$$\n",
    "\n",
    "Think about how this new feature changes the clustering problem, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "circles_3d = circles_data.copy()\n",
    "circles_3d['x2'] = circles_3d['x0'] ** 2 + circles_3d['x1'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "By *increasing* the dimensionality of our data, we have turned a very nonlinear clustering problem into a trivial linear problem!  To see this, plot the data in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_circles_3d(labels, data=circles_3d):\n",
    "    X = data.values\n",
    "    cmap = sns.color_palette('colorblind', 2)\n",
    "    c = [cmap[label] for label in labels]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=c)\n",
    "    \n",
    "plot_circles_3d(labels=circles_targets['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "___<span style=\"color:violet\">EXERCISE</span>___: Finally, fit for two KMeans clusters in your new `circles_3d` data and plot the results, as above. Are the results surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "fit = cluster.KMeans(n_clusters=2, n_init=10).fit(circles_3d)\n",
    "plot_circles(labels = fit.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "This is exactly the result we wanted, but not so surprising after seeing the 3D plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Kernel Functions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many classes of problems where nonlinearities in your data can be handled with linear methods by first embedding in a higher dimension.\n",
    "\n",
    "The embedding we used in the example above was hand picked for that data, but a generic embedding will often work if it adds enough dimensions.  For example, the function below is commonly used to embed 2D features $(x_0, x_1)$ into a 7D space:\n",
    "\n",
    "$$ \\Large\n",
    "\\phi(x_0, x_1) = \\begin{pmatrix}\n",
    "x_0^2 \\\\\n",
    "x_0 x_1 \\\\\n",
    "x_1 x_0 \\\\\n",
    "x_1^2 \\\\\n",
    "\\sqrt{2 c} x_0 \\\\\n",
    "\\sqrt{2 c} x_1 \\\\\n",
    "c\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(data, c=1):\n",
    "    # Embed X using the poly kernel with n=2, c=1 (see p.16 of CS229 SVM notes)\n",
    "    x0, x1 = data['x0'], data['x1']\n",
    "    X = np.vstack([\n",
    "        x0 * x0, x0 * x1, x1 * x0, x1 * x1,\n",
    "        np.sqrt(2 * c) * x0, np.sqrt(2 * c) * x1,\n",
    "        c + 0 * x0\n",
    "    ]).T\n",
    "    return pd.DataFrame(X, columns=('x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6'))\n",
    "\n",
    "circles_embedded = embed(circles_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pairplot of the 7D embedded `circles_data` shows that this is peculiar embedding, but it does allow a linear separation of the two clusters (via its $x_0^2$ and $x_1^2$ components). It also seems inefficient, with one feature repeated ($x_0 x_1$) and another one constant ($c$).  However, this is just the simplest member of a family of embeddings where $c$ plays an important role in fixing the relative normalization of the different groups of [monomials](https://en.wikipedia.org/wiki/Monomial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(circles_embedded);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for chosing this peculiar embedding is that it has the following very useful property:\n",
    "\n",
    "$$ \\Large\n",
    "\\phi(X_i) \\cdot \\phi(X_j) = \\left(X_i\\cdot X_j + c\\right)^2 \\; ,\n",
    "$$\n",
    "\n",
    "where $X_i$ and $X_j$ are arbitrary samples (rows) of our data.\n",
    "\n",
    "First, lets check this explicity for `circles_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHS = np.einsum('ik,jk->ij', circles_embedded, circles_embedded)\n",
    "RHS = (np.einsum('ik,jk->ij', circles_data, circles_data) + 1) ** 2\n",
    "assert np.allclose(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this property is so useful is that the RHS can be evaluated much faster than the LHS and never requires us to actually embed our original samples in the higher-dimensional space.\n",
    "\n",
    "Functions on the sample space that evalute a dot product in a different space are called <span style=\"color:violet\">kernel functions</span>:\n",
    "\n",
    "$$ \\Large\n",
    "K(X_i, X_j) = \\phi(X_i) \\cdot \\phi(X_j) \\; .\n",
    "$$\n",
    "\n",
    "A kernel function is a [similarity measure](https://en.wikipedia.org/wiki/Similarity_measure) since it measures the similarity of samples $i$ and $j$, with a maximum value for identical samples and zero for orthogonal samples.  Similarity measures are related to distance measures (e.g. metrics in relativity) but with the opposite behaviour:\n",
    "- very similar samples: distance ~ 0, large similarity.\n",
    "- very different samples: large distance, similarity ~ 0.\n",
    "\n",
    "The importance of kernel functions is deeper than just their computational efficiency: many algorithms can be expressed using only dot products between samples, and therefore can be applied to data embedded in a higher dimensional without ever doing the embedding.  This insight is known as the <span style=\"color:violet\">kernel trick</span>:\n",
    "- Pick a kernel function $K$.\n",
    "- Pick an algorithm that can be expressed using only dot products.\n",
    "\n",
    "When these pre-requisites are met, the algorithm can be easily and efficiently applied to data that is effectively boosted to a high dimensional space.  As we saw in the example above, the main benefit is that nonlinear data can now be analyzed using linear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there are a limited number of suitable kernel functions $K$ (start with [Mercer's theorem](https://en.wikipedia.org/wiki/Mercer%27s_theorem) if you are interested to learn more about why this is). We have already met the [polynomial kernel](https://en.wikipedia.org/wiki/Polynomial_kernel), which can be written most generally as:\n",
    "\n",
    "$$ \\Large\n",
    "K(X_i, X_j) = \\left( \\gamma X_i\\cdot X_j + c\\right)^d \\; ,\n",
    "$$\n",
    "\n",
    "where $\\gamma$, $c$ and $d$ are all hyperparameters (our earlier example used $\\gamma = 1$, $c = 1$ and $d = 2$). The `metrics.pairwise` module of sklearn can calculate the matrix of all possible sample dot products for this and other kernels, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK = metrics.pairwise.polynomial_kernel(circles_data, degree=2, gamma=1., coef0=1.)\n",
    "assert np.allclose(LHS, PK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other popular kernels are the [sigmoid kernel](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.sigmoid_kernel.html):\n",
    "\n",
    "$$ \\Large\n",
    "K(X_i, X_j) = \\tanh\\left( \\gamma X_i\\cdot X_j + c\\right) \\; ,\n",
    "$$\n",
    "\n",
    "and the [radial basis function (rbf) kernel](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.rbf_kernel.html) (whose embedding is infinite dimensional due to the infinite series expansion of $e^{-x}$):\n",
    "\n",
    "$$ \\Large\n",
    "K(X_i, X_j) = \\exp\\left(-\\gamma \\left| X_i - X_j\\right|^2\\right) \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:violet\">EXAMPLE</span>: <span style=\"color:Orange\">Kernel PCA</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of the **kernel trick**, the PCA algorithm can be adapted to use only dot products to project each sample onto the higher-dimensional eigenvectors. The resulting [KernelPCA algorithm](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html) is used just like the linear decomposition methods, but with some additional hyperparameters, e.g.\n",
    "```\n",
    "fit = decomposition.KernelPCA(n_components=d, kernel='poly', gamma=1., degree=2).fit(X)\n",
    "Y = fit.transform(X)\n",
    "```\n",
    "One limitation of the kernel trick for PCA is that the original samples cannot be reconstructed using only dot products, so reconstruction of the latent variables becomes a challenging new problem that requires a [separate supervised machine learning approach](http://papers.nips.cc/paper/2417-learning-to-find-pre-images.pdf). Fortunately, the sklearn implementation takes care of all of this for you:\n",
    "```\n",
    "fit = decomposition.KernelPCA(n_components=d, kernel='poly', gamma=1., degree=2, inverse_transform=True).fit(X)\n",
    "Y = fit.transform(X)\n",
    "reconstructed = fit.inverse_transform(Y)\n",
    "```\n",
    "\n",
    "We will use the following function to demonstrate the KernelPCA method, which allows you to set the $\\gamma$ hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpca_demo(gamma=1.0, data=circles_data, labels=circles_targets['y']):\n",
    "    X = data.values\n",
    "    \n",
    "    # Use PCA with RBF kernel.\n",
    "    kpca = decomposition.KernelPCA(kernel='rbf', gamma=gamma, n_components=2)\n",
    "    reduced = kpca.fit_transform(X)\n",
    "    \n",
    "    # Plot 2D latent space.Results are very sensitive to choice of gamma!\n",
    "    cmap = sns.color_palette('colorblind', 2)\n",
    "    c = [cmap[label] for label in labels]\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], c=c)\n",
    "    plt.text(0.02, 0.92, '$\\gamma={:.1f}$'.format(gamma),\n",
    "             transform=plt.gca().transAxes, fontsize='x-large')\n",
    "    plt.xlim(-0.8, 0.8)\n",
    "    plt.ylim(-0.8, 0.8)\n",
    "    plt.gca().set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is spectacular: our nonlinear data is completely linearized when transformed to a 2D latent variable space. Note that, in this example, we are not performing any overall dimensionality reduction: we started with 2 features, implicitly expanded to an infinite number of features using the RBF kernel, then shrank back down to two latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "___<span style=\"color:violet\">EXERCISE</span>___: The results above are quite sensitive to the choice of hyperparameters. To explore this, rerun `kpca_demo` with different values of $\\gamma$.  What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "for gamma in (0.3, 0.4, 3., 15.):\n",
    "    kpca_demo(gamma)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The results are finely tuned and small variations in $\\gamma$ can destroy the linear separation. Sensitivity to $\\gamma$ is not too surprising since it is a parameter of the kernel function.  However, KernelPCA results can also change dramatically with a small change to the input data.  See [this github issue](https://github.com/scikit-learn/scikit-learn/issues/10530) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:violet\">EXAMPLE</span>: <span style=\"color:Orange\">Locally Linear Embedding</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **kernel trick** is not the only way to harness linear methods to nonlinear problems. For our next example, we consider <span style=\"color:violet\">locally linear embedding</span> (LLE), which is a type of [manifold learning](https://scikit-learn.org/stable/modules/manifold.html), i.e., a dimensionality reduction method for data on a non-linear manifold.\n",
    "\n",
    "First lets look at some 3D data that is clearly 2D, but requires a nonlinear decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(ess_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairplot is confusing until you see the following plot where, in addition, each point is colored according to its true 1D coordinate along the main direction of the manifold (stored in the 'y' column of `ess_targets`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_ess_3d():\n",
    "    X = ess_data.values\n",
    "    y = ess_targets['y']\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='plasma')\n",
    "    ax.view_init(10, -70)\n",
    "    \n",
    "plot_ess_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLE takes advantage of the fact that the manifold is \"flat\" in the neighborhood of each sample, so can be described locally with a linear approximation.  We construct such a local linear approximation to a sample $\\vec{X}_i$ as:\n",
    "\n",
    "$$ \\Large\n",
    "\\vec{X}_i \\simeq \\sum_{j\\ne i} W_{ij} \\vec{X}_j \\; ,\n",
    "$$\n",
    "\n",
    "where the sum is restricted to the $n$ \"nearest\" samples to $X_i$. In other words, we find the set of weights $W_{ij}$ that best predict where sample $i$ based on its nearest neighbors.\n",
    "\n",
    "We can do this for all samples simultaneously by minimizing the goal function\n",
    "\n",
    "$$ \\Large\n",
    "\\sum_i \\left|\\, \\vec{X}_i - \\sum_{j\\ne i} W_{ij} \\vec{X}_j\\,\\right|^2\n",
    "$$\n",
    "\n",
    "with respect to the $n\\times N$ weights in $W$.\n",
    "\n",
    "The key insight of LLE is that once a suitable set of weights $W$ have been found:\n",
    " - they fully describe the manifold's local geometry, and\n",
    "\n",
    " - this geometry can then be *transferred* to another (smaller) space of latent variables.\n",
    " \n",
    "The way this works is we minimize a second, very similar, goal function\n",
    "\n",
    "$$ \\Large\n",
    "\\sum_i \\left|\\, \\vec{Y}_i - \\sum_{j\\ne i} W_{ij} \\vec{Y}_j\\,\\right|^2\n",
    "$$\n",
    "\n",
    "where each sample $X_i$ has a corresponding $Y_i$ but these can have completely different dimensions!  Note that although the goals functions look similar, the parameters we minimize are different each time:\n",
    " - We first minimize with respect to the elements of $W$, with the input data $X$ fixed.\n",
    "\n",
    " - Next, we minimize with respect to the latent variables $Y_i$, with the weight matrix $W$ fixed.\n",
    " \n",
    "This method was discovered in 2000 and the [original paper](https://doi.org/10.1126/science.290.5500.2323) is quite accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [LLE method](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html) lives in the sklearn `manifold` module and follows the usual calling pattern, with two significant hyperparameters:\n",
    "  1. The number of nearest neighbors to use to calculate $W$.\n",
    "\n",
    "  2. The number of latent variables (components) to use in $Y$.\n",
    "\n",
    "In order to get reproducible results, you should also pass in a RandomState."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = np.random.RandomState(seed=123)\n",
    "fit = manifold.LocallyLinearEmbedding(n_neighbors=10, n_components=2, random_state=gen).fit(ess_data)\n",
    "Y = fit.transform(ess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After projecting into the latent space, we find that the S-shape has been effectively flattened out, although not to a nice rectangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:, 0], Y[:, 1], c=ess_targets['y'], cmap='plasma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with what a linear PCA finds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = decomposition.PCA(n_components=2, random_state=gen).fit(ess_data)\n",
    "Y2 = fit.transform(ess_data)\n",
    "plt.scatter(Y2[:, 0], Y2[:, 1], c=ess_targets['y'], cmap='plasma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a KernelPCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = decomposition.KernelPCA(kernel='rbf', n_components=2, random_state=gen).fit(ess_data)\n",
    "Y3 = fit.transform(ess_data)\n",
    "plt.scatter(Y3[:, 0], Y3[:, 1], c=ess_targets['y'], cmap='plasma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn LLE class also provides some variants of LLE that can perform even better on this problem, for example (note the larger `n_neighbors` required - another example fine tuning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = manifold.LocallyLinearEmbedding(n_neighbors=25, n_components=2, method='ltsa', random_state=gen).fit(ess_data)\n",
    "Y4 = fit.transform(ess_data)\n",
    "plt.scatter(Y4[:, 0], Y4[:, 1], c=ess_targets['y'], cmap='plasma');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "© Copyright 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
