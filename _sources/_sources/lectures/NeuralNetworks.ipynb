{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn\n",
    "import functools\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we inlucde a set of helper functions to visualize the calculations for illustration purposes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_map2d(fx, params=None, x_range=3, ax=None, vlim=None, label=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    if vlim is None:\n",
    "        vlim = np.max(np.abs(fx))\n",
    "    ax.imshow(fx, interpolation='none', origin='lower', cmap='coolwarm',\n",
    "              aspect='equal', extent=[-x_range, +x_range, -x_range, +x_range],\n",
    "              vmin=-vlim, vmax=+vlim)\n",
    "    if params:\n",
    "        w, b = params\n",
    "        w = np.asarray(w)\n",
    "        x0 = -b * w / np.dot(w, w)\n",
    "        ax.annotate('', xy=x0 + w, xytext=x0,\n",
    "                    arrowprops=dict(arrowstyle='->', lw=3, color='k'))\n",
    "    if label:\n",
    "        ax.text(0.5, 0.9, label, horizontalalignment='center',\n",
    "                color='k', fontsize=16, transform=ax.transAxes)\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "def nn_unit_draw2d(w, b, phi, x_range=3, nx=250, ax=None, vlim=None, label=None):\n",
    "    \"\"\"Draw a single network unit or layer with 2D input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array\n",
    "        1D array of weight values to use.\n",
    "    b : float or array\n",
    "        scalar (unit) or 1D array (layer) of bias values to use.\n",
    "    phi : callable\n",
    "        Activation function to use.\n",
    "    \"\"\"\n",
    "    x_i = np.linspace(-x_range, +x_range, nx)\n",
    "    X = np.stack(np.meshgrid(x_i, x_i), axis=2).reshape(nx ** 2, 2)\n",
    "    W = np.asarray(w).reshape(2, 1)\n",
    "    fx = phi(np.dot(X, W) + b).reshape(nx, nx)\n",
    "    nn_map2d(fx, (w, b), x_range, ax, vlim, label)\n",
    "\n",
    "\n",
    "def nn_graph_draw2d(*layers, x_range=3, label=None, n_grid=250, n_bins=25):\n",
    "    \"\"\"Draw the response of a neural network with 2D input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    *layers : tuple\n",
    "        Each layer is specified as a tuple (W, b, phi).\n",
    "    \"\"\"\n",
    "    x_i = np.linspace(-x_range, +x_range, n_grid)\n",
    "    layer_in = np.stack(np.meshgrid(x_i, x_i), axis=0).reshape(2, -1)\n",
    "    layer_out = [ ]\n",
    "    layer_s = [ ]\n",
    "    depth = len(layers)\n",
    "    n_max, vlim = 0, 0.\n",
    "    for i, (W, b, phi) in enumerate(layers):\n",
    "        WT = np.asarray(W).T\n",
    "        b = np.asarray(b)\n",
    "        n_out, n_in = WT.shape\n",
    "        n_max = max(n_max, n_out)\n",
    "        if layer_in.shape != (n_in, n_grid ** 2):\n",
    "            raise ValueError(\n",
    "                'LYR{}: number of rows in W ({}) does not match layer input size ({}).'\n",
    "                .format(i + 1, n_in, layer_in.shape))\n",
    "        if b.shape != (n_out,):\n",
    "            raise ValueError(\n",
    "                'LYR{}: number of columns in W ({}) does not match size of b ({}).'\n",
    "                .format(i + 1, n_out, b.shape))\n",
    "        s = np.dot(WT, layer_in) + b.reshape(-1, 1)\n",
    "        layer_s.append(s)\n",
    "        layer_out.append(phi(s))\n",
    "        layer_in = layer_out[-1]\n",
    "        vlim = max(vlim, np.max(layer_in))\n",
    "    _, ax = plt.subplots(n_max, depth, figsize=(3 * depth, 3 * n_max),\n",
    "                         sharex=True, sharey=True, squeeze=False)\n",
    "    for i, (W, b, phi) in enumerate(layers):\n",
    "        W = np.asarray(W)\n",
    "        for j in range(n_max):\n",
    "            if j >= len(layer_out[i]):\n",
    "                ax[j, i].axis('off')\n",
    "                continue\n",
    "            label = 'LYR{}-NODE{}'.format(i + 1, j + 1)\n",
    "            params = (W[:,j], b[j]) if i == 0 else None\n",
    "            fx = layer_out[i][j]\n",
    "            nn_map2d(fx.reshape(n_grid, n_grid), params=params,\n",
    "                      ax=ax[j, i], vlim=vlim, x_range=x_range, label=label)\n",
    "            if i > 0 and n_bins:\n",
    "                s = layer_s[i][j]\n",
    "                s_min, s_max = np.min(s), np.max(s)\n",
    "                t = 2 * x_range * (s - s_min) / (s_max - s_min) - x_range\n",
    "                rhs = ax[j, i].twinx()\n",
    "                hist, bins, _ = rhs.hist(\n",
    "                    t, bins=n_bins, range=(-x_range, x_range),\n",
    "                    histtype='stepfilled', color='w', lw=1, alpha=0.25)\n",
    "                s_grid = np.linspace(s_min, s_max, 101)\n",
    "                t_grid = np.linspace(-x_range, +x_range, 101)\n",
    "                phi_grid = phi(s_grid)\n",
    "                phi_min, phi_max = np.min(phi_grid), np.max(phi_grid)\n",
    "                z_grid = (\n",
    "                    (phi_grid - phi_min) / (phi_max - phi_min) * np.max(hist))\n",
    "                rhs.plot(t_grid, z_grid, 'k--', lw=1, alpha=0.5)\n",
    "                rhs.axvline(-x_range * (s_max + s_min) / (s_max - s_min),\n",
    "                            c='w', lw=1, alpha=0.5)\n",
    "                rhs.set_xlim(-x_range, +x_range)\n",
    "                rhs.axis('off')\n",
    "    plt.subplots_adjust(wspace=0.015, hspace=0.010,\n",
    "                        left=0, right=1, bottom=0, top=1)\n",
    "\n",
    "def sizes_as_string(tensors):\n",
    "    if isinstance(tensors, torch.Tensor):\n",
    "        return str(tuple(tensors.size()))\n",
    "    else:\n",
    "        return ', '.join([sizes_as_string(T) for T in tensors])\n",
    "\n",
    "def trace_forward(module, input, output, name='', verbose=False):\n",
    "    \"\"\"Implement the module forward hook API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input : tuple or tensor\n",
    "        Input tensor(s) to this module. We save a detached\n",
    "        copy to this module's `input` attribute.\n",
    "    output : tuple or tensor\n",
    "        Output tensor(s) to this module. We save a detached\n",
    "        copy to this module's `output` attribute.        \n",
    "    \"\"\"\n",
    "    if isinstance(input, tuple):\n",
    "        module.input = [I.detach() for I in input]\n",
    "        if len(module.input) == 1:\n",
    "            module.input = module.input[0]\n",
    "    else:\n",
    "        module.input = input.detach()\n",
    "    if isinstance(output, tuple):\n",
    "        module.output = tuple([O.detach() for O in output])\n",
    "        if len(module.output) == 1:\n",
    "            module.output = module.output[0]\n",
    "    else:\n",
    "        module.output = output.detach()\n",
    "    if verbose:\n",
    "        print(f'{name}: IN {sizes_as_string(module.input)} OUT {sizes_as_string(module.output)}')\n",
    "\n",
    "def trace_backward(module, grad_in, grad_out, name='', verbose=False):\n",
    "    \"\"\"Implement the module backward hook API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grad_in : tuple or tensor\n",
    "        Gradient tensor(s) for each input to this module.\n",
    "        These are the *outputs* from backwards propagation and we\n",
    "        ignore them.\n",
    "    grad_out : tuple or tensor\n",
    "        Gradient tensor(s) for each output to this module.\n",
    "        Theser are the *inputs* to backwards propagation and\n",
    "        we save detached views to the module's `grad` attribute.\n",
    "        If grad_out is a tuple with only one entry, which is usually\n",
    "        the case, save the tensor directly.\n",
    "    \"\"\"\n",
    "    if isinstance(grad_out, tuple):\n",
    "        module.grad = tuple([O.detach() for O in grad_out])\n",
    "        if len(module.grad) == 1:\n",
    "            module.grad = module.grad[0]\n",
    "    else:\n",
    "        module.grad = grad_out.detach()\n",
    "    if verbose:\n",
    "        print(f'{name}: GRAD {sizes_as_string(module.grad)}')\n",
    "\n",
    "def trace(module, active=True, verbose=False):\n",
    "    if hasattr(module, '_trace_hooks'):\n",
    "        # Remove all previous tracing hooks.\n",
    "        for hook in module._trace_hooks:\n",
    "            hook.remove()\n",
    "    if not active:\n",
    "        return\n",
    "    module._trace_hooks = []\n",
    "    for name, submodule in module.named_modules():\n",
    "        if submodule is module:\n",
    "            continue\n",
    "        module._trace_hooks.append(submodule.register_forward_hook(\n",
    "            functools.partial(trace_forward, name=name, verbose=verbose)))\n",
    "        module._trace_hooks.append(submodule.register_full_backward_hook(\n",
    "            functools.partial(trace_backward, name=name, verbose=verbose)))\n",
    "\n",
    "\n",
    "def get_lr(self, name='lr'):\n",
    "    lr_grps = [grp for grp in self.param_groups if name in grp]\n",
    "    if not lr_grps:\n",
    "        raise ValueError(f'Optimizer has no parameter called \"{name}\".')\n",
    "    if len(lr_grps) > 1:\n",
    "        raise ValueError(f'Optimizer has multiple parameters called \"{name}\".')\n",
    "    return lr_grps[0][name]\n",
    "\n",
    "def set_lr(self, value, name='lr'):\n",
    "    lr_grps = [grp for grp in self.param_groups if name in grp]\n",
    "    if not lr_grps:\n",
    "        raise ValueError(f'Optimizer has no parameter called \"{name}\".')\n",
    "    if len(lr_grps) > 1:\n",
    "        raise ValueError(f'Optimizer has multiple parameters called \"{name}\".')\n",
    "    lr_grps[0][name] = value\n",
    "\n",
    "# Add get_lr, set_lr methods to all Optimizer subclasses.\n",
    "torch.optim.Optimizer.get_lr = get_lr\n",
    "torch.optim.Optimizer.set_lr = set_lr\n",
    "\n",
    "def lr_scan(loader, model, loss_fn, optimizer, lr_start=1e-6, lr_stop=1., lr_steps=100):\n",
    "    \"\"\"Implement the learning-rate scan described in Smith 2015.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Save the model and optimizer states before scanning.\n",
    "    model_save = copy.deepcopy(model.state_dict())\n",
    "    optim_save = copy.deepcopy(optimizer.state_dict())\n",
    "    # Schedule learning rate to increase in logarithmic steps.\n",
    "    lr_schedule = np.logspace(np.log10(lr_start), np.log10(lr_stop), lr_steps)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    scanning = True\n",
    "    while scanning:\n",
    "        for x_in, y_tgt in loader:\n",
    "            optimizer.set_lr(lr_schedule[len(losses)])\n",
    "            y_pred = model(x_in)\n",
    "            loss = loss_fn(y_pred, y_tgt)\n",
    "            losses.append(loss.data)\n",
    "            if len(losses) == lr_steps or losses[-1] > 10 * losses[0]:\n",
    "                scanning = False\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Restore the model and optimizer state.\n",
    "    model.load_state_dict(model_save)\n",
    "    optimizer.load_state_dict(optim_save)\n",
    "    # Plot the scan results.\n",
    "    plt.plot(lr_schedule[:len(losses)], losses, '.')\n",
    "    plt.ylim(0.5 * np.min(losses), 10 * losses[0])\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning rate')\n",
    "    plt.ylabel('Loss')\n",
    "    # Return an optimizer with set_lr/get_lr methods, and lr set to half of the best value found.\n",
    "    idx = np.argmin(losses)\n",
    "    lr_set = 0.5 * lr_schedule[idx]\n",
    "    print(f'Recommended lr={lr_set:.3g}.')\n",
    "    optimizer.set_lr(lr_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Overview</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a user's perspective, a neural network (NN) is a class of models \n",
    "\n",
    "$$ \\Large\n",
    "X_\\text{out} = N(X_\\text{in}; \\Theta)\n",
    "$$\n",
    "that are:\n",
    " - <span style=\"color:Violet\">Generic</span>: they are not tailored to any particular application.\n",
    "\n",
    " - <span style=\"color:Violet\">Flexible:</span> they can accurately represent a wide range of non-linear $X_\\text{in}\\rightarrow X_\\text{out}$ mappings with a suitable choice of parameters $\\Theta$.\n",
    "\n",
    " - <span style=\"color:Violet\">Trainable:</span> a robust optimization algorithm (backpropagation) can learn parameters $\\Theta$ given enough training data $D = (X_\\text{in},Y_\\text{tgt})$.\n",
    "\n",
    " - <span style=\"color:Violet\">Modular:</span> it is straightforward to scale the model complexity (and number of parameters) to match the available training data.\n",
    "\n",
    " - <span style=\"color:Violet\">Efficient:</span> most of the internal computations are linear and amenable to parallel computation and hardware acceleration.\n",
    "\n",
    "The \"neural\" aspect of a NN is tenuous. Their design mimics some aspects of biological neurons, but also differs in fundamental ways.\n",
    "\n",
    "In this notebook, we will explore NNs from several different perspectives:\n",
    " - <span style=\"color:Violet\">Mathematical</span>: What equations describe a network?\n",
    "\n",
    " - <span style=\"color:Violet\">Visual</span>: What does the network graph look like? How is the input space mapped through the network?\n",
    "\n",
    " - <span style=\"color:Violet\">Data Flow</span>: What are the tensors that parameterize and flow (forwards and backwards) through a network?\n",
    "\n",
    " - <span style=\"color:Violet\">Statistical</span>: What are typical distributions of tensor values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Mathematical Perspective</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Building Block</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal structure of a NN is naturally described by a computation graph that connects simple building blocks. The basic building-block unit is a function of $D$ input features $x_i$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large\n",
    "f(\\mathbf{x}) = \\phi\\left(\\mathbf{x}\\cdot\\mathbf{w} + b\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $D+1$ parameters consisting of $D$ **weights** $w_i$ and a single **bias** $b$. The corresponding [graph](http://alexlenail.me/NN-SVG/index.html) (with $D=8$) is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/NeuralNetworks-nn_unit.png\" width=400 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the left nodes correspond to the elements of the input $\\mathbf{x}$, the edges correspond to the elements of $\\mathbf{w}$ (thickness ~ strength, red/blue are pos/neg values), and the right node is the output value $f(\\mathbf{x})$. The recipe for obtaining the output value is then:\n",
    "\n",
    " - propagate each input value $x_i$ with a strength $w_i$,\n",
    "\n",
    " - sum the values $x_i w_i$,\n",
    "\n",
    " - apply the activation $\\phi$.\n",
    "\n",
    "Note that this building block is mostly linear, except for the **activation function** $\\phi(s)$. This is an application of the kernel trick that we met [earlier](Nonlinear.ipynb), and allows us to implicitly work in a higher dimensional space where non-linear structure in data is easier to model.\n",
    "\n",
    "The building-block equation is straightfoward to implement as code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_unit(x, w, b, phi):\n",
    "    return phi(np.dot(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with a 3D input $\\mathbf{x}$, the weight vector $\\mathbf{w}$ should also be 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_unit(x=[0, 1, -1], w=[1, 2, 3], b=-1, phi=np.tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Activation Functions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function $\\phi$ argument $s$ is always a scalar and, by convention, activation functions are always defined in a standard form, without any parameters (since $\\mathbf{w}$ and $b$ already provide enough learning flexibility).\n",
    "\n",
    "Some popular activations are defined below (using [lambda functions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)). For the full list supported in PyTorch see [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = lambda s: np.maximum(0, s)\n",
    "elu = lambda s: np.maximum(0, s) + np.minimum(0, np.expm1(s)) # expm1(s) = exp(s) - 1\n",
    "softplus = lambda s: np.log(1 + np.exp(s))\n",
    "sigmoid = lambda s: 1 / (1 + np.exp(-s)) # also known as the \"logistic function\"\n",
    "tanh = lambda s: np.tanh(s)\n",
    "softsign = lambda s: s / (np.abs(s) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These activations divide naturally into two categories depending on their asymptotic behavior as $s\\rightarrow +\\infty$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(ax, names, s_range=5, y_range=2):\n",
    "    s = np.linspace(-s_range, +s_range, 101)\n",
    "    for name in names.split(','):\n",
    "        phi = eval(name)\n",
    "        ax.plot(s, phi(s), lw=4, alpha=0.5, label=name)\n",
    "    ax.legend(fontsize='x-large')\n",
    "    ax.set_xlabel('Activation input $s$')\n",
    "    ax.set_ylabel('Activation output $\\phi(s)$')\n",
    "    ax.set_xlim(-s_range, +s_range)\n",
    "    ax.set_ylim(-y_range, +y_range)\n",
    "    ax.axhline(-1, c='gray', ls=':')\n",
    "    ax.axhline(+1, c='gray', ls=':')\n",
    "    \n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "plot_activations(ax[0], 'relu,elu,softplus')\n",
    "plot_activations(ax[1], 'sigmoid,tanh,softsign')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all activations saturate (at -1 or 0) for $s\\rightarrow -\\infty$, but differ in their behavior when $s\\rightarrow +\\infty$ (linear vs saturate at +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "___<span style=\"color:Violet\">DISCUSS</span>___:\n",
    " - Which activation would you expect to be the fastest to compute?\n",
    "\n",
    " - Which activations are better suited for a binary classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The `relu` activation is the fastest to compute since it does not involve any transcendental function calls (exp, log, ...).\n",
    "\n",
    "The activations that are bounded on both sides only have a narrow range near $s=0$ where they distinguish between different input values, and otherwise are essentially saturated at one of two values.  This is desirable for classification, where the aim is to place $s=0$ close to the \"decision boundary\" (by learning a suitable bias).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Network Layer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we replace the vectors $\\mathbf{x}$ and $\\mathbf{w}$ above with matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "F(X) = \\phi\\left( X W + \\mathbf{b}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $X$ has shape $(N, D)$ and holds $N$ samples of $D$ features, then $W$ must have shape $(D, M)$ so $F(X)$ converts the $D$ input features into $M$ output features for each sample. We say that $F$ represents a linear network **layer** with $D$ input nodes and $M$ output nodes. Note that the bias is now a vector of $M$ bias values, one for each output value.\n",
    "\n",
    "We cannot really add a vector $\\mathbf{b}$ to the matrix $X W$ but we are using the \"broadcasting\" convention that this means add the same vector to each row (sample) of $X W$.  We also cannot apply $\\phi(s)$ to a matrix, but we are using the \"elementwise\" convention that this means apply $\\phi$ separately to each element of the matrix.\n",
    "\n",
    "To connect this matrix version with our earlier vector version, notice that $F(X)$ transforms a single input sample $\\mathbf{x}_i$ (row of $X$) into $M$ different outputs, $f_m(\\mathbf{x}_i)$ each with their own weight vector and bias value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "f_m(\\mathbf{x}_i) = \\phi\\left(\\mathbf{x}_i\\cdot \\mathbf{w}_m + b_m\\right) \\; ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{w}_m$ is the $m$-th column of $W$ and $b_m$ is the $m$-th element of $\\mathbf{b}$.\n",
    "\n",
    "The corresponding graph (with $D=8$ and $M=4$) is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/NeuralNetworks-nn_layer.png\" width=400 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn_unit` function we defined above already implements a layer if we pass it matrices $X$ and $W$ and a vector $\\mathbf{b}$. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_unit(x=[[1., 0.5], [-1, 1]], w=[[1, -1, 1], [2, 0, 1]], b=[-1, 1, 0], phi=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer with $n_{in}$ inputs and $n_{out}$ outputs has a total of $(n_{in} + 1) ~n_{out}$ parameters. These can add up quickly when building useful networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Network Graph</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build a simple **fully connected graph** by stacking layers horizontally, which corresponds to nested calls of each layer's function. For example, with 3 layers computed by $F$, $G$, $H$ stacked (left to right), the overall graph computation is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "N(X) = H\\left(G\\left(F(X)\\right)\\right) \\; ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a corresponding graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/NeuralNetworks-nn_graph.png\" width=900 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes between the input (leftmost) and output (rightmost) nodes are known as **hidden nodes**.\n",
    "\n",
    "The corresponding code for arbitrary layers is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_graph(X, *layers):\n",
    "    for W, b, phi in layers:\n",
    "        X = nn_unit(X, W, b, phi)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here is a three-layer network with the same architecture as the graph above. Note how the output dimension of one layer must match the input dimension of the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph([1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    ([\n",
    "        [11, 12, 13, 14],\n",
    "        [21, 22, 23, 24],\n",
    "        [31, 32, 33, 34],\n",
    "        [41, 42, 43, 44],\n",
    "        [51, 52, 53, 54],\n",
    "        [61, 62, 63, 64],\n",
    "        [71, 72, 73, 74],\n",
    "        [81, 82, 83, 84],\n",
    "    ], [1, 2, 3, 4], tanh),    # LYR1: n_in=8, n_out=4\n",
    "    ([\n",
    "        [11, 12, 13],\n",
    "        [21, 22, 23],\n",
    "        [31, 32, 33],\n",
    "        [41, 42, 43],\n",
    "    ], [1, 2, 3], relu),       # LYR2: n_in=4, n_out=3\n",
    "    ([\n",
    "        [11, 12],\n",
    "        [21, 22],\n",
    "        [31, 32],\n",
    "    ], [1, 2], sigmoid)        # LYR3: n_in=3, n_out=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight and bias values are chosen to make the tensors easier to read, but would not make sense for a real network. As a result, the final output of `[1., 1.]` is not surprising given how the sigmoid activation saturates for input outside a narrow range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Visual Perspective</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/NeuralNetworks-activation_maps.png\" width=900 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:Violet\">EXERCISE</span>:___ Identify which activation function was used to make each plot above, which shows the building block "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "f(\\mathbf{x}) = \\phi\\left(\\mathbf{x}\\cdot\\mathbf{w} + b\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "for a 2D $\\mathbf{x}$ with the same $\\mathbf{w}$ and $b$ used in each plot. Red and blue indicate positive and negative values, respectively, with zero displayed as white. For calibration, (a) shows a \"linear\" activation which passes its input straight through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "- (a) linear\n",
    "\n",
    "- (b) tanh\n",
    "\n",
    "- (c) relu\n",
    "\n",
    "- (d) softsign\n",
    "\n",
    "- (e) sigmoid\n",
    "\n",
    "- (f) elu\n",
    "\n",
    "To distinguish between (b) and (d), note that both go asymptotically to constant negative and positive values (so sigmoid is ruled out), but the white transition region is narrower for (d).\n",
    "\n",
    "To distinguish between (c) and (f), note that (c) goes asymptotically to zero (white) in the top-left corner, while (e) goes asymptotically to a constant negative value (blue).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "___<span style=\"color:Violet\">EXERCISE</span>___: Experiment with the following function to determine how the displayed arrow relates to the three model parameters $w_0, w_1, b$:\n",
    "```\n",
    "nn_unit_draw2d(w=[0, 2], b=-1, phi=tanh)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The arrow has the direction and magnitude of the 2D vector $\\mathbf{w}$, with its origin at $\\mathbf{x} = -b \\mathbf{w}\\, / \\, |\\mathbf{w}|^2$ where $s = 0$. The line $s=0$ is perpendicular to the arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_unit_draw2d(w=[2, 1], b=+1, phi=tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the plots below which show the hidden (left) and output (right) node values for a network with 2 + 2 + 1 nodes. Each graph shows the node value as a function of the 2D input value.\n",
    "\n",
    "Note how the hidden nodes divide the input space into two halves, with a dividing line determined by their $\\mathbf{w}$ and $b$ values. The output layer then mixes these halves and can therefore \"select\" any of the four quadrants with an appropriate choice of its $\\mathbf{w}$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_graph_draw2d(\n",
    "    ([[2, 0],[-1, -2]], [0, 0], tanh), # LYR1\n",
    "    ([[1], [-1]], [-1], tanh)          # LYR2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram on the second layer plot shows the distribution of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large\n",
    "s = X W  + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feeding its activation function (shown as the dashed curve). Note how the central histogram peak is higher because both the lower-right and upper-left quadrants of $(x_1, x_2)$ have $Y W \\simeq 0$. The vertical white line shows how our choice of bias $b = -0.5$ places these quadrants in the \"rejected\" (blue) category with $s < 0$.\n",
    "\n",
    "Generalizing this example, a layer with $n$ inputs can \"select\" a different $n$-sided (soft-edged) polygon with each of its outputs. To see this in action, try [this demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Data Flow Perspective</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below show the tensors flowing forward (left to right) in a typical fully connected graph. The main flow consists of $N$ input samples flowing from $X_0$ to $X_4$ with a number of features that varies between layers. \n",
    "\n",
    "The computation of each layer's output is parameterized by the weight and bias tensors shown: note how their shapes are determined by the number of input and output features for each layer. The parameter tensors are usually randomly initialized (more on this soon) so only the input $X_0$ and target $Y$ are needed to drive the calculation (and so must be copied to GPU memory when using hardware acceleration).\n",
    "\n",
    "The final output $X_4$ is compared with the target values $Y$ to calculate a \"loss\" $\\ell(X_4, Y)$ that decreases as $X_4$ becomes more similar to $Y$ (more on this soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/NeuralNetworks-forward_flow.png\" width=1000 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below shows the gradient (partial derivative) tensors flowing backwards (\"backpropagation\") through the same graph using the chain rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "\\frac{\\partial \\ell}{\\partial X_n} = \\frac{\\partial \\ell}{\\partial X_{n+1}} \\frac{\\partial X_{n+1}}{\\partial X_n}\n",
    "\\quad, \\quad\n",
    "\\frac{\\partial \\ell}{\\partial W_{n+1}} = \\frac{\\partial \\ell}{\\partial X_{n+1}} \\frac{\\partial X_{n+1}}{\\partial W_{n+1}} \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/NeuralNetworks-backward_flow.png\" width=1000 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these gradient tensors are just numbers, not functions. All of these tensors occupy the (limited) GPU memory when using hardware acceleration but, in most applications, only the final output and the parameter gradients are stored (with 32-bit floating point precision).\n",
    "\n",
    "When working with large datasets, the $N$ input samples are usually broken up into fixed-size randomly subsampled \"minibatches\". Optimiztion with the resulting parameter gradients leads to the \"stochastic gradient descent\" (SGD) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">PyTorch Primer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fully connected network can be created with a few lines in PyTorch (for a similar high-level API in Tensorflow checkout [Keras](https://www.tensorflow.org/guide/keras)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(8, 4), #0\n",
    "    torch.nn.ReLU(),    #1\n",
    "    torch.nn.Linear(4, 3), #2\n",
    "    torch.nn.ReLU(),    #3\n",
    "    torch.nn.Linear(3, 2)  #4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each `Linear` layer is created, its weight and bias tensors are automatically initialized with random values, so we initially set the torch random seed for reproducible results.\n",
    "\n",
    "This construction breaks each layer into separate linear and activation \"modules\". Each module can be accessed via its index (0-4 in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[4].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our network in the forward direction, we need some data with the expected number of features ($D=8$ in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = net[0].in_features\n",
    "Xin = torch.randn(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xout = net(Xin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate tensors ($X_1$, $\\partial\\ell/\\partial X_1$, ...) shown in the data flow diagrams above are usually not preserved, but can be useful to help understand how a network is performing and diagnose problems. To cache these intermediate tensors, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xout = net(Xin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each submodule now has `input` and `output` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(Xin, net[0].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(net[0].output, net[1].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `verbose` option to watch the flow of tensors through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace(net, verbose=True)\n",
    "Xout = net(Xin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the computational graph we need to calculate a (scalar) loss, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean(Xout ** 2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now back propagate gradients of this loss through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients of each layer's parameters are now computed and stored, ready to \"learn\" better parameters through (stochastic) gradient descent (or one of its variants):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `trace` above we have also captured the gradients of the loss with respect to each module's outputs $\\partial\\ell /\\partial X_n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].output.size(), net[0].grad.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients can be useful to study since learning of all upstream parameters effectively stops when they become vanishly small (since they multiply those parameter gradients via the chain rule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Statistical Perspective</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors behind a practical network contain so many values that it is usually not practical to examine them individually. However, we can still gain useful insights if we study their probability distributions.\n",
    "\n",
    "Build a network to process a large dataset so we have some distributions to study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "N, D = 500, 100\n",
    "Xin = torch.randn(N, D)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D, 2 * D),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(2 * D, D),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(D, 10)\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our network ends with a `Linear` module instead of an activation, which is typical for regression problems.\n",
    "\n",
    "Perform forward and backward passes to capture some values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace(net, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xout = net(Xin)\n",
    "loss = torch.mean(Xout ** 2)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check that the input to the first module has the expected (unit normal) distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[0].input.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does torch initialize the parameters (weights and biases) for each layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[0].weight.data.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[0].bias.data, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These initial parameter values are sampled from uniform distributions centered on zero with a spread that depends on the number of inputs to the layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "\\left|W_{ij}\\right|, \\left|b_j\\right| \\le n_{in}^{-1/2} \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This default choice is based on [empirical studies](https://arxiv.org/abs/1502.01852) of image classification problems where the input features (RGB pixel values) were preprocessed to have zero mean and unit variance.\n",
    "\n",
    "With this choice of weights, the first `Linear` module mixes up its input values ($X_0$) but generally preserves Gaussian shape while slightly reducing its variance (which helps prevent the subsequent activation module from saturating):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[0].output.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot of the the first `Tanh` activation function's input and output values just traces out function since it is applied element wise.  Note how most of input values do not saturate, which is generally desirable for efficient learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(net[1].input.reshape(-1), net[1].output.reshape(-1), s=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linear activation distorts and clips the output so it no longer resembles a Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[1].output.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the next `Linear` module restores the Gaussian distribution!  How does this happen when neither its inputs nor its parameters have a Gaussian distribution? (Answer: the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) which we briefly covered [earlier](https://nbviewer.jupyter.org/github/dkirkby/MachineLearningStatistics/blob/master/notebooks/Statistics.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[2].output.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next activation is `ReLU`, which effectively piles up all negative values from the previous `Linear` module into the zero bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(net[3].input.reshape(-1), net[3].output.reshape(-1), s=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[3].output.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final linear layer's output is again roughly Gaussian, thanks to the central limit theorem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(net[4].output.reshape(-1), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only looked at distributions of the tensors involved in the forward pass, but there is also a lot to learn from the backwards gradient tensors that we do not have time to delve in to.  For example, this scatter plot offers some insight into a suitable learning rate for the second `Linear` module's weight parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(net[2].weight.data.reshape(-1), net[2].weight.grad.reshape(-1), s=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the normalization of the loss function feeds directly into these gradients, so needs to be considered when setting the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xout = net(Xin)\n",
    "loss = 100 * torch.mean(Xout ** 2)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(net[2].weight.data.reshape(-1), net[2].weight.grad.reshape(-1), s=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Loss Functions</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order discover a good set of parameters using optimization, we need to specify a loss function to optimize.\n",
    "\n",
    "The loss function $\\ell(X_\\text{out}, Y_\\text{tgt})$ compares the actual network output $X_\\text{out}$ with a corresponding target value $Y_\\text{tgt}$ and approaches some minimum value as their agreement improves.\n",
    "\n",
    "A loss function must be scalar valued since we need a single gradient for each parameter to implement gradient descent,\n",
    "$$\\Large\n",
    "\\theta \\rightarrow \\theta_i - \\eta\\,\\frac{\\partial\\ell}{\\partial\\theta} \\; .\n",
    "$$\n",
    "Note that the loss normalization is degenerate with the learning rate $\\eta$.\n",
    "\n",
    "Our choice of loss function is primarily driven by the type of problem we are solving: regression or classification. We introduce the most obvious choices below but there are lots of reasonable variations (see [here](https://pytorch.org/docs/stable/nn.html#id51) for the complete PyTorch list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Regression Loss</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, the $L_2$ norm is a popular choice,\n",
    "\n",
    "$$\\Large\n",
    "L_2 = \\frac{1}{2}\\, \\left|\n",
    "X_\\text{out} - Y_\\text{tgt}\\right|^2 \\; .\n",
    "$$\n",
    "\n",
    "Optimizing the $L_2$ norm is equivalent to finding the maximum-likelihood (ML) point estimate for the network parameters (weights and biases) if we assume that the uncertainties in $Y_\\text{tgt}$ are \"homoscedastic\" (drawn from the same Gaussian distribution).\n",
    "\n",
    "In PyTorch, the $L_2$ norm is implemented as [torch.nn.MSELoss](https://pytorch.org/docs/stable/nn.html#mseloss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.zeros_like(Xout)\n",
    "loss = torch.nn.MSELoss()(Xout, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you actually have a reasonable estimate $\\sigma_Y^i$ of the $i$-th sample's target uncertainty, a better loss function is the $\\chi^2$ statistic:\n",
    "\n",
    "$$\\Large\n",
    "\\chi^2 = \\sum_{i=1}^N\\, \\left( \\frac{X_\\text{out}^i - Y_\\text{tgt}^i}{\\sigma_Y^i}\\right)^2 \\; .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:LightGreen\">Binary Classification Loss</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification problems, the L2 norm can also be used but the binary [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between the target and output probability distributions is often a better choice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "\\text{BCE} \\equiv -\\sum_{i=1}^N\\, \\left[\n",
    "Y_{tgt}^i ~\\log \\phi_S(X_\\text{out}^i) + (1 - Y_\\text{tgt}^i) ~\\log (1 - \\phi_S(X_\\text{out}^i)) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\phi_S$ is the sigmoid (aka logistic) activation function used to coerce arbitrary real values into the range $[0,1]$ required for a probability. An input real value used with sigmoid like this is known as a [logit](https://en.wikipedia.org/wiki/Logit).\n",
    "\n",
    "The equivalent PyTorch code uses [torch.nn.BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xout = torch.ones(10)\n",
    "Y = torch.zeros(10)\n",
    "loss = torch.nn.BCELoss()(Xout, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross entropy is inspired by information theory and closely related to the KL divergence we met [earlier](Variational.ipynb). With this approach, our assumptions are that:\n",
    " - The target values in $Y_{tgt}$ are all either 0 or 1.\n",
    "\n",
    " - The network output values in $X_{out}$ are continuous and $\\phi_S(y^{out}_i)$ is interpreted as the corresponding probability that the output is 1.\n",
    " \n",
    "Note that *something* like the second assumption is necessary to reconcile the different domains of the data and prediction.\n",
    "\n",
    "With these assumptions, the likelihood is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "P(Y_\\text{tgt}\\mid X_\\text{out}) = \\begin{cases}\n",
    "\\phi_S(X_\\text{out}) & Y_\\text{tgt} = 1 \\\\\n",
    "1 - \\phi_S(X_\\text{out}) & Y_\\text{tgt} = 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a minute to convince yourself that the following expression is equivalent (the case $\\phi_S(X_\\text{out}(\\Theta)) = Y_\\text{tgt} = 0$ requires some care since $0^0$ is indeterminate):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "P(Y_\\text{tgt}\\mid X_\\text{out}(\\Theta)) = \\left[\\phi_S(X_\\text{out}(\\Theta))\\right]^{Y_\\text{tgt}}\\,\n",
    "\\left[1 - \\phi_S(X_\\text{out}(\\Theta))\\right]^{1-Y_\\text{tgt}} \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this form, you can show that the cross entropy loss equals the negative-log-likelihood of the $N$ samples of training data so optimizing BCE is equivalent to finding the ML point estimate of the network parameters under the assumptions above.\n",
    "\n",
    "For fixed training data, optimizing BCE is also equivalent to minimizing the KL divergence of the network's predicted discrete probability distribution with respect to the empirical discrete probability distribution of the training data. Therefore, training a binary classification network using the cross-entropy loss is effectively performing a variational inference (VI) to find the network probabilities that are closest to the empirical training probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Multi-category Classification Loss</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we generalize the binary classification cross-entropy loss to problems with more than two categories?  The usual approach is to increase the number of output nodes from 1 to the number of categories $C$,\n",
    "but we can not directly interpret their values as category probabilities since there is no way to ensure that they sum to one. We could simply require that they are all non-negative and renormalize, but a more more robust approach is to convert the vector of output values $X_\\text{out}$ to a corresponding vector of probabilities $\\mathbf{p}$ for category $j = 1, 2, \\ldots, C$ using the <span style=\"color:Violet\">softmax function</span>,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "\\mathbf{p}(X_\\text{out}) \\equiv \\frac{1}{\\sum_{k=1}^C\\, \\exp(X_\\text{out}^k)}\\,\n",
    "[ \\exp(X_\\text{out}^1), ~\\exp(X_\\text{out}^2), \\ldots, ~\\exp(X_\\text{out}^C) ] \\; ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works fine with positive or negative outputs $X_\\text{out}^j$. Note that softmax generalizes the sigmoid function $\\phi_S$ in the following sense:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large\n",
    "\\mathbf{p}([y_1, y_2]) = [\\,\\phi_S(y_1-y_2)\\,,\\, ~1 - \\phi_S(y_1-y_2)\\,] \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    # subtract out max(y) improve the numerical accuracy\n",
    "    expy = np.exp(y - np.max(y))\n",
    "    return expy / expy.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax([2, -1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function effectively implements a <span style=\"color:Violet\">winner takes all</span> policy, similar to the sigmoid activation $\\phi_S$, as illustrated in the plot below where:\n",
    " - the color scale indicates, from left to right, $p_1, p_2$ and $p_3$ for three categories,\n",
    "\n",
    " - $y_1$ and $y_2$ are varied over the same range, and\n",
    "\n",
    " - $y_3$ is fixed to the middle of this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_softmax(ylo, yhi, n=100):\n",
    "    y_grid = np.linspace(ylo, yhi, n)\n",
    "    y3 = 0.5 * (ylo + yhi)\n",
    "    p_grid = np.array([softmax([y1, y2, y3]) for y1 in y_grid for y2 in y_grid]).reshape(n, n, 3)\n",
    "    _, ax = plt.subplots(1, 3, figsize=(10.5, 3))\n",
    "    for k in range(3):\n",
    "        ax[k].imshow(p_grid[:, :, k], interpolation='none', origin='lower', extent=[ylo, yhi, ylo, yhi])\n",
    "        ax[k].set_xlabel('$y_1$')\n",
    "        ax[k].set_ylabel('$y_2$')\n",
    "        if k != 0: ax[k].axvline(y3, c='gray', ls='--')\n",
    "        if k != 1: ax[k].axhline(y3, c='gray', ls='--')\n",
    "        if k != 2: ax[k].plot([ylo, yhi], [ylo, yhi], c='gray', ls='--')\n",
    "        ax[k].grid(False)\n",
    "    \n",
    "plot_softmax(0, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above assumed output activations that can be large an positive, such as `relu` or `elu`. However, the strength of the *winner takes all* effect depends on how the outputs are scaled, and is relatively weak for output activations that saturate on both sides, such as `sigmoid` or `tanh`, which is why these are generally not used for classification outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_softmax(-1, +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we assume **one-hot encoding** of the vector target values $\\mathbf{y}^{out}$, which is not very efficient (unless using sparse-optimized data structures) compared to a single integer target value $y^{train} = 0, 1, \\ldots, C-1$. However, sklearn has a [convenient utility](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to convert integers to one-hot encoded vectors (use `sparse=True` to return vectors in an efficient [scipy sparse array](https://docs.scipy.org/doc/scipy/reference/sparse.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "© Copyright 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
