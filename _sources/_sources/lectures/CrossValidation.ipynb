{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, pipeline, model_selection, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Model Selection With Cross Validation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there are three main types of learning:\n",
    " - Learning model parameters from data.\n",
    "\n",
    " - Learning to predict new data (unsupervised learning).\n",
    "\n",
    " - Learning to predict target features in new data (supervised learning).\n",
    " \n",
    "All three types of learning require an assumed model with associated parameters, and predicting new data is only possible after learning model parameters from old data.\n",
    "\n",
    "Since all types of learning assume a model, they must all solve the meta-problem of comparing competing models. The Bayesian evidence $P(D\\mid M)$ is our primary quantitative tool for comparing how well different models explain the same data.  When we are primarily interested in a model's ability to generalize and predict new data, ___<span style=\"color:violet\">cross validation</span>___ is a useful alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">Train vs. Validate vs. Test Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During an ML model development and evaluation, various types of data are utilized. Below, we review these types of data, their purpose and the differences between each in how they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/CrossValidation-splits.png\" width=800 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Tan\">Training Dataset</span>: The sample of data used to fit the model.\n",
    "\n",
    "* The actual dataset that we use to train the ML model under development (weights and biases in the case of a Neural Network). The model sees and learns from this data. Training data are collections of examples or samples that are used to \"teach\" or \"train\" the model. The model uses a training data set to understand the patterns and relationships within the data, thereby learning to make predictions or decisions without being explicitly programmed to perform a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Tan\">Validation Dataset</span>: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset (i.e. a \"trained model\") while tuning model hyperparameters. The evaluation becomes more biased as more skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "* It is still possible to tune and control the model at this stage. Working on validation data is used to assess the model performance and fine-tune the parameters of the model. This becomes an iterative process wherein the model learns from the training data and is then validated and fine-tuned on the validation set. A validation dataset tells us how well the model is learning and adapting, allowing for adjustments and optimizations to be made to the model's parameters or hyperparameters before it's finally put to the test. So the validation set affects a model, but only indirectly.\n",
    "\n",
    "  * Note that not all models require validation sets. Some experts consider that ML models with no hyperparameters or those that do not have tuning options do not need a validation set. Still, in most practical applications, validation sets play a crucial role in ensuring the model's robustness and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Tan\">Test Dataset</span>: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "* The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). It is a separate sample, an unseen data set, to provide an unbiased final evaluation of a model. Its primary purpose is to offer a fair and final assessment of how the model would perform when it encounters new data in a live, operational environment.\n",
    "\n",
    "  * The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner). \n",
    "\n",
    "  * Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world on data it has never seen before. The inputs in the test data are similar to the previous stages but not the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">Cross Validation (CV) Process</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is a process that provides the ability to estimate model performance on unseen data not used while training. It is a systematic process that can involve tuning the model hyperparameters, testing different properties of the overall datasets, and iterating the training process. There are many variations of the CV process - we will study primarily the <span style=\"color:violet\">K-folds method</span> in this lecture. We will show some examples using random sampling (called \"folds\") of these types of data to illustrate the process. For simplicity, we will focus on the test and train data and not consider validation data explicitly in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below summarizes the data splitting and process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/CrossValidation-process.png\" width=600 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of ___<span style=\"color:violet\">cross validation</span>___ is to:\n",
    "1. split the observed data into separate training and test datasets\n",
    "\n",
    "2. learn the model from the training dataset\n",
    "\n",
    "3. measure the model's ability to predict the test dataset\n",
    "\n",
    "4. repeat the steps above with different splits (\"folds\") and combine the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">Overfitting and Generalization</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data consisting of 2D points along the path of a projectile, where $x$ is measured with negligible error and $y$ has a known error $\\sigma_y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlo, xhi = 0., 1.\n",
    "poly_coefs = np.array([-1., 2., -1.])\n",
    "f = lambda X: np.dot(poly_coefs, [X ** 0, X ** 1, X ** 2])\n",
    "sigma_y = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(N, seed=123):\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    X = gen.uniform(xlo, xhi, size=N)\n",
    "    y = f(X) + gen.normal(scale=sigma_y, size=N)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare results with different numbers of samples from the same model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa, ya = generate(N=15)\n",
    "Xb, yb = generate(N=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotXy(X, y, ax=None, *fits):\n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X, y, s=25, lw=0)\n",
    "    x_grid = np.linspace(xlo, xhi, 100)\n",
    "    ax.plot(x_grid, f(np.array(x_grid)), '-', lw=10, alpha=0.2)\n",
    "    for fit in fits:\n",
    "        y_fit = fit.predict(x_grid.reshape(-1, 1))\n",
    "        ax.plot(x_grid, y_fit, lw=2, alpha=1)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ylo, yhi = np.percentile(y, (0, 100))\n",
    "    dy = yhi - ylo\n",
    "    ax.set_ylim(ylo - 0.1 * dy, yhi + 0.1 * dy)\n",
    "    \n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "plotXy(Xa, ya, ax[0])\n",
    "plotXy(Xb, yb, ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The family of competing models we consider are polynomials of different degrees $P$,\n",
    "\n",
    "$$ \\Large\n",
    "y(x) = \\sum_{k=0}^P\\, c_k x^k \\; ,\n",
    "$$\n",
    "\n",
    "each with $P+1$ parameters.  The true model that the datasets were generated from have $P=2$.\n",
    "\n",
    "Use sklearn [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to implement this fit after expanding the features with [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html),\n",
    "\n",
    "$$ \\Large\n",
    "x \\; \\rightarrow\\; \\{ x^0, x^1, \\ldots, x^P \\} \\; ,\n",
    "$$\n",
    "\n",
    "and combining the preprocessing and regression steps into a [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(X, y, degree):\n",
    "    degree_is_zero = (degree == 0)\n",
    "    model = pipeline.Pipeline([\n",
    "        ('poly', preprocessing.PolynomialFeatures(degree=degree, include_bias=degree_is_zero)),\n",
    "        ('linear', linear_model.LinearRegression(fit_intercept=not degree_is_zero))])\n",
    "    return model.fit(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare fits with $P = 0, 1, 2, 14$ to each dataset. Note that $P=14$ is an extreme case of overfitting to the smaller dataset, with the model passing exactly through each sample with large oscillations between them.  Similarly, $P=1$ underfits the larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "plotXy(Xa, ya, ax[0], poly_fit(Xa, ya, 0), poly_fit(Xa, ya, 1), poly_fit(Xa, ya, 2), poly_fit(Xa, ya, 14))\n",
    "plotXy(Xb, yb, ax[1], poly_fit(Xb, yb, 0), poly_fit(Xb, yb, 1), poly_fit(Xb, yb, 2), poly_fit(Xb, yb, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">Train-Test Split</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function picks a random fraction of the observed data to hold back when learning the model and then use for latest testing.\n",
    "\n",
    "Note that since train/test splitting involves random numbers, you will need to pass around a random state object for reproducible results.\n",
    "\n",
    "The plots below show 20% of the data reserved for testing (red points) with a $P=2$ fit to the training data superimposed. The primary sklearn test metric for regression problems is the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) $R^2$, for which the goal is $R^2 = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, degree, test_fraction=0.2, ax=None, seed=123):\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=test_fraction, random_state=gen)\n",
    "    train_fit = poly_fit(X_train, y_train, degree)\n",
    "    plotXy(X, y, ax, train_fit)\n",
    "    test_R2 = train_fit.score(X_test.reshape(-1, 1), y_test)\n",
    "    ax.scatter(X_test, y_test, marker='x', color='r', s=40, zorder=10)\n",
    "    ax.text(0.7, 0.1, '$R^2={:.2f}$'.format(test_R2), transform=ax.transAxes, fontsize='x-large', color='r')\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "train_test_split(Xa, ya, 2, ax=ax[0])\n",
    "train_test_split(Xb, yb, 2, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no rigorous procedure for setting an optimum test fraction, and anything between 0.1 and 0.5 would be reasonable (and the sklearn default is 0.25).  A larger test fraction improves the reliability of the test metric but decreases the reliability of the model being tested.  As always, more data always helps and reduces your sensitivity to the training fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fraction_scan(degree=2, seed=123):\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    test_fractions = np.arange(0.05, 0.6, 0.025)\n",
    "    R2 = np.empty((2, len(test_fractions)))\n",
    "    for i, test_fraction in enumerate(test_fractions):\n",
    "        for j, (X, y) in enumerate(((Xa, ya), (Xb, yb))):\n",
    "            X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "                X, y, test_size=test_fraction, random_state=gen)\n",
    "            fit = poly_fit(X_train, y_train, degree)\n",
    "            R2[j, i] = fit.score(X_test.reshape(-1, 1), y_test)\n",
    "    plt.plot(test_fractions, R2[0], 'o:', label='$N = {}$'.format(len(ya)))\n",
    "    plt.plot(test_fractions, R2[1], 'o:', label='$N = {}$'.format(len(yb)))\n",
    "    plt.xlabel('Test fraction')\n",
    "    plt.ylabel('Test score $R^2$')\n",
    "    plt.ylim(-2, 1)\n",
    "    plt.legend()\n",
    "    \n",
    "test_fraction_scan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">K-Folding</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation goes beyond a simple train-test split by repeating the split multiple times and combining the (correlated) results. There are different strategies for picking the different splits, but [K-folding](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) is a good all-around choice:\n",
    " - Specify the number $k$ of splits (folds) to use.\n",
    "\n",
    " - The data is split into $k$ (almost) equal independent subsets.\n",
    "\n",
    " - Each subset is used for testing once, with the remaining subsets used for training.\n",
    " \n",
    "The result is $k$ different train-test splits using a test fraction $1/k$.  For example, with $N=10$ samples and $k=3$ folds, the subsets are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=3)\n",
    "[tuple(split[1]) for split in kfold.split(range(10))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [cross_validate](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) function automates the k-folding and scoring process, and outputs both train and test $R^2$ scores, as well as CPU times, for each split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cross_validate(X, y, degree, n_splits):\n",
    "    model = pipeline.Pipeline([\n",
    "        ('poly', preprocessing.PolynomialFeatures(degree=degree)),\n",
    "        ('linear', linear_model.LinearRegression(fit_intercept=True))])\n",
    "    kfold = model_selection.KFold(n_splits=n_splits)\n",
    "    scores = model_selection.cross_validate(\n",
    "        model, X.reshape(-1, 1), y, cv=kfold, return_train_score=True)\n",
    "    index = [tuple(split[1]) for split in kfold.split(X.reshape(-1, 1))]\n",
    "    return pd.DataFrame(scores, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(Xa, ya, degree=2, n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With enough data, you can use a large number of K-folds but remember that they are highly correlated so you are not increasing the useful information as much as you might think. The sklearn default number of splits is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(Xb, yb, degree=2, n_splits=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Lightgreen\">Hyperparameter Grid Search</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [GridSearchCV]() function puts all the pieces together to scan a grid of one or more hyperparameters for a family of models.  For example, the polynomial degree $P$ corresponds to a pipeline `poly__degree` parameter, which we vary from 0 to 5 below. The full output from `GridSearchCV` is detailed but unwieldy so we use the MLS `grid_search_summary()` function to create a summary table and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_summary(cv):\n",
    "    \"\"\"Summarize the results from a GridSearchCV fit.\n",
    "\n",
    "    Summarize a cross-validation grid search in a pandas DataFrame with the\n",
    "    following transformations of the full results:\n",
    "      - Remove all columns with timing measurements.\n",
    "      - Remove the 'param_' prefix from column names.\n",
    "      - Remove the '_score' suffix from column names.\n",
    "      - Round scores to 3 decimal places.\n",
    "\n",
    "     If the parameter grid is 1D, then this function also plots the test\n",
    "     and training R2 scores versus the parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv : sklearn.model_selection.GridSearchCV\n",
    "        Instance of a GridSearchCV object that has been fit to some data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Summary table of cross-validation results.\n",
    "    \"\"\"\n",
    "    # Look up the list of parameters used in the grid.\n",
    "    params = list(cv.cv_results_['params'][0].keys())\n",
    "    # Index results by the test score rank.\n",
    "    index = cv.cv_results_['rank_test_score']\n",
    "    df = pd.DataFrame(cv.cv_results_, index=index).drop(columns=['params', 'rank_test_score'])\n",
    "    # Remove columns that measure running time.\n",
    "    df = df.drop(columns=[n for n in df.columns.values if n.endswith('_time')])\n",
    "    # Remove param_ prefix from column names.\n",
    "    df = df.rename(lambda n: n[6:] if n.startswith('param_') else n, axis='columns')\n",
    "    # Remove _score suffix from column names.\n",
    "    df = df.rename(lambda n: n[:-6] if n.endswith('_score') else n, axis='columns')\n",
    "    if len(params) == 1:\n",
    "        # Plot the test and training scores vs the grid parameter when there is only one.\n",
    "        plt.plot(df[params[0]], df['mean_train'], 'o:', label='train')\n",
    "        plt.plot(df[params[0]], df['mean_test'], 'o-', label='test')\n",
    "        plt.legend(fontsize='x-large')\n",
    "        plt.xlabel('Hyperparameter value')\n",
    "        plt.ylabel('Score $R^2$')\n",
    "        plt.ylim(max(-2, np.min(df['mean_test'])), 1)\n",
    "    return df.sort_index().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_models(X, y, max_degree=5, n_splits=3, seed=123):\n",
    "    hyper_grid = {'poly__degree': range(max_degree + 1)}\n",
    "    hyper_model = pipeline.Pipeline([\n",
    "        ('poly', preprocessing.PolynomialFeatures()),\n",
    "        ('linear', linear_model.LinearRegression(fit_intercept=True))])\n",
    "    kfold = model_selection.KFold(n_splits=n_splits)\n",
    "    cv = model_selection.GridSearchCV(hyper_model, hyper_grid, cv=kfold, return_train_score=True)\n",
    "    cv.fit(X.reshape(-1, 1), y)\n",
    "    return cv_summary(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a small dataset, a polynomial fit is very prone to overfitting and the training score continues to rise as $P$ increases.  However, only the $P=1$ and $P=2$ models look at all promising on the test data, but are still worse than guessing the average $y$ value (which always has $R^2=0$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(Xa, ya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a larger dataset, the training score is stable over a wide range of $P\\ge 1$ and the test score decreases very slowly. You could make a case for either $P=1$ (less overfitting) or $P=2$ (better test score) from the graph below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(Xb, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "© Copyright 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
