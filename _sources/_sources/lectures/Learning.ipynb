{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Overview</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no consensus on the precise definitions of data science, machine learning, deep learning and artificial intelligence. For our purposes, we consider the following definitions:\n",
    "\n",
    "* ___<span style=\"color:violet\">Data Science</span>___ (DS): a cross-disciplinary field that employs scientific approaches, processes, algorithms and systems used to extract meaning and insights from data.\n",
    "\n",
    "* ___<span style=\"color:violet\">Artificial Intelligence</span>___ (AI): a field of research aiming to develop artificial systems with human-level learning and reasoning abilities, possessing the qualities of intentionality, intelligence and adaptability. \n",
    "\n",
    "* ___<span style=\"color:violet\">Machine Learning</span>___ (ML): a subset of the field of AI which involves algorithms with the ability to learn without being explicitly programmed. These algorithms learn from data to improve their accuracy, adaptability and utility.\n",
    "\n",
    "_<span style=\"color:LightGreen\">A little ML history</span>_: Arthur Samuel is an American computer scientist who is credited for coining the term, “machine learning” with his research in computer systems at UIUC (he initiated the [ILLIAC project](https://en.wikipedia.org/wiki/ILLIAC)) then at IBM where he developed the first checkers program on IBM's first commercial computer in 1959. Robert Nealey, a self-proclaimed checkers master, played the game on an IBM 7094 computer in 1962, and he lost to the computer. Compared to what can be done today, this feat seems trivial, but it’s considered a major milestone in the field of artificial intelligence.\n",
    "\n",
    "* ___<span style=\"color:violet\">Artificial neural networks</span>___ (ANNs): comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network by that node. \n",
    "\n",
    "* ___<span style=\"color:violet\">Deep Learning</span>___ (DL): a subset of ML in which artificial neural networks adapt and learn from large datasets. The “deep” in deep learning is just referring to the number of layers in a neural network. A neural network that consists of more than three layers—which would be inclusive of the input and the output—can be considered a deep learning algorithm or a deep neural network. A neural network that only has three layers is just a basic neural network. You can think of deep learning as \"scalable machine learning\" that eliminates some of the human intervention required (through flexible frameworks) and enables the use of larger data sets to continually improve the model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below summarizes these definitions and relationships:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-AI.png\" width=800 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Types of Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/Learning-TypesOfLearning.png\" width=800 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Supervised Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Violet\">Supervised learning</span>, also known as supervised machine learning, is where machines are taught by example. It is defined by its use of labeled datasets to train algorithms to classify new data or predict outcomes accurately. As input data is fed into the model, the model adjusts its weights until it has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids overfitting or underfitting. Supervised learning helps organizations solve a variety of real-world problems at scale, such as classifying spam in a separate folder from your inbox. Some methods used in supervised learning include neural networks, naïve bayes, linear regression, logistic regression, random forest, and support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main categories of supervised learning that are mentioned below:\n",
    "\n",
    "* ___<span style=\"color:Tan\">Classification</span>___: Classification is a process of categorizing data or objects into predefined categories based on their features or attributes and determining to what category new observations belong.\n",
    "\n",
    "* ___<span style=\"color:Tan\">Regression</span>___: Regression is a process to estimate the relationships among variables when the output variable is a real or continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Advantages of Supervised Machine Learning</span>___:\n",
    "* Supervised Learning models can have high accuracy as they are trained on labelled data.\n",
    "\n",
    "* The process of decision-making in supervised learning models is often interpretable.\n",
    "\n",
    "* It can often be used in pre-trained models which saves time and resources when developing new models from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Disadvantages of Supervised Machine Learning</span>___:\n",
    "* It has limitations in knowing patterns and may struggle with unseen or unexpected patterns that are not present in the training data.\n",
    "\n",
    "* It can be time-consuming and costly as it relies on labeled data only.\n",
    "\n",
    "* It may lead to poor generalizations based on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Unsupervised Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Violet\">Unsupervised learning</span> is a machine learning technique in which an algorithm discovers patterns and relationships using unlabeled data. Unlike supervised learning, unsupervised learning doesn’t involve providing the algorithm with labeled target outputs. The primary goal of Unsupervised learning is often to discover hidden patterns, similarities, or clusters within the data, which can then be used for various purposes, such as data exploration, visualization, dimensionality reduction, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main categories of unsupervised learning that we have already studied extensively:\n",
    "\n",
    "* ___<span style=\"color:Tan\">Clustering</span>___: Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them.\n",
    "\n",
    "* ___<span style=\"color:Tan\">Dimensionality Reduction</span>___: Dimensionality reduction is a technique used to reduce the number of features in a dataset while retaining as much of the important information as possible. In other words, it is a process of transforming high-dimensional data into a lower-dimensional space that still preserves the essence of the original data. This can be done for a variety of reasons, such as to reduce the complexity of a model, to improve the performance of a learning algorithm, or to make it easier to visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Advantages of Unsupervised Machine Learning</span>___:\n",
    "* It helps to discover hidden patterns and various relationships between the data.\n",
    "\n",
    "* Used for tasks such as anomaly detection and data exploration. Techniques such as autoencoders and dimensionality reduction that can be used to extract meaningful features from raw data.\n",
    "\n",
    "* It does not require labeled data and reduces the effort of data labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Disadvantages of Unsupervised Machine Learning</span>___:\n",
    "* Without using labels, it may be difficult to predict the quality of the model’s output.\n",
    "\n",
    "* Cluster Interpretability may not be clear and may not have meaningful interpretations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Semi-Supervised Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Violet\">Semi-supervised learning</span> is a type of machine learning that falls in between supervised and unsupervised learning. It is a method that uses a small amount of labeled data and a large amount of unlabeled data to train a model. The goal of semi-supervised learning is to learn a function that can accurately predict the output variable based on the input variables, similar to supervised learning. However, unlike supervised learning, the algorithm is trained on a dataset that contains both labeled and unlabeled data. Semi-supervised learning is particularly useful when there is a large amount of unlabeled data available, but it’s too expensive or difficult to label all of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Advantages of Semi-supervised Machine Learning</span>___:\n",
    "* It leads to better generalization as compared to supervised learning, as it takes both labeled and unlabeled data.\n",
    "\n",
    "* Can be applied to a wide range of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Disdvantages of Semi-supervised Machine Learning</span>___:\n",
    "* Semi-supervised methods can be more complex to implement compared to other approaches.\n",
    "\n",
    "* It still requires some labeled data that might not always be available or easy to obtain.\n",
    "\n",
    "* The unlabeled data can impact the model performance accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Reinforcement Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Violet\">Reinforcement learning</span> is a learning method that interacts with an environment by producing actions and discovering errors. It is the science of decision making - learning the optimal behavior in an environment to obtain maximum reward. Trial, error, and delay are the most relevant characteristics of reinforcement learning. These methods allows machines to become autonomous, self-learners that automatically determine the ideal behaviour within specific context in order to maximize performance. This type of learning is crucial for applications that involve decision-making in unpredictable environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Advantages of Reinforcement Machine Learning</span>___:\n",
    "* It has autonomous decision-making that is well-suited for tasks and that can learn to make a sequence of decisions without human guidance, like robotics and game-playing. For promising science and engineering applications, it can be used for beam controls in particle acclerators, steering of high-temperature plasma in fusion systems, just to name a few. \n",
    "\n",
    "* This technique is preferred to achieve long-term results that are very difficult to achieve.\n",
    "\n",
    "* It is used to solve a complex problems that cannot be solved by conventional techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___<span style=\"color:LightBlue\">Disadvantages of Reinforcement Machine Learning</span>___:\n",
    "* Training Reinforcement Learning agents can be computationally expensive and time-consuming.\n",
    "\n",
    "* Reinforcement learning is not preferable to solving simple problems.\n",
    "\n",
    "* It needs a lot of data and a lot of computation, which makes it impractical and costly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Learning in a Probabilistic Context</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Learning a Model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have focused recently on two questions:\n",
    " - **Assuming a model, which parameters best explain my data?**\n",
    "\n",
    " - **Given competing models, what are the relative odds that they explain my data?**\n",
    "\n",
    "In the framework of Bayesian inference, we answer the first question by estimating the posterior\n",
    "\n",
    "$$ \\Large\n",
    "P(\\Theta_M\\mid D, M) = \\frac{P(D\\mid \\Theta_M, M)\\, P(\\Theta_M, M)}{P(D\\mid M)}\n",
    "$$\n",
    "\n",
    "for the assumed model $M$ and its parameters $\\Theta_M$.  We answer the second question by estimating the posterior ratio:\n",
    "\n",
    "$$ \\Large\n",
    "\\frac{P(M_1\\mid D)}{P(M_2\\mid D)} = \\frac{P(D\\mid M_1)\\, P(M_1)}{P(D\\mid M_2)\\, P(M_2)} \\; .\n",
    "$$\n",
    "\n",
    "In either case, the fundamental object is the joint probability,\n",
    "\n",
    "$$ \\Large\n",
    "P(D, \\Theta_M, M),\n",
    "$$\n",
    "\n",
    "or, with competing models, $M_1$ and $M_2$, the pair of joint probabilities,\n",
    "\n",
    "$$ \\Large\n",
    "P(D, \\Theta_{M_1}, M_1) \\quad , \\quad\n",
    "P(D, \\Theta_{M_2}, M_2) \\; .\n",
    "$$\n",
    "\n",
    "These are the fundamental objects since any conditional or marginalized probability can be derived from them.  Note that the observed random variables $D$ are given, but the unobserved (latent) random variables $\\Theta_M$ require that we make a choice of model(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Unsupervised Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions above concern models and their parameters.  However, we can also ask questions about \"new\" data, where \"new\" means either not yet observed or else already observed but omitted from our analysis.  For example:\n",
    " - ___<span style=\"color:Violet\">KEY QUESTION</span>___: **Given observed data $D$, how likely is unobserved data $D'$?**\n",
    "\n",
    "This is the fundamental question of <span style=\"color:Violet\">unsupervised learning</span>, and can be framed in probabilistic language starting from the joint probability\n",
    "\n",
    "$$ \\Large\n",
    "P(D, D', \\Theta_M, M)\n",
    "$$\n",
    "\n",
    "assuming the model $M$ with parameters $\\Theta_M$. \n",
    "\n",
    "To answer this question, we must estimate:\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{aligned}\n",
    "P(D'\\mid D, M) &= \\int d\\Theta_M\\, P(D',\\Theta_M\\mid D, M) \\\\\n",
    "&= \\int d\\Theta_M\\, P(D'\\mid D,\\Theta_M, M)\\,P(\\Theta_M\\mid D, M)\\\\\n",
    "&= \\int d\\Theta_M\\, \\frac{P(D',D\\mid \\Theta_M, M)}{P(D\\mid \\Theta_M, M)}\\, P(\\Theta_M\\mid D, M) \\;. \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we assume that $D$ and $D'$ are statistically independent datasets, then\n",
    "\n",
    "$$ \\Large\n",
    "P(D',D\\mid \\Theta_M, M) = P(D'\\mid\\Theta_M, M)\\, P(D\\mid \\Theta_M, M) \\; ,\n",
    "$$\n",
    "\n",
    "and we can simplify\n",
    "\n",
    "$$ \\Large\n",
    "P(D'\\mid D, M) = \\int d\\Theta_M\\, P(D'\\mid\\Theta_M, M)\\, P(\\Theta_M\\mid D, M) \\; .\n",
    "$$\n",
    "\n",
    "Note that in order to evaluate the RHS, we must have already learned the model $M$ and determined the posterior $P(\\Theta_M\\mid D, M)$ of its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Supervised Learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we split the features of our data $D$ into two categories, $X$ and $Y$, we can ask a new question about unobserved data $D'$ that splits into $X'$ and $Y'$:\n",
    "\n",
    " - ___<span style=\"color:Violet\">KEY QUESTION</span>___: **Given observed data $(X,Y)$ from $D$ and $X'$ from $D'$, how likely is the remaining unobserved data $Y'$?**\n",
    " \n",
    "This is the fundamental question of <span style=\"color:Violet\">supervised learning</span>, and the relevant joint probability is now\n",
    "\n",
    "$$ \\Large\n",
    "P(D, D', \\Theta_M, M) = P(X, Y, X', Y', \\Theta_M, M)\n",
    "$$\n",
    "\n",
    "for the assumed model $M$ with parameters $\\Theta_M$. \n",
    "\n",
    "To answer this question, we can estimate:\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{aligned}\n",
    "P(Y'\\mid X, Y, X', M) &= \\int d\\Theta_M\\, ~P(Y',\\Theta_M\\mid X, Y, X', M) \\\\\n",
    "&= \\int d\\Theta_M\\, ~P(Y'\\mid X, Y, X',\\Theta_M, M)\\, ~P(\\Theta_M\\mid X, Y, X', M) \\\\\n",
    "&= \\int d\\Theta_M\\, \\frac{P(X, Y, X', Y'\\mid \\Theta_M, M)}{P(X, Y, X'\\mid \\Theta_M, M)}\\,\n",
    "~P(\\Theta_M\\mid X, Y, X', M) \\; .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we again assume that $D$ and $D'$ are statistically independent, then\n",
    "\n",
    "$$ \\Large\n",
    "P(X,Y,X',Y'\\mid\\Theta_M,M) = P(X',Y'\\mid\\Theta_M,M)\\, ~P(X,Y\\mid\\Theta_M,M) \\; ,\n",
    "$$\n",
    "\n",
    "and (after integrating out $X'$),\n",
    "\n",
    "$$ \\Large\n",
    "P(X,Y,Y'\\mid\\Theta_M,M) = P(Y'\\mid\\Theta_M,M)\\, ~P(X,Y\\mid\\Theta_M,M) \\; .\n",
    "$$\n",
    "\n",
    "We can then simplify:\n",
    "\n",
    "$$ \\Large\n",
    "P(Y'\\mid X, Y, X', M) = \\int d\\Theta_M\\,\n",
    "\\frac{P(X', Y'\\mid \\Theta_M, M)}{P(X'\\mid \\Theta_M, M)}\\, ~P(\\Theta_M\\mid X, Y, X', M) \\; .\n",
    "$$\n",
    "\n",
    "Note that this formulation of the problem has $P(\\Theta_M\\mid X, Y, X', M)$ on the RHS, which indicates that we must re-learn the model $M$ each time we are given new data $X'$.\n",
    "\n",
    "An alternative formulation reveals that, while valid, this is not necessary: start from the unsupervised result above, with $D=(X,Y)$ and $D'=(X',Y')$,\n",
    "\n",
    "$$ \\Large\n",
    "P(X',Y'\\mid X,Y, M) = \\int d\\Theta_M\\, ~P(X',Y'\\mid\\Theta_M, M)\\, ~P(\\Theta_M\\mid X,Y, M)\n",
    "$$\n",
    "\n",
    "then integrate out $X'$,\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{aligned}\n",
    "P(X'\\mid X,Y, M) &= \\int d\\Theta_M\\, \\left[ \\int dX'\\, ~P(X',Y'\\mid\\Theta_M, M)\\right] \n",
    "~P(\\Theta_M\\mid X,Y, M) \\\\\n",
    "&= \\int d\\Theta_M\\, ~P(Y'\\mid\\Theta_M, M)\\, ~P(\\Theta_M\\mid X,Y, M) \\; .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can now answer our original question using\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{aligned}\n",
    "P(Y'\\mid X,Y,X',M) &= \\frac{P(P(X',Y'\\mid X,Y, M)}{P(X'\\mid X,Y, M)} \\\\\n",
    "&= \\frac{\\int d\\Theta_M\\, ~P(X',Y'\\mid\\Theta_M, M)\\, ~P(\\Theta_M\\mid X,Y, M)}\n",
    "{\\int d\\Theta_M\\, ~P(Y'\\mid\\Theta_M, M)\\, ~P(\\Theta_M\\mid X,Y, M)} \\; .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note how this formulation allows us to learn the model $M$ once with the original data $D=(X,Y)$ but requires two separate marginalizations (integrals) over the model parameters $\\Theta_M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data $D$ used to learn the model used in unsupervised or supervised learning is referred as the <span style=\"color:Violet\">training data</span>.  In supervised learning, the features appearing in $X$ are the <span style=\"color:Violet\">input features</span> and $Y$ are referred to as the <span style=\"color:Violet\">target features</span>.\n",
    "\n",
    "We use different terminology (and approaches to modeling) for supervised learning depending on the type of target features we wish to learn:\n",
    " - <span style=\"color:Tan\">regression</span> $~~$ : predict continuous-value target features.\n",
    " - <span style=\"color:Tan\">classification</span>: predict discrete-valued target features.\n",
    "\n",
    "Note that the target features might be a mix of continuous and discrete features, so this terminology is incomplete.\n",
    "\n",
    "Most of the high-profile machine learning applications from Google, Facebook, etc, involve classification rather than regression, so proportionally more effort has gone into developing and optimizing classification algorithms. However, most scientific applications are more naturally expressed as regression problems: this presents both a challenge and an opportunity to the scientific ML community!  Also note that a regression problem can always be converted into a classification problem by binning the output (assuming you don't need infinite accuracy), which can be surprisingly effective.\n",
    "\n",
    "All unsupervised and supervised learning algorithms involve priors, $P(\\Theta_M\\mid M)$ but they are not always stated explicitly. Sometimes priors are expressed implicitly via terms that are referred to as <span style=\"color:Violet\">regularization conditions</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">Model Selection Revisited</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning a model $M$, our quantitative measure of how well it explains the data $D$ is the evidence $P(D\\mid M)$.\n",
    "\n",
    "Since unsupervised and supervised learning requires first learning the model, we could use the same measure, but a different measure is also useful: how well does $M$ explain unobserved data $D'$?  In other words, how well does the learned model generalize and offer predictive power?  In order to answer this question, in practice, you must hold back some of your observed data when learning the model and can then measure how well the model \"postdicts\" the held back data. The held back data is referred to as the <span style=\"color:Violet\">test sample</span> and this process is known as <span style=\"color:Violet\">cross validation</span>.\n",
    "\n",
    "In cases where the model and its parameters have some physical reality (for example, projectile motion modeled with Newtonian physics and parameterized by $g$, ...), these measures are essentially the same since the model is dictated by some independent reality and not chosen specifically to explain the observed data $D$.\n",
    "\n",
    "However, when predicting future data is the main goal and there is no first-principles model available, the model and its parameters are essentially unconstrained and these two measures can easily diverge.  In particular, optimizing how well the model explains the observed data leads to <span style=\"color:Violet\">over-fitting</span> and poor ability to <span style=\"color:Violet\">generalize</span> to new data.\n",
    "\n",
    "For example, suppose the observed data $D$ consists of $N$ samples $x_i$ of a single feature $x$, then a model $M$ with the likelihood ($\\delta_D$ is the Dirac delta function)\n",
    "\n",
    "$$ \\Large\n",
    "P(x\\mid \\Theta_M, M) = \\frac{1}{N}\\, \\sum_{i=1}^N \\delta_D(x - x_i)\n",
    "$$\n",
    "\n",
    "trivially explains the data perfectly with the parameters\n",
    "\n",
    "$$ \\Large\n",
    "\\Theta_M = \\{ x_1, x_2, \\ldots, x_N \\} \\; .\n",
    "$$\n",
    "\n",
    "This purely empirical approach to model building is an extreme case of over-fitting and offers no generalization power. (Note that the likelihood above is a kernel density estimate with a Dirac delta function kernel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "© Copyright 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
