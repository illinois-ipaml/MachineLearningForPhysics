{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "#ensure that the PyTorch and the PyG are the same version\n",
    "!pip install pyvis\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "import torch_geometric\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Orange\">Introduction to Graph Data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs can be seen as a system or language for modeling systems that are complex and linked together.\n",
    "A graph is a data type that is modelled as a set of objects which can be represented as a node or vertex and their relationships which is called edges. A graph data can also be seen as a network data where there are points connected together. \n",
    "\n",
    "A <span style=\"color:Violet\">node</span> (vertex) of a graph is point in a graph while an <span style=\"color:Violet\">edge</span> is a component that joins edges together in a graph. Graphs can be used to represent data from a lot of domains like biology, physics, social science, chemistry and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/GraphNeuralNetworks-GraphExample.jpeg\" width=800 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Orange\">Graph Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs can be classified into different categories <span style=\"color:Violet\">directed/undirected graphs</span>, <span style=\"color:Violet\">weighted/binary graphs</span> and <span style=\"color:Violet\">homogenous/heterogenous graphs</span>.\n",
    "\n",
    "- <span style=\"color:Violet\">Directed/Undirected graphs</span>: Directed graphs are the ones that all the edges have directtions while in undirected graphs, the edges are does not have directions\n",
    "\n",
    "- <span style=\"color:Violet\">Weighted/Binary graphs</span>: Weighted graphs is a type of graph that each of the edges are assigned with a value while binary graphs are the ones that the edges does not have an assigned value.\n",
    "\n",
    "- <span style=\"color:Violet\">Homogenous/Heterogenous graphs</span>: Homogenous graphs are the ones that all the nodes and/or edges are of the same type (e.g. friendship graph) while heterogenous graphs are graphs where the nodes and/or edges are of different types (e.g. knowledge graph).\n",
    "\n",
    "Traditional graph analysis methods requires using searching algorithms, clustering spanning tree algorithms and so on. A major downside to using these methods for analysis of graph data is that you require a prior knowledge of the graph to be able to apply these algorithms.\n",
    "\n",
    "Based on the structure of the graph data, traditional machine learning system will not be able to properly interprete the graph data and thus the advent of the <span style=\"color:Violet\">Graph Neural Network</span> (GNN). Graph neural network is a domain of deep learning that is mostly concerned with deep learning operations on graph datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Orange\">Exploring graph data with Pytorch Geometric</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore graph and graph neural networks using the PyTorch Geometric (PyG) package (already loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that will help us visualize a graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyG provides access to a couple of graph datasets. For this notebook, we are going to use [Zachary's karate club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) network [dataset](https://karateclub.readthedocs.io/en/latest/modules/dataset.html). \n",
    "\n",
    "Zachary's karate club is a great example of social relationships within a small group. This set of data indicated the interactions of club members outside of the club. This dataset also documented the conflict between the instructor, Mr.Hi, and the club president, John because of course price. In the end, half of the members formed a new club around Mr.Hi, and the other half either stayed at the old karate club or gave up karate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import KarateClub\n",
    "\n",
    "dataset = KarateClub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported a graph dataset, let's look at some of the properties of a graph dataset. We will look at some of the properties at the level of the dataset and then select a graph in the dataset to explore it's properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset properties')\n",
    "print('==============================================================')\n",
    "print(f'Dataset: {dataset}') #This prints the name of the dataset\n",
    "print(f'Number of graphs in the dataset: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}') #Number of features each node in the dataset has\n",
    "print(f'Number of classes: {dataset.num_classes}') #Number of classes that a node can be classified into\n",
    "\n",
    "# Since we have one graph in the dataset, we will select the graph and explore it's properties\n",
    "data = dataset[0]\n",
    "print('Graph properties')\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}') #Number of nodes in the graph\n",
    "print(f'Number of edges: {data.num_edges}') #Number of edges in the graph\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}') # Average number of nodes in the graph\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}') #Does the graph contains nodes that are not connected\n",
    "print(f'Contains self-loops: {data.has_self_loops()}') #Does the graph contains nodes that are linked to themselves\n",
    "print(f'Is undirected: {data.is_undirected()}') #Is the graph an undirected graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the graph using the function that we created earlier. But first, we will convert the graph to `networkx` graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize graph is using `pyvis`. You may download (if on Google Colab) and open the generated `nx.html` file from your local browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(G.nodes)):\n",
    "    G.nodes[i]['group']=int(data.y[i]) # assign the class label to the node. class label has to be int.\n",
    "from pyvis.network import Network\n",
    "vis= Network(notebook=True,cdn_resources='remote')\n",
    "vis.barnes_hut(spring_length=25, spring_strength=0.5, damping=0.6)\n",
    "vis.from_nx(G)\n",
    "vis.show_buttons(filter_=[\"physics\"])\n",
    "vis.save_graph('nx.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this file locally, you can directly call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!open nx.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Implementing a Graph Neural Network</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will use a simple GNN which is the [Graph Convolution Network](https://towardsdatascience.com/graph-convolutional-networks-introduction-to-gnns-24b3f60d6c95) (GCN) layer. \n",
    "\n",
    "- GCN is a specific type of GNN that uses convolutional operations to propagate information between nodes in a graph.\n",
    "\n",
    "- GCNs leverage a localized aggregation of neighboring node features to update the representations of the nodes.\n",
    "\n",
    "- GCNs are based on the convolutional operation commonly used in image processing, adapted to the graph domain.\n",
    "\n",
    "- The layers in a GCN typically apply a graph convolution operation followed by non-linear activation functions.\n",
    "\n",
    "- GCNs have been successful in tasks such as node classification, where nodes are labeled based on their features and graph structure.\n",
    "\n",
    "Our GNN is defined by stacking three graph convolution layers, which corresponds to aggregating 3-hop neighborhood information around each node (all nodes up to 3 \"hops\" away). In addition, the GCNConv layers reduce the node feature dimensionality to 2, i.e., `34→4→4→2`. Each GCNConv layer is enhanced by a `tanh` non-linearity. We then apply a linear layer which acts as a classifier to map the nodes to 1 out of the 4 possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.classifier = Linear(2, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()  # Final GNN embedding space.\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h\n",
    "\n",
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Training the model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network, we will use the <span style=\"color:Violet\">CrossEntropyLoss</span> for the loss function and <span style=\"color:Violet\">Adam</span> as the gradient optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "criterion = torch.nn.CrossEntropyLoss()  #Initialize the CrossEntropyLoss function.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Initialize the Adam optimizer.\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss, h\n",
    "\n",
    "for epoch in range(401):\n",
    "    loss, h = train(data)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss}')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is much more analysis that can be done with GNNs on the Karate Club dataseet. See [here](https://www.linkedin.com/pulse/community-detection-social-networks-case-study-zacharys-marin/) for examples. Our goal in this lecture was to use it an way to introduce the implementation and use of GNNs within the Pytorch framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now turn to examples using simulated open data from the CMS experiment at the LHC.  The algorithms' inputs are features of the reconstructed charged particles in a jet and the secondary vertices associated with them. Describing the jet shower as a combination of particle-to-particle and particle-to-vertex interactions, the model is trained to learn a jet representation on which the classification problem isoptimized.\n",
    "\n",
    "We show below an example using a community model called \"DeepSets\" and then compare to an Interaction Model GNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Orange\">Deep Sets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at Deep Sets networks using PyTorch. The architecture is based on the following paper: [DeepSets](https://papers.nips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "\n",
    "!pip install -U PyYAML\n",
    "!pip install uproot\n",
    "!pip install awkward\n",
    "!pip install mplhep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# WGET for colab\n",
    "if not os.path.exists(\"definitions_lorentz.yml\"):\n",
    "    url = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/definitions_lorentz.yml\"\n",
    "    definitionsFile = wget.download(url)\n",
    "\n",
    "with open(\"definitions_lorentz.yml\") as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    definitions = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "features = definitions[\"features\"]\n",
    "spectators = definitions[\"spectators\"]\n",
    "labels = definitions[\"labels\"]\n",
    "\n",
    "nfeatures = definitions[\"nfeatures\"]\n",
    "nspectators = definitions[\"nspectators\"]\n",
    "nlabels = definitions[\"nlabels\"]\n",
    "ntracks = definitions[\"ntracks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Data Loader</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to define the dataset loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in colab\n",
    "if not os.path.exists(\"GraphDataset.py\"):\n",
    "    urlDSD = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/DeepSetsDataset.py\"\n",
    "    DSD = wget.download(urlDSD)\n",
    "if not os.path.exists(\"utils.py\"):\n",
    "    urlUtils = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/utils.py\"\n",
    "    utils = wget.download(urlUtils)\n",
    "\n",
    "from DeepSetsDataset import DeepSetsDataset\n",
    "\n",
    "# For colab\n",
    "import os.path\n",
    "\n",
    "if not os.path.exists(\"ntuple_merged_90.root\"):\n",
    "    urlFILE = \"http://opendata.cern.ch/eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/train/ntuple_merged_90.root\"\n",
    "    dataFILE = wget.download(urlFILE)\n",
    "train_files = [\"ntuple_merged_90.root\"]\n",
    "\n",
    "train_generator = DeepSetsDataset(\n",
    "    features,\n",
    "    labels,\n",
    "    spectators,\n",
    "    start_event=0,\n",
    "    stop_event=10000,\n",
    "    npad=ntracks,\n",
    "    file_names=train_files,\n",
    ")\n",
    "train_generator.process()\n",
    "\n",
    "test_generator = DeepSetsDataset(\n",
    "    features,\n",
    "    labels,\n",
    "    spectators,\n",
    "    start_event=10001,\n",
    "    stop_event=14001,\n",
    "    npad=ntracks,\n",
    "    file_names=train_files,\n",
    ")\n",
    "test_generator.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Deep Sets Network</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Sets models are designed to be explicitly permutation invariant. At their core they are composed of two networks, $\\phi$ and $\\rho$, such that the total network $f$ is given by\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{align}\n",
    "    f &= \\rho\\left(\\Sigma_{\\mathbf{x}_i\\in\\mathcal{X}} ~ \\phi(\\mathbf{x}_i)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_i$ are the features for the $i$-th element in the input sequence $\\mathcal{X}$.\n",
    "\n",
    "We will define a DeepSets model that will take as input up to 60 of the tracks (with 48 features) with zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import (\n",
    "    Sequential as Seq,\n",
    "    Linear as Lin,\n",
    "    ReLU,\n",
    "    BatchNorm1d,\n",
    "    AvgPool1d,\n",
    "    Sigmoid,\n",
    "    Conv1d,\n",
    ")\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "# ntracks = 60\n",
    "inputs = 6\n",
    "hidden1 = 64\n",
    "hidden2 = 32\n",
    "hidden3 = 16\n",
    "classify1 = 50\n",
    "outputs = 2\n",
    "\n",
    "class DeepSets(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepSets, self).__init__()\n",
    "        self.phi = Seq(\n",
    "            Conv1d(inputs, hidden1, 1),\n",
    "            BatchNorm1d(hidden1),\n",
    "            ReLU(),\n",
    "            Conv1d(hidden1, hidden2, 1),\n",
    "            BatchNorm1d(hidden2),\n",
    "            ReLU(),\n",
    "            Conv1d(hidden2, hidden3, 1),\n",
    "            BatchNorm1d(hidden3),\n",
    "            ReLU(),\n",
    "        )\n",
    "        self.rho = Seq(\n",
    "            Lin(hidden3, classify1),\n",
    "            BatchNorm1d(classify1),\n",
    "            ReLU(),\n",
    "            Lin(classify1, outputs),\n",
    "            Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.phi(x)\n",
    "        out = scatter_mean(out, torch.LongTensor(np.zeros(ntracks)), dim=-1)\n",
    "        return self.rho(torch.squeeze(out))\n",
    "\n",
    "\n",
    "model = DeepSets()\n",
    "print(model)\n",
    "print(\"----------\")\n",
    "print({l: model.state_dict()[l].shape for l in model.state_dict()})\n",
    "\n",
    "model = DeepSets().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Define the training loop</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, loader, total, batch_size, leave=False):\n",
    "    model.eval()\n",
    "\n",
    "    xentropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total / batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        y = torch.argmax(y, dim=1)\n",
    "        batch_output = model(x)\n",
    "        batch_loss_item = xentropy(batch_output, y).item()\n",
    "        sum_loss += batch_loss_item\n",
    "        t.set_description(\"loss = %.5f\" % (batch_loss_item))\n",
    "        t.refresh()  # to show immediately the update\n",
    "\n",
    "    return sum_loss / (i + 1)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader, total, batch_size, leave=False):\n",
    "    model.train()\n",
    "\n",
    "    xentropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total / batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "        x = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        y = torch.argmax(y, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = model(x)\n",
    "        batch_loss = xentropy(batch_output, y)\n",
    "        batch_loss.backward()\n",
    "        batch_loss_item = batch_loss.item()\n",
    "        t.set_description(\"loss = %.5f\" % batch_loss_item)\n",
    "        t.refresh()  # to show immediately the update\n",
    "        sum_loss += batch_loss_item\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Define training, validation, testing data generators</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_generator_data = ConcatDataset(train_generator.datas)\n",
    "test_generator_data = ConcatDataset(test_generator.datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n",
    "valid_frac = 0.20\n",
    "train_length = len(train_generator_data)\n",
    "valid_num = int(valid_frac * train_length)\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    train_generator_data, [train_length - valid_num, valid_num]\n",
    ")\n",
    "\n",
    "\n",
    "def collate(items):\n",
    "    l = sum(items, [])\n",
    "    return Batch.from_data_list(l)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# train_loader.collate_fn = collate\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "# valid_loader.collate_fn = collate\n",
    "test_loader = DataLoader(test_generator_data, batch_size=batch_size, shuffle=False)\n",
    "# test_loader.collate_fn = collate\n",
    "\n",
    "train_samples = len(train_dataset)\n",
    "valid_samples = len(valid_dataset)\n",
    "test_samples = len(test_generator_data)\n",
    "print(train_length)\n",
    "print(train_samples)\n",
    "print(valid_samples)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Train</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "n_epochs = 30\n",
    "stale_epochs = 0\n",
    "best_valid_loss = 99999\n",
    "patience = 5\n",
    "t = tqdm(range(0, n_epochs))\n",
    "\n",
    "for epoch in t:\n",
    "    loss = train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        train_samples,\n",
    "        batch_size,\n",
    "        leave=bool(epoch == n_epochs - 1),\n",
    "    )\n",
    "    valid_loss = test(\n",
    "        model,\n",
    "        valid_loader,\n",
    "        valid_samples,\n",
    "        batch_size,\n",
    "        leave=bool(epoch == n_epochs - 1),\n",
    "    )\n",
    "    print(\"Epoch: {:02d}, Training Loss:   {:.4f}\".format(epoch, loss))\n",
    "    print(\"           Validation Loss: {:.4f}\".format(valid_loss))\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        modpath = osp.join(\"deepsets_best.pth\")\n",
    "        print(\"New best model saved to:\", modpath)\n",
    "        torch.save(model.state_dict(), modpath)\n",
    "        stale_epochs = 0\n",
    "    else:\n",
    "        print(\"Stale epoch\")\n",
    "        stale_epochs += 1\n",
    "    if stale_epochs >= patience:\n",
    "        print(\"Early stopping after %i stale epochs\" % patience)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Evaluate on Test Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you need to load the model from a pth file\n",
    "# Trained on 4 vectors (as above in notebook)\n",
    "# urlPTH = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/deepsets_best_4vec.pth\"\n",
    "# pthFile = wget.download(urlPTH)\n",
    "# model.load_state_dict(torch.load(\"deepsets_best_4vec.pth\"))\n",
    "# Trained on all possible inputs (a different configuration)\n",
    "# urlPTH = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/deepsets_best_AllTraining.pth\"\n",
    "# pthFile = wget.download(urlPTH)\n",
    "# model.load_state_dict(torch.load(\"deepsets_best_AllTraining.pth\"))\n",
    "\n",
    "model.eval()\n",
    "t = tqdm(enumerate(test_loader), total=test_samples / batch_size)\n",
    "y_test = []\n",
    "y_predict = []\n",
    "track_pt = []\n",
    "for i, data in t:\n",
    "    x = data[0].to(device)\n",
    "    y = data[1].to(device)\n",
    "    track_pt.append(x[:, 0, 0].numpy())\n",
    "    batch_output = model(x)\n",
    "    y_predict.append(batch_output.detach().cpu().numpy())\n",
    "    y_test.append(y.cpu().numpy())\n",
    "track_pt = np.concatenate(track_pt)\n",
    "y_test = np.concatenate(y_test)\n",
    "y_predict = np.concatenate(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bins, _ = plt.hist(track_pt[y_test[:, 1] == 1], bins=50, label=\"sig\", histtype=\"step\")\n",
    "_, bins, _ = plt.hist(track_pt[y_test[:, 1] == 0], bins=bins, label=\"bkg\", histtype=\"step\")\n",
    "plt.legend()\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import mplhep as hep\n",
    "\n",
    "plt.style.use(hep.style.ROOT)\n",
    "# create ROC curves\n",
    "fpr_deepset, tpr_deepset, threshold_deepset = roc_curve(y_test[:, 1], y_predict[:, 1])\n",
    "with open(\"deepset_roc.npy\", \"wb\") as f:\n",
    "    np.save(f, fpr_deepset)\n",
    "    np.save(f, tpr_deepset)\n",
    "    np.save(f, threshold_deepset)\n",
    "\n",
    "# plot ROC curves\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    tpr_deepset,\n",
    "    fpr_deepset,\n",
    "    lw=2.5,\n",
    "    label=\"DeepSet, AUC = {:.1f}%\".format(auc(fpr_deepset, tpr_deepset) * 100),\n",
    ")\n",
    "plt.xlabel(r\"True positive rate\")\n",
    "plt.ylabel(r\"False positive rate\")\n",
    "plt.ylim(0.001, 1)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightGreen\">AUC and ROC Curves</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand this curve, let define the <span style=\"color:Violet\">confusion matrix</span>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/GraphNeuralNetworks-ConfusionMatrix.png\" width=400 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Violet\">True Positive Rate</span> (TPR) is a synonym for recall/sensitivity and is therefore defined as follows:\n",
    "\n",
    "$$ \\Large\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "TPR tells us what proportion of the positive class got correctly classified. A simple example would be determining what proportion of the actual sick people were correctly detected by the model.\n",
    "\n",
    "<span style=\"color:Violet\">False Negative Rate</span> (FNR) is defined as follows:\n",
    "\n",
    "$$ \\Large\n",
    "FNR = \\frac{FN}{TP + FN}\n",
    "$$\n",
    "\n",
    "FNR tells us what proportion of the positive class got incorrectly classified by the classifier. A higher TPR and a lower FNR are desirable since we want to classify the positive class correctly.\n",
    "\n",
    "<span style=\"color:Violet\">True Negative Rate</span> (TNR) is a synonym for specificity and is defined as follows:\n",
    "\n",
    "$$ \\Large\n",
    "TNR = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "Specificity tells us what proportion of the negative class got correctly classified. Taking the same example as in Sensitivity, Specificity would mean determining the proportion of healthy people who were correctly identified by the model.\n",
    "\n",
    "<span style=\"color:Violet\">False Positive Rate</span> (FPR) is defined as follows:\n",
    "\n",
    "$$ \\Large\n",
    "TPR = \\frac{FP}{TN + FP}\n",
    "$$\n",
    "\n",
    "FPR tells us what proportion of the negative class got incorrectly classified by the classifier. A higher TNR and a lower FPR are desirable since we want to classify the negative class correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <span style=\"color:Violet\">ROC curve</span> (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "\n",
    "- True Positive Rate\n",
    "\n",
    "- False Positive Rate\n",
    "\n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/GraphNeuralNetworks-ROC.png\" width=400 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Violet\">AUC</span> stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). AUC provides an aggregate measure of performance across all possible classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/img/GraphNeuralNetworks-AUC.png\" width=400 align=left></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Orange\">Interaction Network</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at graph neural networks using the PyTorch Geometric library: <https://pytorch-geometric.readthedocs.io/>. See {cite:p}`PyTorchGeometric` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os.path\n",
    "\n",
    "# WGET for colab\n",
    "if not os.path.exists(\"definitions.yml\"):\n",
    "    url = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/definitions.yml\"\n",
    "    definitionsFile = wget.download(url)\n",
    "\n",
    "with open(\"definitions.yml\") as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    definitions = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# You can test with using only 4-vectors by using:\n",
    "# if not os.path.exists(\"definitions_lorentz.yml\"):\n",
    "#    url = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/definitions_lorentz.yml\"\n",
    "#    definitionsFile = wget.download(url)\n",
    "# with open('definitions_lorentz.yml') as file:\n",
    "#    # The FullLoader parameter handles the conversion from YAML\n",
    "#    # scalar values to Python the dictionary format\n",
    "#    definitions = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "features = definitions[\"features\"]\n",
    "spectators = definitions[\"spectators\"]\n",
    "labels = definitions[\"labels\"]\n",
    "\n",
    "nfeatures = definitions[\"nfeatures\"]\n",
    "nspectators = definitions[\"nspectators\"]\n",
    "nlabels = definitions[\"nlabels\"]\n",
    "ntracks = definitions[\"ntracks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Graph Datasets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to define the graph dataset. We do this in a separate class following this example: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_dataset.html#creating-larger-datasets\n",
    "\n",
    "Formally, a graph is represented by a triplet $\\mathcal G = (\\mathbf{u}, V, E)$, consisting of a graph-level, or *global*, feature vector $\\mathbf{u}$, a set of $N^v$ nodes $V$, and a set of $N^e$ edges $E$.\n",
    "The nodes are given by $V = \\{\\mathbf{v}_i\\}_{i=1:N^v}$, where $\\mathbf{v}_i$ represents the $i$th node's attributes.\n",
    "The edges connect pairs of nodes and are given by $E = \\{\\left(\\mathbf{e}_k, r_k, s_k\\right)\\}_{k=1:N^e}$, where $\\mathbf{e}_k$ represents the $k$th edge's attributes, and $r_k$ and $s_k$ are the indices of the \"receiver\" and \n",
    "\"sender\" nodes, respectively, connected by the $k$th edge (from the sender node to the receiver node).\n",
    "The receiver and sender index vectors are an alternative way of encoding the directed adjacency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jmduarte/capstone-particle-physics-domain/raw/master/weeks/attributes.png\" alt=\"attributes\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in colab\n",
    "if not os.path.exists(\"GraphDataset.py\"):\n",
    "    urlDSD = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/GraphDataset.py\"\n",
    "    DSD = wget.download(urlDSD)\n",
    "if not os.path.exists(\"utils.py\"):\n",
    "    urlUtils = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/utils.py\"\n",
    "    utils = wget.download(urlUtils)\n",
    "\n",
    "from GraphDataset import GraphDataset\n",
    "\n",
    "# For Colab\n",
    "if not os.path.exists(\"ntuple_merged_90.root\"):\n",
    "    urlFILE = \"http://opendata.cern.ch/eos/opendata/cms/datascience/HiggsToBBNtupleProducerTool/HiggsToBBNTuple_HiggsToBB_QCD_RunII_13TeV_MC/train/ntuple_merged_90.root\"\n",
    "    dataFILE = wget.download(urlFILE)\n",
    "file_names = [\"ntuple_merged_90.root\"]\n",
    "\n",
    "graph_dataset = GraphDataset(\n",
    "    \"gdata_train\",\n",
    "    features,\n",
    "    labels,\n",
    "    spectators,\n",
    "    start_event=0,\n",
    "    stop_event=8000,\n",
    "    n_events_merge=1,\n",
    "    file_names=file_names,\n",
    ")\n",
    "\n",
    "test_dataset = GraphDataset(\n",
    "    \"gdata_test\",\n",
    "    features,\n",
    "    labels,\n",
    "    spectators,\n",
    "    start_event=8001,\n",
    "    stop_event=10001,\n",
    "    n_events_merge=1,\n",
    "    file_names=file_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Graph Neural Network</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we recapitulate the \"graph network\" (GN) formalism described in this [paper](https://arxiv.org/abs/1612.00222), which generalizes various GNNs and other similar methods.\n",
    "\n",
    "GNs are graph-to-graph mappings, whose output graphs have the same node and edge structure as the input. Formally, a GN block contains three \"update\" functions, $\\phi$, and three \"aggregation\" functions, $\\rho$. The stages of processing in a single GN block are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{e}'_k &= \\phi^e\\left(\\mathbf{e}_k, \\mathbf{v}_{r_k}, \\mathbf{v}_{s_k}, \\mathbf{u} \\right) & \\mathbf{\\bar{e}}'_i &= \\rho^{e \\rightarrow v}\\left(E'_i\\right) & \\text{(Edge block),} \\\\\n",
    "    \\mathbf{v}'_i &= \\phi^v\\left(\\mathbf{\\bar{e}}'_i, \\mathbf{v}_i, \\mathbf{u}\\right) & \n",
    "    \\mathbf{\\bar{e}}' &= \\rho^{e \\rightarrow u}\\left(E'\\right) &  \\text{(Node block),} \\\\\n",
    "    \\mathbf{u}' &= \\phi^u\\left(\\mathbf{\\bar{e}}', \\mathbf{\\bar{v}}', \\mathbf{u}\\right) & \n",
    "    \\mathbf{\\bar{v}}' &= \\rho^{v \\rightarrow u}\\left(V'\\right) &\\text{(Global block).}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $E'_i = \\left\\{\\left(\\mathbf{e}'_k, r_k, s_k \\right)\\right\\}_{r_k=i,\\; k=1:N^e}$ contains the updated edge features for edges whose receiver node is the $i^\\text{th}$ node, $E' = \\bigcup_i E_i' = \\left\\{\\left(\\mathbf{e}'_k, r_k, s_k \\right)\\right\\}_{k=1:N^e}$ is the set of updated edges, and $V'=\\left\\{\\mathbf{v}'_i\\right\\}_{i=1:N^v}$ is the set of updated nodes.\n",
    "\n",
    "<img src=\"https://github.com/jmduarte/capstone-particle-physics-domain/raw/master/weeks/GN-full-block.png\" alt=\"GN full block\" width=\"1000\">\n",
    "\n",
    "We will define an interaction network model similar to this [paper](https://arxiv.org/abs/1909.12285), but just modeling the particle-particle interactions. It will take as input all of the tracks (with 48 features) without truncating or zero-padding. Another modification is the use of batch normalization {cite:p}`bn` layers to improve the stability of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import EdgeConv, global_mean_pool\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, BatchNorm1d\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "\n",
    "inputs = 48\n",
    "hidden = 128\n",
    "outputs = 2\n",
    "\n",
    "class EdgeBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeBlock, self).__init__()\n",
    "        self.edge_mlp = Seq(\n",
    "            Lin(inputs * 2, hidden), BatchNorm1d(hidden), ReLU(), Lin(hidden, hidden)\n",
    "        )\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        out = torch.cat([src, dest], 1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "\n",
    "class NodeBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NodeBlock, self).__init__()\n",
    "        self.node_mlp_1 = Seq(\n",
    "            Lin(inputs + hidden, hidden),\n",
    "            BatchNorm1d(hidden),\n",
    "            ReLU(),\n",
    "            Lin(hidden, hidden),\n",
    "        )\n",
    "        self.node_mlp_2 = Seq(\n",
    "            Lin(inputs + hidden, hidden),\n",
    "            BatchNorm1d(hidden),\n",
    "            ReLU(),\n",
    "            Lin(hidden, hidden),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[row], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out)\n",
    "        out = scatter_mean(out, col, dim=0, dim_size=x.size(0))\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        return self.node_mlp_2(out)\n",
    "\n",
    "class GlobalBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalBlock, self).__init__()\n",
    "        self.global_mlp = Seq(\n",
    "            Lin(hidden, hidden), BatchNorm1d(hidden), ReLU(), Lin(hidden, outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        out = scatter_mean(x, batch, dim=0)\n",
    "        return self.global_mlp(out)\n",
    "\n",
    "class InteractionNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InteractionNetwork, self).__init__()\n",
    "        self.interactionnetwork = MetaLayer(EdgeBlock(), NodeBlock(), GlobalBlock())\n",
    "        self.bn = BatchNorm1d(inputs)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x, edge_attr, u = self.interactionnetwork(x, edge_index, None, None, batch)\n",
    "        return u\n",
    "\n",
    "\n",
    "model = InteractionNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Define the training loop</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, loader, total, batch_size, leave=False):\n",
    "    model.eval()\n",
    "\n",
    "    xentropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total / batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "        data = data.to(device)\n",
    "        y = torch.argmax(data.y, dim=1)\n",
    "        batch_output = model(data.x, data.edge_index, data.batch)\n",
    "        batch_loss_item = xentropy(batch_output, y).item()\n",
    "        sum_loss += batch_loss_item\n",
    "        t.set_description(\"loss = %.5f\" % (batch_loss_item))\n",
    "        t.refresh()  # to show immediately the update\n",
    "\n",
    "    return sum_loss / (i + 1)\n",
    "\n",
    "def train(model, optimizer, loader, total, batch_size, leave=False):\n",
    "    model.train()\n",
    "\n",
    "    xentropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total / batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "        data = data.to(device)\n",
    "        y = torch.argmax(data.y, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = model(data.x, data.edge_index, data.batch)\n",
    "        batch_loss = xentropy(batch_output, y)\n",
    "        batch_loss.backward()\n",
    "        batch_loss_item = batch_loss.item()\n",
    "        t.set_description(\"loss = %.5f\" % batch_loss_item)\n",
    "        t.refresh()  # to show immediately the update\n",
    "        sum_loss += batch_loss_item\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Define training, validation, testing data generators</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataListLoader, Batch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "def collate(items):\n",
    "    l = sum(items, [])\n",
    "    return Batch.from_data_list(l)\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "valid_frac = 0.20\n",
    "full_length = len(graph_dataset)\n",
    "valid_num = int(valid_frac * full_length)\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    graph_dataset, [full_length - valid_num, valid_num]\n",
    ")\n",
    "\n",
    "train_loader = DataListLoader(\n",
    "    train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True\n",
    ")\n",
    "train_loader.collate_fn = collate\n",
    "valid_loader = DataListLoader(\n",
    "    valid_dataset, batch_size=batch_size, pin_memory=True, shuffle=False\n",
    ")\n",
    "valid_loader.collate_fn = collate\n",
    "test_loader = DataListLoader(\n",
    "    test_dataset, batch_size=batch_size, pin_memory=True, shuffle=False\n",
    ")\n",
    "test_loader.collate_fn = collate\n",
    "\n",
    "\n",
    "train_samples = len(train_dataset)\n",
    "valid_samples = len(valid_dataset)\n",
    "test_samples = len(test_dataset)\n",
    "print(full_length)\n",
    "print(train_samples)\n",
    "print(valid_samples)\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Train</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "n_epochs = 10\n",
    "stale_epochs = 0\n",
    "best_valid_loss = 99999\n",
    "patience = 5\n",
    "t = tqdm(range(0, n_epochs))\n",
    "\n",
    "for epoch in t:\n",
    "    loss = train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        train_samples,\n",
    "        batch_size,\n",
    "        leave=bool(epoch == n_epochs - 1),\n",
    "    )\n",
    "    valid_loss = test(\n",
    "        model,\n",
    "        valid_loader,\n",
    "        valid_samples,\n",
    "        batch_size,\n",
    "        leave=bool(epoch == n_epochs - 1),\n",
    "    )\n",
    "    print(\"Epoch: {:02d}, Training Loss:   {:.4f}\".format(epoch, loss))\n",
    "    print(\"           Validation Loss: {:.4f}\".format(valid_loss))\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        modpath = osp.join(\"interactionnetwork_best.pth\")\n",
    "        print(\"New best model saved to:\", modpath)\n",
    "        torch.save(model.state_dict(), modpath)\n",
    "        stale_epochs = 0\n",
    "    else:\n",
    "        print(\"Stale epoch\")\n",
    "        stale_epochs += 1\n",
    "    if stale_epochs >= patience:\n",
    "        print(\"Early stopping after %i stale epochs\" % patience)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:LightBlue\">Evaluate on Test Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you need to load the model from a pth file\n",
    "# Trained on all possible inputs (as above in notebook)\n",
    "# urlPTH = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/interactionnetwork_best_Aug1_AllTraining.pth\"\n",
    "# pthFile = wget.download(urlPTH)\n",
    "# model.load_state_dict(torch.load(\"interactionnetwork_best_Aug1_AllTraining.pth\"))\n",
    "# Trained on 4 vector input (different setup)\n",
    "# urlPTH = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/interactionnetwork_best_Aug1_4vec.pth\"\n",
    "# pthFile = wget.download(urlPTH)\n",
    "# model.load_state_dict(torch.load(\"interactionnetwork_best_Aug1_4vec.pth\"))\n",
    "\n",
    "model.eval()\n",
    "t = tqdm(enumerate(test_loader), total=test_samples / batch_size)\n",
    "y_test = []\n",
    "y_predict = []\n",
    "for i, data in t:\n",
    "    data = data.to(device)\n",
    "    batch_output = model(data.x, data.edge_index, data.batch)\n",
    "    y_predict.append(batch_output.detach().cpu().numpy())\n",
    "    y_test.append(data.y.cpu().numpy())\n",
    "y_test = np.concatenate(y_test)\n",
    "y_predict = np.concatenate(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "plt.style.use(hep.style.ROOT)\n",
    "# create ROC curves\n",
    "fpr_gnn, tpr_gnn, threshold_gnn = roc_curve(y_test[:, 1], y_predict[:, 1])\n",
    "with open(\"gnn_roc.npy\", \"wb\") as f:\n",
    "    np.save(f, fpr_gnn)\n",
    "    np.save(f, tpr_gnn)\n",
    "    np.save(f, threshold_gnn)\n",
    "\n",
    "\n",
    "# For colab:\n",
    "#if not os.path.exists(\"deepset_roc.py\"):\n",
    "if not os.path.exists(\"/Users/msn/repos/jmduarte/iaifi-summer-school/book/deepset_roc.npy\"):\n",
    "    urlROC = \"https://raw.githubusercontent.com/jmduarte/iaifi-summer-school/main/book/deepset_roc.npy\"\n",
    "    rocFile = wget.download(urlROC)\n",
    "\n",
    "#with open(\"deepset_roc.npy\", \"rb\") as f:\n",
    "with open(\"/Users/msn/repos/jmduarte/iaifi-summer-school/book/deepset_roc.npy\", \"rb\") as f:\n",
    "    fpr_deepset = np.load(f)\n",
    "    tpr_deepset = np.load(f)\n",
    "    threshold_deepset = np.load(f)\n",
    "\n",
    "# plot ROC curves\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    tpr_deepset,\n",
    "    fpr_deepset,\n",
    "    lw=2.5,\n",
    "    label=\"DeepSet, AUC = {:.1f}%\".format(auc(fpr_deepset, tpr_deepset) * 100),\n",
    ")\n",
    "plt.plot(\n",
    "    tpr_gnn,\n",
    "    fpr_gnn,\n",
    "    lw=2.5,\n",
    "    label=\"GNN, AUC = {:.1f}%\".format(auc(fpr_gnn, tpr_gnn) * 100),\n",
    ")\n",
    "plt.xlabel(r\"True positive rate\")\n",
    "plt.ylabel(r\"False positive rate\")\n",
    "plt.semilogy()\n",
    "plt.ylim(0.001, 1)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "  * Materials from:\n",
    "    * https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html\n",
    "    * https://jduarte.physics.ucsd.edu/iaifi-summer-school/1.4_gnn_in.html\n",
    "\n",
    "© Copyright 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
