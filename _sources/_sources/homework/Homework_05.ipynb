{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05: Kernel Density Estimation, Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for Getting, Loading and Locating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_data(url: str):\n",
    "    local_path = './tmp_data'\n",
    "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
    "    rc = None\n",
    "    while rc is None:\n",
    "      line = p.stderr.readline().strip('\\n')\n",
    "      if len(line) > 0:\n",
    "        print(line)\n",
    "      rc = p.poll()\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/blobs_data.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_data = pd.read_hdf(locate_data('blobs_data.hf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement the core of the E- and M-steps for the [Gaussian mixture model (GMM)](http://scikit-learn.org/stable/modules/mixture.html) method. Note the similarities with the E- and M-steps of the K-means method.\n",
    "\n",
    "First, implement the function below to evaluate the [multidimensional Gaussian probability density](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) for arbitrary mean $\\vec{\\mu}$ and covariance matrix $C$ (refer to the lecture for more details on the notation used here):\n",
    "$$\n",
    "G(\\vec{x} ; \\vec{\\mu}, C) = \\left(2\\pi\\right)^{-D/2}\\,\\left| C\\right|^{-1/2}\\,\n",
    "\\exp\\left[  -\\frac{1}{2} \\left(\\vec{x} - \\vec{\\mu}\\right)^T C^{-1} \\left(\\vec{x} - \\vec{\\mu}\\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b48ba26027b4ee50d4805dba5049f685",
     "grade": false,
     "grade_id": "cell-3e1557ddc299c998",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Gaussian_pdf(x, mu, C):\n",
    "    \"\"\"Evaluate the Gaussian probability density.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        1D array of D feature values for a single sample\n",
    "    mu : array\n",
    "        1D array of D mean feature values for this component.\n",
    "    C : array\n",
    "        2D array with shape (D, D) of covariance matrix elements for this component.\n",
    "        Must be positive definite (and therefore symmetric).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Probability density.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    mu = np.asarray(mu)\n",
    "    C = np.asarray(C)\n",
    "    D = len(x)\n",
    "    assert x.shape == (D,) and mu.shape == (D,)\n",
    "    assert C.shape == (D, D)\n",
    "    assert np.allclose(C.T, C)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a5e0506b7cb3a234462031ca4aa844f1",
     "grade": true,
     "grade_id": "cell-2c49da9532bcc30e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [1], [[1]]), 1 / np.sqrt(2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([0], [0], [[2]]), 1 / np.sqrt(4 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1], [0], [[1]]), np.exp(-0.5) / np.sqrt(2 * np.pi))\n",
    "\n",
    "assert np.allclose(Gaussian_pdf([0, 0], [0, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, -1], [1, -1], [[1, 0], [0, 1]]), 1 / (2 * np.pi))\n",
    "assert np.allclose(Gaussian_pdf([1, 0], [1, 0], [[4, 0], [0, 1]]), 1 / (4 * np.pi))\n",
    "\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, +1], [+1, 1]]), 5) == 0.07778\n",
    "assert np.round(Gaussian_pdf([0, 0], [1, 0], [[4, -1], [-1, 1]]), 5) == 0.07778\n",
    "assert np.round(np.log(Gaussian_pdf([1, 0, -1], [1, 2, 3], [[4, -1, 0], [-1, 1, 0], [0, 0, 2]])), 5) == -10.31936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the E-step in the function below. This consists of calculating the relative probability that each sample $\\vec{x}_n$ ($n$-th row of $X$) belongs to each component $k$:\n",
    "$$\n",
    "p_{nk} = \\frac{\\omega_k G(\\vec{x}_n; \\vec{\\mu}_k, C_k)}\n",
    "{\\sum_{j=1}^K\\, \\omega_j G(\\vec{x}_n; \\vec{\\mu}_j, C_j)}\n",
    "$$\n",
    "Note that these relative probabilities (also called *responsibilities*) sum to one over components $k$ for each sample $n$.  Also note that we consider the parameters ($\\omega_k$, $\\vec{\\mu}_k$, $C_k$) of each component fixed during this step. *Hint: use your `Gaussian_pdf` function here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "61fd366e12f666c81f70dbff0cb2112b",
     "grade": false,
     "grade_id": "cell-918f72c2d88d9e0b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def E_step(X, w, mu, C):\n",
    "    \"\"\"Perform a GMM E-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    w : array with shape (K,)\n",
    "        Per-component weights.\n",
    "    mu : array with shape (K, D)\n",
    "        Array of mean vectors for each component.\n",
    "    C : array with shape (K, D, D).\n",
    "        Array of covariance matrices for each component.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(w)\n",
    "    assert w.shape == (K,)\n",
    "    assert mu.shape == (K, D)\n",
    "    assert C.shape == (K, D, D)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ea2e10e850207fd584ff9751640780ab",
     "grade": true,
     "grade_id": "cell-bdaef2ea56dd9bee",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "w = np.full(4, 0.25)\n",
    "mu = np.array([[-2], [-1], [0], [1]])\n",
    "C = np.ones((4, 1, 1))\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.258,  0.134,  0.058,  0.021,  0.006],\n",
    "     [ 0.426,  0.366,  0.258,  0.152,  0.077],\n",
    "     [ 0.258,  0.366,  0.426,  0.414,  0.346],\n",
    "     [ 0.058,  0.134,  0.258,  0.414,  0.57 ]])\n",
    "\n",
    "X = np.zeros((1, 3))\n",
    "w = np.ones((2,))\n",
    "mu = np.zeros((2, 3))\n",
    "C = np.zeros((2, 3, 3))\n",
    "diag = range(3)\n",
    "C[:, diag, diag] = 1\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.5], [ 0.5]])\n",
    "\n",
    "X = np.array([[0,0,0], [1,0,0]])\n",
    "mu = np.array([[0,0,0], [1,0,0]])\n",
    "#print(repr(np.round(E_step(X, w, mu, C), 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C), 3) ==\n",
    "    [[ 0.622,  0.378], [ 0.378,  0.622]])\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "subsample = X.reshape(K, (N//K), D)\n",
    "mu = subsample.mean(axis=1)\n",
    "C = np.empty((K, D, D))\n",
    "w = gen.uniform(size=K)\n",
    "w /= w.sum()\n",
    "for k in range(K):\n",
    "    C[k] = np.cov(subsample[k], rowvar=False)\n",
    "#print(repr(np.round(E_step(X, w, mu, C)[:, :5], 3)))\n",
    "assert np.all(\n",
    "    np.round(E_step(X, w, mu, C)[:, :5], 3) ==\n",
    "    [[ 0.422,  0.587,  0.344,  0.279,  0.19 ],\n",
    "     [ 0.234,  0.11 ,  0.269,  0.187,  0.415],\n",
    "     [ 0.291,  0.194,  0.309,  0.414,  0.279],\n",
    "     [ 0.053,  0.109,  0.077,  0.12 ,  0.116]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the M-step in the function below.  During this step, we consider the relative weights $p_{nk}$ from the previous step fixed and instead update the parameters of each component (which were fixed in the previous step), using:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\omega_k &= \\frac{1}{N}\\, \\sum_{n=1}^N\\, p_{nk} \\\\\n",
    "\\vec{\\mu}_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\vec{x}_n}{\\sum_{n=1}^N\\, p_{nk}} \\\\\n",
    "C_k &= \\frac{\\sum_{n=1}^N\\, p_{nk} \\left( \\vec{x}_n - \\vec{\\mu}_k\\right) \\left( \\vec{x}_n - \\vec{\\mu}_k\\right)^T}\n",
    "{\\sum_{n=1}^N\\, p_{nk}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Make sure you understand why the last expression yields a matrix rather than a scalar dot product before jumping into the code. (If you would like a numpy challenge, try implementing this function without any loops, e.g., with `np.einsum`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "65d96b3fc5370a9805c19411492121a7",
     "grade": false,
     "grade_id": "cell-6cd6167c32f19845",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def M_step(X, p):\n",
    "    \"\"\"Perform a GMM M-step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    p : array with shape (K, N)\n",
    "        Array of relative probabilities that each sample belongs to\n",
    "        each component, normalized so that the per-component probabilities\n",
    "        for each sample sum to one.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple w, mu, C of arrays with shapes (K,), (K, D) and (K, D, D) giving\n",
    "        the updated component parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(p)\n",
    "    assert p.shape == (K, N)\n",
    "    assert np.allclose(p.sum(axis=0), 1)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afb18d41f5b985695d83f17fcbaca522",
     "grade": true,
     "grade_id": "cell-7359cf92c75ff48d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "X = np.linspace(-1, 1, 5).reshape(-1, 1)\n",
    "p = np.full(20, 0.25).reshape(4, 5)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C, 5)))\n",
    "assert np.all(np.round(w, 5) == 0.25)\n",
    "assert np.all(np.round(mu, 5) == 0.0)\n",
    "assert np.all(np.round(C, 5) == 0.5)\n",
    "\n",
    "gen = np.random.RandomState(seed=123)\n",
    "K, N, D = 4, 1000, 5\n",
    "X = gen.normal(size=(N, D))\n",
    "p = gen.uniform(size=(K, N))\n",
    "p /= p.sum(axis=0)\n",
    "w, mu, C = M_step(X, p)\n",
    "#print(repr(np.round(w, 5)))\n",
    "#print(repr(np.round(mu, 5)))\n",
    "#print(repr(np.round(C[0], 5)))\n",
    "assert np.all(\n",
    "    np.round(w, 5) == [ 0.25216,  0.24961,  0.24595,  0.25229])\n",
    "assert np.all(\n",
    "    np.round(mu, 5) ==\n",
    "    [[ 0.06606,  0.06   , -0.00413,  0.01562,  0.00258],\n",
    "     [ 0.02838,  0.01299,  0.01286,  0.03068, -0.01714],\n",
    "     [ 0.03157,  0.04558, -0.01206,  0.03493, -0.0326 ],\n",
    "     [ 0.05467,  0.06293, -0.01779,  0.04454,  0.00065]])\n",
    "assert np.all(\n",
    "    np.round(C[0], 5) ==\n",
    "    [[ 0.98578,  0.01419, -0.03717,  0.01403,  0.0085 ],\n",
    "     [ 0.01419,  0.95534, -0.02724,  0.03201, -0.00648],\n",
    "     [-0.03717, -0.02724,  0.90722,  0.00313,  0.0299 ],\n",
    "     [ 0.01403,  0.03201,  0.00313,  1.02891,  0.0813 ],\n",
    "     [ 0.0085 , -0.00648,  0.0299 ,  0.0813 ,  0.922  ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented the core of the GMM algorithm. Next you will use KMeans as a means of seeding the GMM model fit. First we include two helpful functions `draw_ellipses` and `GMM_parplot` to help display results. The details of these methods are not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import colorConverter, ListedColormap\n",
    "from matplotlib.collections import EllipseCollection\n",
    "\n",
    "def draw_ellipses(w, mu, C, nsigmas=2, color='red', outline=None, filled=True, axis=None):\n",
    "    \"\"\"Draw a collection of ellipses.\n",
    "\n",
    "    Uses the low-level EllipseCollection to efficiently draw a large number\n",
    "    of ellipses. Useful to visualize the results of a GMM fit via\n",
    "    GMM_pairplot() defined below.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : array\n",
    "        1D array of K relative weights for each ellipse. Must sum to one.\n",
    "        Ellipses with smaller weights are rendered with greater transparency\n",
    "        when filled is True.\n",
    "    mu : array\n",
    "        Array of shape (K, 2) giving the 2-dimensional centroids of\n",
    "        each ellipse.\n",
    "    C : array\n",
    "        Array of shape (K, 2, 2) giving the 2 x 2 covariance matrix for\n",
    "        each ellipse.\n",
    "    nsigmas : float\n",
    "        Number of sigmas to use for scaling ellipse area to a confidence level.\n",
    "    color : matplotlib color spec\n",
    "        Color to use for the ellipse edge (and fill when filled is True).\n",
    "    outline : None or matplotlib color spec\n",
    "        Color to use to outline the ellipse edge, or no outline when None.\n",
    "    filled : bool\n",
    "        Fill ellipses with color when True, adjusting transparency to\n",
    "        indicate relative weights.\n",
    "    axis : matplotlib axis or None\n",
    "        Plot axis where the ellipse collection should be drawn. Uses the\n",
    "        current default axis when None.\n",
    "    \"\"\"\n",
    "    # Calculate the ellipse angles and bounding boxes using SVD.\n",
    "    U, s, _ = np.linalg.svd(C)\n",
    "    angles = np.degrees(np.arctan2(U[:, 1, 0], U[:, 0, 0]))\n",
    "    widths, heights = 2 * nsigmas * np.sqrt(s.T)\n",
    "    # Initialize colors.\n",
    "    color = colorConverter.to_rgba(color)\n",
    "    if filled:\n",
    "        # Use transparency to indicate relative weights.\n",
    "        ec = np.tile([color], (len(w), 1))\n",
    "        ec[:, -1] *= w\n",
    "        fc = np.tile([color], (len(w), 1))\n",
    "        fc[:, -1] *= w ** 2\n",
    "    # Data limits must already be defined for axis.transData to be valid.\n",
    "    axis = axis or plt.gca()\n",
    "    if outline is not None:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=4,\n",
    "            transOffset=axis.transData, facecolors='none', edgecolors=outline))\n",
    "    if filled:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=2,\n",
    "            transOffset=axis.transData, facecolors=fc, edgecolors=ec))\n",
    "    else:\n",
    "        axis.add_collection(EllipseCollection(\n",
    "            widths, heights, angles, units='xy', offsets=mu, linewidths=2.5,\n",
    "            transOffset=axis.transData, facecolors='none', edgecolors=color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_pairplot(data, w, mu, C, limits=None, entropy=False):\n",
    "    \"\"\"Display 2D projections of a Gaussian mixture model fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas DataFrame\n",
    "        N samples of D-dimensional data.\n",
    "    w : array\n",
    "        1D array of K relative weights for each ellipse. Must sum to one.\n",
    "    mu : array\n",
    "        Array of shape (K, 2) giving the 2-dimensional centroids of\n",
    "        each ellipse.\n",
    "    C : array\n",
    "        Array of shape (K, 2, 2) giving the 2 x 2 covariance matrix for\n",
    "        each ellipse.\n",
    "    limits : array or None\n",
    "        Array of shape (D, 2) giving [lo,hi] plot limits for each of the\n",
    "        D dimensions. Limits are determined by the data scatter when None.\n",
    "    \"\"\"\n",
    "    colnames = data.columns.values\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    if entropy:\n",
    "        n_components = len(w)\n",
    "        # Pick good colors to distinguish the different clusters.\n",
    "        cmap = ListedColormap(\n",
    "            sns.color_palette('husl', n_components).as_hex())\n",
    "        # Calculate the relative probability that each sample belongs to each cluster.\n",
    "        # This is equivalent to fit.predict_proba(X)\n",
    "        lnprob = np.zeros((n_components, N))\n",
    "        for k in range(n_components):\n",
    "            lnprob[k] = scipy.stats.multivariate_normal.logpdf(X, mu[k], C[k])\n",
    "        lnprob += np.log(w)[:, np.newaxis]\n",
    "        prob = np.exp(lnprob)\n",
    "        prob /= prob.sum(axis=0)\n",
    "        prob = prob.T\n",
    "        # Assign each sample to its most probable cluster.\n",
    "        labels = np.argmax(prob, axis=1)\n",
    "        color = cmap(labels)\n",
    "        if n_components > 1:\n",
    "            # Calculate the relative entropy (0-1) as a measure of cluster assignment ambiguity.\n",
    "            relative_entropy = -np.sum(prob * np.log(prob), axis=1) / np.log(n_components)\n",
    "            color[:, :3] *= (1 - relative_entropy).reshape(-1, 1)        \n",
    "    # Build a pairplot of the results.\n",
    "    fs = 5 * min(D - 1, 3)\n",
    "    fig, axes = plt.subplots(D - 1, D - 1, sharex='col', sharey='row',\n",
    "                             squeeze=False, figsize=(fs, fs))\n",
    "    for i in range(1, D):\n",
    "        for j in range(D - 1):\n",
    "            ax = axes[i - 1, j]\n",
    "            if j >= i:\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            # Plot the data in this projection.\n",
    "            if entropy:\n",
    "                ax.scatter(X[:, j], X[:, i], s=5, c=color, cmap=cmap)\n",
    "                draw_ellipses(\n",
    "                    w, mu[:, [j, i]], C[:, [[j], [i]], [[j, i]]],\n",
    "                    color='w', outline='k', filled=False, axis=ax)\n",
    "            else:\n",
    "                ax.scatter(X[:, j], X[:, i], s=10, alpha=0.3, c='k', lw=0)\n",
    "                draw_ellipses(\n",
    "                    w, mu[:, [j, i]], C[:, [[j], [i]], [[j, i]]],\n",
    "                    color='red', outline=None, filled=True, axis=ax)\n",
    "            # Overlay the fit components in this projection.\n",
    "            # Add axis labels and optional limits.\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(colnames[i])\n",
    "                if limits: ax.set_ylim(limits[i])\n",
    "            if i == D - 1:\n",
    "                ax.set_xlabel(colnames[j])\n",
    "                if limits: ax.set_xlim(limits[j])\n",
    "    plt.subplots_adjust(hspace=0.02, wspace=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple wrapper that uses KMeans to initialize the relative probabilities to all be either zero or one, based on each sample's cluster assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_fit(data, n_components, nsteps, init='random', seed=123):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    p = np.zeros((n_components, N))\n",
    "    if init == 'kmeans':\n",
    "        # Use KMeans to divide the data into clusters.\n",
    "        fit = cluster.KMeans(n_clusters=n_components, random_state=gen, n_init=10).fit(data)\n",
    "        # Initialize the relative weights using cluster membership.\n",
    "        # The initial weights are therefore all either 0 or 1.\n",
    "        for k in range(n_components):\n",
    "            p[k, fit.labels_ == k] = 1\n",
    "    else:\n",
    "        # Assign initial relative weights in quantiles of the first feature.\n",
    "        # This is not a good initialization strategy, but shows how well\n",
    "        # GMM converges from a poor starting point.\n",
    "        x0 = X[:, 0]\n",
    "        edges = np.percentile(x0, np.linspace(0, 100, n_components + 1))\n",
    "        for k in range(n_components):\n",
    "            quantile = (edges[k] <= x0) & (x0 <= edges[k + 1])\n",
    "            p[k, quantile] = 1.\n",
    "    # Normalize relative weights.\n",
    "    p /= p.sum(axis=0)\n",
    "    # Perform an initial M step to initialize the component params.\n",
    "    w, mu, C = M_step(X, p)\n",
    "    # Loop over iterations.\n",
    "    for i in range(nsteps):\n",
    "        p = E_step(X, w, mu, C)\n",
    "        w, mu, C = M_step(X, p)\n",
    "    # Plot the results.\n",
    "    GMM_pairplot(data, w, mu, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this out on the 3D `blobs_data` and notice that it converges close to the correct solution after 8 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0)\n",
    "GMM_fit(blobs_data, 3, nsteps=4)\n",
    "GMM_fit(blobs_data, 3, nsteps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence is even faster if you use KMeans to initialize the relative weights (which is why most implementations do this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMM_fit(blobs_data, 3, nsteps=0, init='kmeans')\n",
    "GMM_fit(blobs_data, 3, nsteps=1, init='kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A density estimator should provide a probability density function $P(\\vec{x})$ that is normalized over its feature space $\\vec{x}$\n",
    "$$\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) = 1 \\; .\n",
    "$$\n",
    "In this problem you will verify this normalization for KDE using two different numerical approaches for the integral.\n",
    "\n",
    "First, implement the function below to accept a 1D KDE fit object and estimate its normalization integral using the trapezoid rule with the specified grid. *Hint: the `np.trapz` function will be useful.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c7c730306185db01f0a5f377b61660a4",
     "grade": false,
     "grade_id": "cell-f08e5071751cdabd",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def check_grid_normalization(fit, xlo, xhi, ngrid):\n",
    "    \"\"\"Check 1D denstity estimator fit result normlization using grid quadrature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.KernelDensity fit object\n",
    "        Result of fit to 1D dataset.\n",
    "    xlo : float\n",
    "        Low edge of 1D integration range.\n",
    "    xhi : float\n",
    "        High edge of 1D integration range.\n",
    "    ngrid : int\n",
    "        Number of equally spaced grid points covering [xlo, xhi],\n",
    "        including both end points.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ec1ffc451d8c87dc51acb674601e07b",
     "grade": true,
     "grade_id": "cell-d535958d6d6e7dc6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']).values)\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 5), 3) == 1.351\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 10), 3) == 1.019\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 20), 3) == 0.986\n",
    "assert np.round(check_grid_normalization(fit, 0, 15, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x2']).values)\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 5), 3) == 1.108\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 10), 3) == 0.993\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 20), 3) == 0.971\n",
    "assert np.round(check_grid_normalization(fit, -4, 12, 100), 3) == 1.000\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x0', 'x1']).values)\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 5), 3) == 1.311\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 10), 3) == 0.954\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 20), 3) == 1.028\n",
    "assert np.round(check_grid_normalization(fit, 2, 18, 100), 3) == 1.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to estimate a multidimensional fit normalization using [Monte Carlo integration](https://en.wikipedia.org/wiki/Monte_Carlo_integration):\n",
    "$$\n",
    "\\int d\\vec{x}\\, P(\\vec{x}) \\simeq \\frac{V}{N_{mc}}\\, \\sum_{j=1}^{N_{mc}} P(\\vec{x}_j) = V \\langle P\\rangle \\; ,\n",
    "$$\n",
    "where the $\\vec{x}_j$ are uniformly distributed over the integration domain and $V$ is the integration domain volume. Note that `trapz` gives more accurate results for a fixed number of $P(\\vec{x})$ evaluations, but MC integration is much easier to generalize to higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "039324c2b4b3e33fe81db69b161bfb2f",
     "grade": false,
     "grade_id": "cell-6a773ed33ecb1799",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def check_mc_normalization(fit, xlo, xhi, nmc, seed=123):\n",
    "    \"\"\"Check denstity estimator fit result normlization using MC integration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fit : neighbors.KernelDensity fit object\n",
    "        Result of fit to arbitrary dataset of dimension D.\n",
    "    xlo : array\n",
    "        1D array of length D with low limits of integration domain along each dimension.\n",
    "    xhi : array\n",
    "        1D array of length D with high limits of integration domain along each dimension.\n",
    "    nmc : int\n",
    "        Number of random MC integration points within the domain to use.\n",
    "    \"\"\"\n",
    "    xlo = np.asarray(xlo)\n",
    "    xhi = np.asarray(xhi)\n",
    "    assert xlo.shape == xhi.shape\n",
    "    assert np.all(xhi > xlo)\n",
    "    D = len(xlo)\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # Use gen.uniform() in your solution, not gen.rand(), for consistent random numbers.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "276d5e5fe9b26b0e892d8f1327a6fe22",
     "grade": true,
     "grade_id": "cell-7b392286cb763415",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### A correct solution should pass these tests.\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x1', 'x2']).values)\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10), 3) == 1.129\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 100), 3) == 1.022\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 1000), 3) == 1.010\n",
    "assert np.round(check_mc_normalization(fit, [0], [15], 10000), 3) == 0.999\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.drop(columns=['x2']).values)\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10), 3) == 1.754\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 100), 3) == 1.393\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 1000), 3) == 0.924\n",
    "assert np.round(check_mc_normalization(fit, [0, -4], [15, 12], 10000), 3) == 1.019\n",
    "\n",
    "fit = neighbors.KernelDensity(kernel='gaussian', bandwidth=0.1).fit(blobs_data.values)\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10), 3) == 2.797\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 100), 3) == 0.613\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 1000), 3) == 1.316\n",
    "assert np.round(check_mc_normalization(fit, [0, -4, 2], [15, 12, 18], 10000), 3) == 1.139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal (aka Gaussian) distribution is one of the fundamental probability densities that we will encounter often.\n",
    "\n",
    "Implement the function below using `np.random.multivariate_normal` to generate random samples from an arbitrary multidimensional normal distribution, for a specified random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal(mu, C, n, seed=123):\n",
    "    \"\"\"Generate random samples from a normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu : array\n",
    "        1D array of mean values of length N.\n",
    "    C : array\n",
    "        2D array of covariances of shape (N, N). Must be a positive-definite matrix.\n",
    "    n : int\n",
    "        Number of random samples to generate.\n",
    "    seed : int\n",
    "        Random number seed to use.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        2D array of shape (n, N) where each row is a random N-dimensional sample.\n",
    "    \"\"\"\n",
    "    assert len(mu.shape) == 1, 'mu must be 1D.'\n",
    "    assert C.shape == (len(mu), len(mu)), 'C must be N x N.'\n",
    "    assert np.all(np.linalg.eigvals(C) > 0), 'C must be positive definite.'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "mu = np.array([-1., 0., +1.])\n",
    "C = np.identity(3)\n",
    "C[0, 1] = C[1, 0] = -0.9\n",
    "Xa = generate_normal(mu, C, n=500, seed=1)\n",
    "Xb = generate_normal(mu, C, n=500, seed=1)\n",
    "Xc = generate_normal(mu, C, n=500, seed=2)\n",
    "assert np.array_equal(Xa, Xb)\n",
    "assert not np.array_equal(Xb, Xc)\n",
    "X = generate_normal(mu, C, n=2000, seed=3)\n",
    "assert np.allclose(np.mean(X, axis=0), mu, rtol=0.001, atol=0.1)\n",
    "assert np.allclose(np.cov(X, rowvar=False), C, rtol=0.001, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a generated 3D dataset using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d():\n",
    "    mu = np.array([-1., 0., +1.])\n",
    "    C = np.identity(3)\n",
    "    C[0, 1] = C[1, 0] = -0.9\n",
    "    X = generate_normal(mu, C, n=2000, seed=3)\n",
    "    df = pd.DataFrame(X, columns=('x0', 'x1', 'x2'))\n",
    "    sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about [correlation and covariance](https://en.wikipedia.org/wiki/Covariance_and_correlation), then implement the function below to create a 2x2 covariance matrix given values of $\\sigma_x$, $\\sigma_y$ and the correlation coefficient $\\rho$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_covariance(sigma_x, sigma_y, rho):\n",
    "    \"\"\"Create and return the 2x2 covariance matrix specified by the input args.\n",
    "    \"\"\"\n",
    "    assert (sigma_x > 0) and (sigma_y > 0), 'sigmas must be > 0.'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "assert np.array_equal(create_2d_covariance(1., 1.,  0.0), [[1.,  0.], [ 0., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1.,  0.0), [[4.,  0.], [ 0., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1.,  0.5), [[4.,  1.], [ 1., 1.]])\n",
    "assert np.array_equal(create_2d_covariance(2., 1., -0.5), [[4., -1.], [-1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell that uses your `create_2d_covariance` and `generate_normal` functions to compare the 2D normal distributions with $\\rho = 0$ (blue), $\\rho = +0.9$ (red) and $\\rho = -0.9$ (green):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rhos():\n",
    "    mu = np.zeros(2)\n",
    "    sigma_x, sigma_y = 2., 1.\n",
    "    for rho, cmap in zip((0., +0.9, -0.9), ('Blues', 'Reds', 'Greens')):\n",
    "        C = create_2d_covariance(sigma_x, sigma_y, rho)\n",
    "        X = generate_normal(mu, C, 10000)\n",
    "        sns.kdeplot(x=X[:, 0], y=X[:, 1], cmap=cmap)\n",
    "    plt.xlim(-4, +4)\n",
    "    plt.ylim(-2, +2)\n",
    "        \n",
    "compare_rhos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
