{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 09: Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import subprocess\n",
    "import autograd.numpy as anp\n",
    "import autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for Getting, Loading and Locating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_data(url: str):\n",
    "    local_path = './tmp_data'\n",
    "    p = subprocess.Popen([\"wget\", \"-nc\", \"-P\", local_path, url], stderr=subprocess.PIPE, encoding='UTF-8')\n",
    "    rc = None\n",
    "    while rc is None:\n",
    "      line = p.stderr.readline().strip('\\n')\n",
    "      if len(line) > 0:\n",
    "        print(line)\n",
    "      rc = p.poll()\n",
    "\n",
    "def locate_data(name, check_exists=True):\n",
    "    local_path='./tmp_data'\n",
    "    path = os.path.join(local_path, name)\n",
    "    if check_exists and not os.path.exists(path):\n",
    "        raise RuxntimeError('No such data file: {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Problem 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we hand-tuned a network to assign 2D points to the correct cluster, then repeated the exercise using tensorflow to automatically learn network parameters from the training data. In this problem, you will construct and optimize the same neural network by hand (without tensorflow) to get a deeper understanding of what is involved (and why tensorflow saves a lot of work).\n",
    "\n",
    "First, load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/circles_data.hf5')\n",
    "wget_data('https://raw.githubusercontent.com/illinois-ipaml/MachineLearningForPhysics/main/data/circles_targets.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_hdf(locate_data('circles_data.hf5')).values\n",
    "y = pd.read_hdf(locate_data('circles_targets.hf5')).values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circles():\n",
    "    cmap = sns.color_palette('colorblind', 2)\n",
    "    colors = [cmap[int(c)] for c in y]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors)\n",
    "    plt.gca().set_aspect(1)\n",
    "    \n",
    "plot_circles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to evalute the network prediction given its parameter values and a 2D input point $(x_1, x_2)$. Recall that the network consists of:\n",
    " - 2 input nodes\n",
    "\n",
    " - 4 hidden nodes with sigmoid activation\n",
    "\n",
    " - 1 output node with sigmoid activation\n",
    " \n",
    "The corresponding mathematical function is:\n",
    "\n",
    "$$ \\Large\n",
    "\\begin{align*}\n",
    "y^{out}(\\mathbf{x},\\Theta)\n",
    "&= \\phi\\left([u_1, u_2, u_3, u_4]\\cdot \\mathbf{G}(x_1, x_2) + a \\right) \\\\\n",
    "\\mathbf{G}(x_1, x_2) &= \\phi\\left(\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} \\\\\n",
    "v_{21} & v_{22} \\\\\n",
    "v_{31} & v_{32} \\\\\n",
    "v_{41} & v_{42}\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix} +\n",
    "\\begin{bmatrix}b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4\\end{bmatrix}\n",
    "\\right)\n",
    "\\end{align*} \\\\\n",
    "$$\n",
    "\n",
    "$$ \\Large\n",
    "\\phi(s) = \\frac{1}{1 + e^{-s}}\n",
    "$$\n",
    "\n",
    "In the function, we pack all the parameters into a single array to simplify the later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0106a1c071acfb08",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def network(x, params):\n",
    "    \"\"\"Evaluate the network output at (x1,x2) with the specified parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        Array of length 2 giving the 2D coordinates of a point to classify.\n",
    "    params : array\n",
    "        Array of length 17 containing values for the parameters: u1, u2, u3, u4, a,\n",
    "        v11, v12, v21, v22, v31, v32, v41, v42, b1, b2, b3, b4.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value of the network's single output node.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a73df46e7a2172d5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "initial = np.array([\n",
    "    5,5,5,5,              # ui\n",
    "    -18,                  # a\n",
    "    8,0, -8,0, 0,8, 0,-8, # vij\n",
    "    10,10,10,10           # bi\n",
    "], dtype=float)\n",
    "assert np.round(network([0, 0], initial), 3) == 0.881\n",
    "assert np.round(network([0, 1], initial), 3) == 0.803\n",
    "assert np.round(network([1, 0], initial), 3) == 0.803\n",
    "assert np.round(network([1, -1], initial), 3) == 0.692\n",
    "assert np.round(network([-1, 1], initial), 3) == 0.692"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn the parameters from the data by optimizing the [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) loss function:\n",
    "\n",
    "$$ \\Large\n",
    "E(\\Theta, \\mathbf{X}^{train}, \\mathbf{y}^{train}) =\n",
    "-\\sum_{i=1}^N\\, \\left[\n",
    "y^{train}_i \\log y^{out}(\\mathbf{X}^{train}_i, \\Theta) + (1 - y^{train}_i) \\log (1 - y^{out}(\\mathbf{X}^{train}_i, \\Theta)) \\right]\n",
    "$$\n",
    "\n",
    "(Refer to the Neural Network lecture notes for details on why this is a good loss function).\n",
    "\n",
    "Implement the function below to evaluate this loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c3b6059f054ac65",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(params, X_train, y_train):\n",
    "    \"\"\"Evaluate the cross-entropy loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array\n",
    "        Array of length 17 containing values for the parameters: u1, u2, u3, u4, a,\n",
    "        v11, v12, v21, v22, v31, v32, v41, v42, b1, b2, b3, b4.\n",
    "    X_train : array\n",
    "        Array of shape (N, 2) with training values.\n",
    "    y_train : array\n",
    "        Array of N target values which are each either 0. or 1.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The cross-entropy loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7cac2e52358561d7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "assert np.round(loss(initial, np.array([[0.,0.]]), np.array([0.])), 3) == 2.126\n",
    "assert np.round(loss(initial, np.array([[0.,0.]]), np.array([1.])), 3) == 0.127\n",
    "assert np.round(loss(initial, np.array([[2.,1.]]), np.array([0.])), 3) == 0.027\n",
    "assert np.round(loss(initial, np.array([[2.,1.]]), np.array([1.])), 3) == 3.611\n",
    "assert np.round(loss(initial, X, y), 1) == 41.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the initial parameter values are at a local minimum of the loss by running 1D scans along each parameter axis. Scans are also useful for rough numerical estimates of the partial derivatives\n",
    "\n",
    "$$ \\Large\n",
    "\\frac{\\partial}{\\partial \\Theta_k} E(\\Theta, \\mathbf{X}^{train}, \\mathbf{y}^{train}) \\; .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scans(params, X_train, y_train, step_size=0.5, n=3):\n",
    "    names = 'u1,u2,u3,u4,a,v11,v12,v21,v22,v31,v32,v41,v42,b1,b2,b3,b4'.split(',')\n",
    "    dp_grid = np.linspace(-step_size, +step_size, 2 * n + 1)\n",
    "    L = np.empty_like(dp_grid)\n",
    "    for i, p0 in enumerate(initial):\n",
    "        pscan = params.copy()\n",
    "        for j, dp in enumerate(dp_grid):\n",
    "            pscan[i] = p0 + dp\n",
    "            L[j] = loss(pscan, X_train, y_train)\n",
    "        plt.plot(dp_grid, L, label=names[i])\n",
    "        print('numerical grad[{}] = {:.3f}'.format(names[i], np.gradient(L, dp_grid)[n]))\n",
    "    plt.legend(ncol=5)\n",
    "    plt.ylim(35., 50.)\n",
    "    plt.xlabel('Change in parameter value')\n",
    "    plt.ylabel('Cross-entropy loss')\n",
    "\n",
    "plot_scans(initial, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the function below to calculate the partial derivatives of the loss function,\n",
    "\n",
    "$$ \\Large\n",
    "\\frac{\\partial}{\\partial \\Theta_k} E(\\Theta, \\mathbf{X}^{train}, \\mathbf{y}^{train}) \\; .\n",
    "$$\n",
    "\n",
    "These should be calculated to high accuracy, to allow stable optimization, so either using the autograd package for automatic differentiation (refer to the Optimization lecture) or else using derivative formulas that you calculate by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dac6169cbccf26d5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_gradient(params, X_train, y_train):\n",
    "    \"\"\"Calculate the partial derivatives of the loss function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : array\n",
    "        Array of length 17 containing values for the parameters: u1, u2, u3, u4, a,\n",
    "        v11, v12, v21, v22, v31, v32, v41, v42, b1, b2, b3, b4.\n",
    "    X_train : array\n",
    "        Array of shape (N, 2) with training values.\n",
    "    y_train : array\n",
    "        Array of N target values which are each either 0. or 1.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Array of length 17 containing the partial derivatives of the loss function\n",
    "        with respect to each of the parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd2f9cce7531b7ac",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass the tests below.\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[0.,0.]]), np.array([0.])), 3),\n",
    "    [ 0.881, 0.881, 0.881, 0.881, 0.881, 0., 0., 0.,\n",
    "      0., 0., 0., 0., 0., 0., 0., 0., 0.], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[0.,0.]]), np.array([1.])), 3),\n",
    "    [ -0.119, -0.119, -0.119, -0.119, -0.119, 0., 0.,\n",
    "      0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[2.,1.]]), np.array([0.])), 3),\n",
    "    [ 0.027, 0., 0.027, 0.024, 0.027, 0., 0., 0.001,\n",
    "      0., 0., 0., 0.028, 0.014, 0., 0., 0., 0.014], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, np.array([[2.,1.]]), np.array([1.])), 3),\n",
    "    [ -0.973, -0.002, -0.973, -0.857, -0.973, -0., -0., -0.024,\n",
    "      -0.012, 0., 0., -1.022, -0.511, 0., -0.012, 0., -0.511], atol=1e-3)\n",
    "assert np.allclose(\n",
    "    np.round(loss_gradient(initial, X, y), 1),\n",
    "    [ -24.1, -24.1, -24.1, -24.1, -21.9, -0.5, 0.0, 0.5, 0.0,\n",
    "      0.0, -0.5, 0.0, 0.4, 0.3, 0.3, 0.3, 0.3 ], atol=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented all of the pieces necessary to optimize the 17 network parameters using the gradient descent update rule,\n",
    "\n",
    "$$ \\Large\n",
    "\\Theta \\rightarrow \\Theta - \\eta \\nabla E(\\Theta) \\; ,\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate.  The large values of some of the derivatives we found above indicate that $\\eta$ will need to be small in order for the optimization to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(initial, X_train, y_train, eta=0.001, n_steps=30):\n",
    "    loss_history = [loss(initial, X_train, y_train)]\n",
    "    params = initial.copy()\n",
    "    for i in range(n_steps):\n",
    "        params -= eta * loss_gradient(params, X_train, y_train)\n",
    "        loss_history.append(loss(params, X_train, y_train))\n",
    "        print('step {:02d}: loss={:.1f}'.format(i + 1, loss_history[-1]))\n",
    "    plt.plot(loss_history, 'o-')\n",
    "    plt.xlabel('Optimization step')\n",
    "    plt.ylabel('Cross-entropy loss')\n",
    "\n",
    "optimize(initial, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Orange\">Acknowledgments</span>\n",
    "\n",
    "* Initial version: Mark Neubauer\n",
    "\n",
    "Â© Copyright 2024"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
